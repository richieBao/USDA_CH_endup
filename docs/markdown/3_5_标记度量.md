Created on Tue Feb  7 08:03:51 2023 @author: Richie Bao-caDesign设计(cadesign.cn)

# 3.5 标记距离

地理空间数据（Geospatial data）通常是带有空间坐标的数据，如果为二维经纬度坐标，则通常表达为二维数组形式，可以用DataFrame（`Pandas`）、2darray（`Numpy`），及可以存储具有地理坐标属性几何对象（点（Point），线（Line）或面（Polygon））的GeoDataFrame（`GeoPandas`）表述。为了发现地理空间数据隐藏的规律和联系，一般可以通过聚类（Clustering）的方法分析一类或多类特征数据集聚后各簇具有的特征及分布特点。聚类的方式包括考虑地理空间位置、空间权重的方式；也可以直接聚类特征数据，分析簇分布的模式。例如聚类具有相似城市化特征的全球城市模式分析<sup>[1]</sup>，根据物理、生物和气候地理空间属性划分大型受管理的湿地<sup>[2]</sup>，城市生物物理组成对城市地表温度热岛群的影响<sup>[3]</sup>，分区（zoning）与土地利用不匹配的机制<sup>[4]</sup>，使用自组织特征映射神经网络模型聚类城市多功能景观<sup>[5]</sup>。聚类通常使用的方法有K-Means<sup>[6]</sup>、K-Medoids<sup>[7]</sup>、Gi（Getis-Ord）<sup>[8]</sup>、AMOEBA (Multidirectional optimal ecotope-based algorithm )<sup>[9]</sup>、Poisson’s model<sup>[10]</sup>、AGNES (AGglomerative NESting) algorithm（hierarchical clustering）<sup>[11]</sup>、ISO- DATA (Iterative Self-Organising Data Analysis Technique) <sup>[12]</sup>、Moran's Index<sup>[13，14]</sup>、object-bsed segmentation<sup>[15]</sup>等。通常聚类使用的库为[scikit-learn](https://scikit-learn.org/stable/#)<sup>①</sup>下`Classification`模块集成的多种聚类算法，及[PySAL](https://pysal.org/)<sup>②</sup>集成的空间自相关分析等。

对城市地理空间数据执行聚类之后获得簇分布，通过对簇集聚特征、结构和形成机制的分析，可以发现城市内在规律，找到形成问题和解决问题的途径。同时，如果可以将聚类后的簇分类或划分等级，将可能的影响因素（单个或多个特征数据）与簇分类进行（空间）相关性分析，并建立回归模型，则可以探测多个影响因素的贡献度（影响簇分类的重要性，权重）排序，且可以用于簇分类的预测，对城市结构调整的合理性给予预判，或找到调整的合理模式。

聚类是寻找城市模式有效的分析途径，簇本身就表征了类似特征的集聚，从而可以根据簇总结各类特征的模式，例如土地利用（Land Use，LU ）、土地覆盖（Land Cover，LC）、地表温度热岛效应、景观格局、开放空间、相关不平等性（不均衡性）、声环境等各类城市特征。城市特征数据包括一维到多维数据，并对位着地理空间坐标，例如表征土地覆盖的栅格数据，每一个栅格为一个土地利用类型；或高程数据，每一个栅格为一个高程值；或碳存储和固持、生境质量、生态系统服务等特征数据，均每一栅格存储一个特征值。应用聚类时，最直接的方式就是喂入具有地理空间坐标的特征数据，分析簇结果。因为这些特征每一位置（栅格单元）为一个值，因此可以直接比较不同位置值（含空间权重或不含），例如A点与B点之间值的差值绝对值可以表征AB点间的特征变化幅度或特征距离；但是如果提取样方或不规则区域的特征，而这个样方特征并不是表示为一个值，而是一个一维或二维，甚至多维的数组，例如统计各个样方各类土地覆盖在多个数量等级划分下的频数（直方）（类/簇大小）为一个二维数组，各个样方下各个栅格单元邻接单元两两分类的频数（直方图）（共现关系）为一个一维数组，而各个样方划分为不同尺度的等级细分区域（例如四叉树，Quadtree），统计不同等级尺度下分类所属不同数量级别的频数（直方图）（层级分解）为一个二维数组，这些以样方为统计单元的各类特征算法，称之为标记特征（signature），而不同样方之间标记特征的距离（distance）/相似（不相似）度（Similarity/Dissimilarity），通常是比较一维或二维至多维数组之间的距离，其距离算法有至少40种之多。通过样方的方式，而不是栅格单元的方式，计算标记特征及其距离，是寻找城市模式，分析模式特征的又一途径。因为可以配置样方的不同尺度大小，而样方较之单个栅格更能表征城市特征结构，因此基于样方标记特征的提取和不同样方间的距离计算，更又有利于寻找城市特征模式，并进一步根据样方模式寻找最近距离，即相似特征的样方；或对分类数据执行分割，进一步融合相似样方；也可以对样方间的距离执行层次聚类（hierarchical clustering），形成样方簇，分析簇分类形成的机制等。

样方标记特征的提取如果是面向分类数据，可以使用地图分类的方法将连续值数据分为指定数量的类别后（通常指定为整数形式代表类别），一般使用`PySal`库的`mapclassify`模块集成的地图分类算法，再进行计算。

地域分区的阐释侧重于样方方式的探索，实验数据类型包括随机生成的分类数据和[NLCD（National Land Cover Database）](https://www.mrlc.gov/data/legends/national-land-cover-database-class-legend-and-description)<sup>③</sup>美国土地覆盖类型数据集。

> 对于聚类方法参考文献未局限于规划建筑领域。地域分区算法集成到了[USDA](https://richiebao.github.io/USDA_PyPI/#/)<sup>④</sup>库。


```python
# IPython extension to reload modules before executing user code.
%load_ext autoreload 
# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.
%autoreload 2 
from usda import datasets as usda_datasets
from usda import data_visualization as usda_vis
from usda import pattern_signature as usda_signature
from usda import geodata_process as usda_geodataProcess
from usda import utils as usda_utils

import mapclassify
import matplotlib.pyplot as plt
import matplotlib
import cc3d
import numpy as np
import math
import itertools
from collections import Counter
import pandas as pd

import math
from toolz import partition

import rasterio as rio
import earthpy.plot as ep
import copy
from rasterio.plot import show
from shapely.geometry import mapping
import rioxarray as rxr
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial import distance
import seaborn as sns

from scipy.cluster.hierarchy import cophenet
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import fcluster

from tqdm import tqdm
```

## 3.5.1 生成分类样本数据和地图分类算法

在探索实验阶段，使用小样本有助于算法的理解和代码书写调试，因此构建生成分类样本数据函数`generate_categorical_2darray()`<sup>[16]</sup>。生成给定大小`size`参数的2维数组，应用`mapclassify`（PySAL）库提供的`JenksCaspall`地图分类方法，分类连续值为4个分类，构建分类数组（矩阵）`X4`。

* 地图分类算法<sup>[17]</sup>

`mapclassify`模块提供了多种地图分类的算法，迁移其简短示例说明如下。


```python
np.set_printoptions(linewidth=200)

size=32*2
X_,_=usda_datasets.generate_categorical_2darray(size=size,sigma=2.5,seed=57)
X=X_[0].reshape(size,size)*100
print(f'shape={X.shape};mean={X.mean()};min={X.min()};max={X.max()}')
print(X)
```

    shape=(64, 64);mean=-0.4164102800829479;min=-141.42135371890384;max=141.42126462645655
    [[-1.20682035e+02 -1.24669256e+02 -1.32210173e+02 ... -1.35750700e+02 -1.29449959e+02 -1.24486742e+02]
     [-1.12928541e+02 -1.16222038e+02 -1.21915747e+02 ... -1.29734128e+02 -1.19581762e+02 -1.12432782e+02]
     [-9.94183817e+01 -1.01979959e+02 -1.05885383e+02 ... -1.11112007e+02 -9.30813905e+01 -8.25790131e+01]
     ...
     [-3.66612346e+00  2.22469529e-01  5.63082871e+00 ...  4.51026524e+01  5.76400064e+01  6.34867964e+01]
     [-1.30267216e+01 -8.15066985e+00 -1.05805803e-01 ...  5.96014546e+01  7.24579129e+01  7.84844755e+01]
     [-1.83131353e+01 -1.30284918e+01 -3.90484961e+00 ...  6.71943372e+01  8.00656496e+01  8.61194831e+01]]
    

查看生成的样本数据数值分布。


```python
sns.kdeplot(data=X.flatten());
```

<img src="./imgs/3_5/output_7_0.png" height='auto' width='auto' title="caDesign">


```python
fig, ax=plt.subplots(figsize=(7,7))
ax.imshow(X, cmap="gray")
ax.axis('off')
plt.show()
```


<img src="./imgs/3_5/output_8_0.png" height='auto' width='auto' title="caDesign">   



1. BoxPlot

`class mapclassify.BoxPlot`(y, hinge=1.5)：箱型图（BoxPlot）地图分类。其组距（bins）如下：

```python
bins[0] = q[0]-hinge*IQR # 异常值（离群值）
bins[1] = q[0] # (下边缘，下四分位数]
bins[2] = q[1] # (下四分位数，中位数]
bins[3] = q[2] # (中位数，上四分位数]
bins[4] = q[2]+hinge*IQR # (上四分位数，上边缘]
bins[5] = inf  # 异常值
```

其中，$q$是数组$y$的前3个四分位数，并且$IQR=q[2]-q[0]$。如果$q[2]+hinge*IQR > max(y)$，将只有 5 个类别且没有高离群值，否则将有 6 个类别和至少一个高离群值。


```python
X_BoxPlot=mapclassify.BoxPlot(X)
print(f'median={np.median(X)}')
print(X_BoxPlot)
print(X_BoxPlot.bins)
print(list(X_BoxPlot.counts))
y=X_BoxPlot.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    median=-2.818266836217331
    BoxPlot
    
         Interval        Count
    --------------------------
    (   -inf, -406.12] |     0
    (-406.12, -101.72] |  1024
    (-101.72,   -2.82] |  1024
    (  -2.82,  101.21] |  1024
    ( 101.21,  405.61] |  1024
    [-406.1221183  -101.72404543   -2.81826684  101.20800315  405.60607602]
    [0, 1024, 1024, 1024, 1024]
    


<img src="./imgs/3_5/output_10_1.png" height='auto' width='auto' title="caDesign">    


2. EqualInterval

`class mapclassify.EqualInterval`(y, k=5)：等区间分类，参数`k`为分类数。区间具有相等宽度，$bins_j = min(y)+w*(j+1)$，式中$w=\frac{max(y)-min(y)}{k}$。


```python
X_EqualInterval=mapclassify.EqualInterval(X.flatten(),k=5)
print(X_EqualInterval)
y=X_EqualInterval.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    EqualInterval
    
         Interval        Count
    --------------------------
    [-141.42,  -84.85] |  1243
    ( -84.85,  -28.28] |   557
    ( -28.28,   28.28] |   551
    (  28.28,   84.85] |   521
    (  84.85,  141.42] |  1224
    


<img src="./imgs/3_5/output_12_1.png" height='auto' width='auto' title="caDesign">    

3. FisherJenks

`class mapclassify.FisherJenks`(y, k=5)：基于均值，Fisher Jenks 最优分类器，也称为Jenks自然中断分类方法（Jenks natural breaks classification method），是一种数据聚类方法（类似K-Means聚类），旨在确定值在不同类别中的最佳排列。 这是通过寻求最小化每个类与类均值的平均偏差，同时最大化每个类与其他类均值的偏差来实现的，是寻求减少类内方差并最大化类间方差的方法。


```python
X_FisherJenks=mapclassify.FisherJenks(X.flatten(),k=5)
print(X_FisherJenks)
y=X_FisherJenks.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    FisherJenks
    
         Interval        Count
    --------------------------
    [-141.42,  -94.54] |  1121
    ( -94.54,  -29.55] |   672
    ( -29.55,   39.07] |   653
    (  39.07,  100.37] |   623
    ( 100.37,  141.42] |  1027
    


<img src="./imgs/3_5/output_14_1.png" height='auto' width='auto' title="caDesign">    



4. FisherJenksSampled 

`class mapclassify.FisherJenksSampled`(y, k=5, pct=0.1, truncate=True)：使用随机样本，基于均值，Fisher Jenks 最优分类器。


```python
X_FisherJenksSampled=mapclassify.FisherJenksSampled(X.flatten(),k=5)
print(X_FisherJenksSampled)
y=X_FisherJenksSampled.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    FisherJenksSampled
    
         Interval        Count
    --------------------------
    [-141.42,  -95.89] |  1112
    ( -95.89,  -31.76] |   663
    ( -31.76,   35.27] |   642
    (  35.27,  101.14] |   655
    ( 101.14,  141.42] |  1024
    


<img src="./imgs/3_5/output_16_1.png" height='auto' width='auto' title="caDesign">    


5. JenksCaspall 

`class mapclassify.JenksCaspall`(y, k=5)：Jenks-Caspall 地图分类。类似K-Medians聚类方法。


```python
X_JenksCaspall=mapclassify.JenksCaspall(X.flatten(),k=5)
print(X_JenksCaspall)
y=X_JenksCaspall.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    JenksCaspall
    
         Interval        Count
    --------------------------
    [-141.42, -102.66] |  1002
    (-102.66,  -36.39] |   728
    ( -36.39,   37.96] |   707
    (  37.96,  103.40] |   655
    ( 103.40,  141.42] |  1004
    


<img src="./imgs/3_5/output_18_1.png" height='auto' width='auto' title="caDesign">    


6. JenksCaspallForced

`class mapclassify.JenksCaspallForced`(y, k=5)：具有强制移动（forced movements）的Jenks-Caspall 地图分类。


```python
X_JenksCaspallForced=mapclassify.JenksCaspallForced(X.flatten(),k=5)
print(X_JenksCaspallForced)
y=X_JenksCaspallForced.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    JenksCaspallForced
    
         Interval        Count
    --------------------------
    [-141.42, -113.92] |   820
    (-113.92,  -46.34] |   819
    ( -46.34,   40.90] |   819
    (  40.90,  117.88] |   819
    ( 117.88,  141.42] |   819
    


<img src="./imgs/3_5/output_20_1.png" height='auto' width='auto' title="caDesign">    



7. JenksCaspallSampled

`class mapclassify.JenksCaspallSampled`(y, k=5, pct=0.1)：使用随机样本，Jenks-Caspall 地图分类。


```python
X_JenksCaspallSampled=mapclassify.JenksCaspallSampled(X.flatten(),k=5)
print(X_JenksCaspallSampled)
y=X_JenksCaspallSampled.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    JenksCaspallSampled
    
         Interval        Count
    --------------------------
    [-141.42, -112.26] |   853
    (-112.26,  -47.87] |   769
    ( -47.87,   35.27] |   795
    (  35.27,  103.96] |   684
    ( 103.96,  141.42] |   995
    


<img src="./imgs/3_5/output_22_1.png" height='auto' width='auto' title="caDesign">    



8. HeadTailBreaks

`class mapclassify.HeadTailBreaks`(y)：适合于重尾分布（Heavy-tailed Distributions）数据的头尾断点（Head/tail Breaks）地图分类。重尾分布是尾部没有指数限制的概率分布，通常较小的值数量分布较多，而位于头部较大的值数量较少。


```python
X_HeadTailBreaks=mapclassify.HeadTailBreaks(X.flatten())
print(X_HeadTailBreaks)
y=X_HeadTailBreaks.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    HeadTailBreaks
    
         Interval        Count
    --------------------------
    [-141.42,   -0.42] |  2063
    (  -0.42,   90.99] |   915
    (  90.99,  126.10] |   447
    ( 126.10,  136.26] |   278
    ( 136.26,  139.50] |   170
    ( 139.50,  140.73] |    99
    ( 140.73,  141.18] |    53
    ( 141.18,  141.34] |    30
    ( 141.34,  141.39] |    17
    ( 141.39,  141.41] |     9
    ( 141.41,  141.42] |     6
    ( 141.42,  141.42] |     2
    ( 141.42,  141.42] |     3
    ( 141.42,  141.42] |     2
    ( 141.42,  141.42] |     1
    ( 141.42,  141.42] |     1
    


<img src="./imgs/3_5/output_24_1.png" height='auto' width='auto' title="caDesign">    


9. MaxP

`class mapclassify.MaxP`(y, k=5, initial=1000, seed1=0, seed2=1)：MaxP 地图分类。基于Max-p区域化算法（regionalization algorithm）。MaxP将一组地理区域聚类成最大数量的同质区域，使得空间广泛区域属性（spatially extensive regional attribute）的值高于预定义的阈值<sup>[18]</sup>。


```python
X_MaxP=mapclassify.MaxP(X.flatten(),k=5,initial=50)
print(X_MaxP)
y=X_MaxP.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    MaxP
    
         Interval        Count
    --------------------------
    [-141.42, -114.24] |   816
    (-114.24,   40.83] |  1641
    (  40.83,   47.62] |    46
    (  47.62,  118.35] |   779
    ( 118.35,  141.42] |   814
    


<img src="./imgs/3_5/output_26_1.png" height='auto' width='auto' title="caDesign">    


10. MaximumBreaks

`class mapclassify.MaximumBreaks`(y, k=5, mindiff=0)：最大断点地图分类。类边界由值的最大梯度确定，分布不均但不偏向任何一端。


```python
X_MaximumBreaks=mapclassify.MaximumBreaks(X.flatten(),k=5)
print(X_MaximumBreaks)
y=X_MaximumBreaks.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    MaximumBreaks
    
         Interval        Count
    --------------------------
    [-141.42,  -51.45] |  1592
    ( -51.45,   30.88] |   779
    (  30.88,   36.66] |    53
    (  36.66,  100.73] |   645
    ( 100.73,  141.42] |  1027
    


<img src="./imgs/3_5/output_28_1.png" height='auto' width='auto' title="caDesign">    


11. NaturalBreaks

`class mapclassify.NaturalBreaks`(y, k=5, initial=10)：自然断点地图分类。基本同FisherJenks算法，应用的是K-Means聚类。


```python
X_NaturalBreaks=mapclassify.NaturalBreaks(X.flatten(),k=5)
print(X_NaturalBreaks)
y=X_NaturalBreaks.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    NaturalBreaks
    
         Interval        Count
    --------------------------
    [-141.42,  -95.06] |  1118
    ( -95.06,  -30.30] |   671
    ( -30.30,   38.51] |   652
    (  38.51,  100.37] |   628
    ( 100.37,  141.42] |  1027
    


<img src="./imgs/3_5/output_30_1.png" height='auto' width='auto' title="caDesign">    


12. Quantiles 

`class mapclassify.Quantiles`(y, k=5)：分位数地图分类。分位数（Quantile），亦称分位点，是将一个随机变量的概率分布范围分为几个等份的数值点，常用的有中位数（即二分位数）、四分位数、百分位数等。


```python
X_Quantiles=mapclassify.Quantiles(X.flatten(),k=5)
print(X_Quantiles)
y=X_Quantiles.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    Quantiles
    
         Interval        Count
    --------------------------
    [-141.42, -113.92] |   820
    (-113.92,  -46.34] |   819
    ( -46.34,   40.90] |   819
    (  40.90,  117.88] |   819
    ( 117.88,  141.42] |   819
    


<img src="./imgs/3_5/output_32_1.png" height='auto' width='auto' title="caDesign">    


13. Percentiles

`class mapclassify.Percentiles`(y, pct=[1, 10, 50, 90, 99, 100])：百分位数地图分类。基本同分位数，参数`pct`可以以百分位数的形式传入自定义分隔点。


```python
X_Percentiles=mapclassify.Percentiles(X.flatten(),[0,25,50,75,100])
print(X_Percentiles)
y=X_Percentiles.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    Percentiles
    
         Interval        Count
    --------------------------
    [-141.42, -141.42] |     1
    (-141.42, -101.72] |  1023
    (-101.72,   -2.82] |  1024
    (  -2.82,  101.21] |  1024
    ( 101.21,  141.42] |  1024
    


<img src="./imgs/3_5/output_34_1.png" height='auto' width='auto' title="caDesign">    


14. StdMean

`class mapclassify.StdMean`(y, multiples=[-2, -1, 1, 2])：标准偏差和平均地图分类。参数`multiples`是要从样本均值中添加/减去的标准差的倍数以定义组距（bins），即对原始数据标准化之后，根据距离样本均值的不同标准差范围来划分数据。


```python
X_StdMean=mapclassify.StdMean(X.flatten())
print(X_StdMean)
y=X_StdMean.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    StdMean
    
         Interval        Count
    --------------------------
    (   -inf, -201.92] |     0
    (-201.92, -101.17] |  1033
    (-101.17,  100.33] |  2035
    ( 100.33,  201.09] |  1028
    


<img src="./imgs/3_5/output_36_1.png" height='auto' width='auto' title="caDesign">    


15. UserDefined

`class mapclassify.UserDefined`(y, bins, lowest=None)：自定义组距，划分分类。


```python
X_UserDefined=mapclassify.UserDefined(X.flatten(),[-50,50])
print(X_UserDefined)
y=X_UserDefined.yb.reshape(size,size)
usda_vis.imshow_label2darray(y,figsize=(7,7),random_seed=77,fontsize=6) 
```

    UserDefined
    
         Interval        Count
    --------------------------
    [-141.42,  -50.00] |  1605
    ( -50.00,   50.00] |   914
    (  50.00,  141.42] |  1577
    


<img src="./imgs/3_5/output_38_1.png" height='auto' width='auto' title="caDesign">    


16. greedy

`mapclassify.greedy`(gdf, strategy='balanced', balance='count', min_colors=4, sw='queen', min_distance=None, silence_warnings=True, interchange=False)：为GeoDataFrame地理空间数据着色，使用多种贪婪拓扑（greedy/topological）着色策略。为图论（复杂网络）中的着色图问题，尝试使用尽可能少的颜色为GeoDataFrame地理空间数据着色，使得没有邻接单元与自身具有相同的颜色。

## 3.5.2 模式标记特征（Pattern Signature）

基于单个栅格单元（像素）聚类后的簇，或者用于回归模型构建识别某一特征地理空间类型的分类器（例如识别高容积率、高密度的城市中心），往往忽略了代表某一特征的栅格单元所在的环境（context），无法捕捉到复杂的空间模式，从而降低了模式识别的准确性。因此Ranga Raju Vatsavai提出使用连续像素组（groups of contiguous pixels）（图像块（image patches））的高斯分布（Gaussian distribution）为单元特征，即基于对象（Object-based）的分类方案<sup>[19]</sup>。对于连续像素组的描述有网格（grids）、块（blocks）、斑块（patches）、方形（square（kernal））、瓦片（tile）或者场景（scene）等，为了便于阐释，统一解释为样方（quadrat）。描述样方（2维矩阵）特征的方法有多种，例如可以表述为一个值的景观指数，例如形状指数（Shape index）、分形维数（Fractal dimension）、紧凑度（compactness）等；及表述为多维度的多变量高斯分布（multi-variate Gaussian distribution），其公式为：$p(x |  y_{i} )= \frac{1}{ \sqrt{ (2 \pi )^{-N} |  \sum_j  |  } }  e^{ \frac{-1}{2} (x-  \mu _{j} )^{t}    | \sum_j  | ^{-1}(x-  \mu _{j})   }     $，式中，$\mu$为均值;$\sum$为协方差矩阵。在[GeoPAT](https://github.com/Nowosad/geopat2)<sup>⑤</sup>软件中，Jarosław Jasiewicz等人继承了样方为单元提取标记特征的方法，并集成了多种多维标记特征算法，例如类/簇大小直方图 （class/clump-size histogram），共现关系（Co-occurrence）和层级分解（Hierarchical decomposition）等<sup>[20，21]</sup>。

GeoPAT模块由ANSI C 编写，旨在嵌入[GRASS GIS](https://grass.osgeo.org/)<sup>⑥</sup>软件环境，为了在Python中实现模式分析，将样方标记特征的算法写入到`USDA`包中。用Python重写算法时，先使用小样本数据（为一个样方），通过`generate_categorical_2darra()`方法生成随机数组，并采样`FisherJenks`地图分类方法将随机数组分为4类。


```python
size=16
X,_=usda_datasets.generate_categorical_2darray(size=size,seed=7)
X4=mapclassify.FisherJenks(X[0], k=4).yb.reshape(size,size)+1
usda_vis.imshow_label2darray(X4,figsize=(7,7),random_seed=29,fontsize=10) 
```


<img src="./imgs/3_5/output_41_0.png" height='auto' width='auto' title="caDesign">    


### 3.5.2.1 连通域标签（connected components labeling（CCL））

在计算类/簇大小直方图标记特征时，需要计算分类单元的连通域，就是将各个分类按照是否连通细分标记为单独的标签。连通域标签的计算使用[connected-components-3d](https://github.com/seung-lab/connected-components-3d/)<sup>⑦</sup>库提供的`connected_components`方法，该方法提供有三维数组（26、18或6）和二维数组（4或8）连通域计算。该方法使用了A. Rosenfeld和J. Pfaltz用Union-Find增强两遍法（two pass method）的三维变体（3D variant）<sup>[22]</sup>和基于K. Wu等2维度8向连通（2D 8-connected）决策树<sup>[23]</sup>。同时，基于区域增长分割算法<sup>[24]</sup>，迁移、更新类`Categorical_data_region_growing`，实现2维连通域标签计算。两种方式实现的连通域标签结果一致。


```python
clump_labels,N=cc3d.connected_components(X4,connectivity=8,return_N=True,out_dtype=np.uint64) 
print('-'*10,N)
usda_vis.imshow_label2darray(clump_labels,figsize=(7,7),random_seed=29,fontsize=10) 
```

    ---------- 37
    


<img src="./imgs/3_5/output_43_1.png" height='auto' width='auto' title="caDesign">    



```python
CCL_X4=usda_signature.Categorical_data_region_growing(X4,1)
CCL_X4.ApplyRegionGrow()
usda_vis.imshow_label2darray(CCL_X4.SEGS,figsize=(7,7),random_seed=29,fontsize=10) 
```


<img src="./imgs/3_5/output_44_0.png" height='auto' width='auto' title="caDesign">    



### 3.5.2.2 类/簇大小直方图 （class/clump-size histogram）

类/簇大小直方图样方标记特征，是以连通域标签为区域统计分类频数，对于同一分类通常存在有多个频数，因此需要离散化（discretize ）频数值，以2为基的幂（例如1-2，2-4，3-6、4-8等）为组距进行分层，统计各个分类多个频数所属组距，如位于同一组距下则归为一个分层，并计算所属分层栅格数总和（即各组距下各分类栅格单元的数量）<sup>[26]</sup>。定义`class_clumpSize_histogram()`函数计算 类/簇大小直方图，返回值为DataFrame数据格式，其列为分类标签，行为簇大小。


```python
class_clumpSize_histogram_X4=usda_signature.class_clumpSize_histogram(X4,clump_labels)   
class_clumpSize_histogram_X4
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>class</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
    <tr>
      <th>inds</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>8</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>7</td>
      <td>10</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>4</td>
      <td>6</td>
      <td>13</td>
    </tr>
    <tr>
      <th>4</th>
      <td>15</td>
      <td>18</td>
      <td>35</td>
      <td>8</td>
    </tr>
    <tr>
      <th>5</th>
      <td>57</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>66</td>
    </tr>
  </tbody>
</table>
</div>



为了方便查看2维矩阵的数值分布情况，定义`histogram_3d()`方法，显示类/簇大小直方图标记特征。从图中可以发现，分类1位于各个以2的幂为组距的各个离散化分层下的1、2、4和5组距标签下有值，且组距标签5下具有较高值；分类4，则在组距标签7，即$[ 2^{6},   2^{7} )$组距内具有较高的簇大小。


```python
usda_vis.histogram_3d(class_clumpSize_histogram_X4,xlabel='clump-size',ylabel='class',zlabel='frequency',roll=-30,figsize=(8,8),zoom=0.9)    
```


<img src="./imgs/3_5/output_48_0.png" height='auto' width='auto' title="caDesign">    



### 3.5.2.3  共现关系（Co-occurrence）

共现关系标记特征是计算样方内各个栅格单元邻接单元对（8个方向）（（邻接事件/关系，adjacency-event））所属两两分类值的频数。对于具有$k$个分类的样方，分类两两组合的数量为$( k^{2}+k )/2$对，实验数据含4个分类，因此总共对数为10对，其中不同分类组合的对数为`( k^{2}-k )/2`，即6对。共现关系标记特征反映了高复杂性分类数据空间结构<sup>[25]</sup>。


```python
class_pairs_frequency=usda_signature.class_co_occurrence(X4)
class_pairs_frequency.sort_index(inplace=True)
class_pairs_frequency
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(1, 1)</th>
      <td>182.0</td>
    </tr>
    <tr>
      <th>(1, 2)</th>
      <td>83.0</td>
    </tr>
    <tr>
      <th>(1, 3)</th>
      <td>39.0</td>
    </tr>
    <tr>
      <th>(1, 4)</th>
      <td>91.0</td>
    </tr>
    <tr>
      <th>(2, 2)</th>
      <td>34.0</td>
    </tr>
    <tr>
      <th>(2, 3)</th>
      <td>58.0</td>
    </tr>
    <tr>
      <th>(2, 4)</th>
      <td>49.0</td>
    </tr>
    <tr>
      <th>(3, 3)</th>
      <td>80.0</td>
    </tr>
    <tr>
      <th>(3, 4)</th>
      <td>111.0</td>
    </tr>
    <tr>
      <th>(4, 4)</th>
      <td>203.0</td>
    </tr>
  </tbody>
</table>
</div>



打印1维数组的直方图，可以发现，(2,2)即分类2和自身成对在样方中各个栅格单元邻接关系出现的频数较低，从分类地图中也可以观察到，分类2较为分散；而(4,4)对，具有最高值，从分类地图中也可以观察到具有大面积的集聚。


```python
class_pairs_frequency.reset_index().plot.bar(x='index', y=0, rot=0,figsize=(6,3.5),xlabel='class pairs');
```


<img src="./imgs/3_5/output_52_0.png" height='auto' width='auto' title="caDesign">    



### 3.5.2.4  层级分解（Hierarchical decomposition）

层级分解标记特征是将样方（方形）类似四叉树（Quadtree）层层分解。对于任一层级，每一子样方的高宽为$w= 2^{i} $，式中$i=2, \cdots ,D$。当只有一个子样方（同样方）时，即其宽度为样方的宽度，为最大层级，对于实验数据样方宽度为16，则$2^{D}=16 $，求得$D=4$。那么，可以推断出对于宽度为16的样方总共可以划分为3各层级，分别为2、3和4。对于每一层级，统计各个子样方中每个分类的百分比（各分类的栅格数占子样方的比例）。类似类/簇大小直方图标记特征离散化处理方法，将分类百分比值按照小于1/4为值1，在1/4到1/2为值2，大于1/2为值3重分类，再计算所有子样方各个分类重分类的频数<sup>[27，20]</sup>。可知直方图的总个数为$3 \times k \times (D-1)$，实验数据则为28个，因为层级4（对应返回值一级索引8）中没有高于1/2的比例，因此实际直方图总数为24个。定义`class_decomposition()`方法计算层级分解标记特征，返回值为一个DataFrame格式数据，其列为分类名，其行为层级索引，一级索引代表层级，层级的表达方式为子图的宽度。


```python
class_hierarchical_decomposition=usda_signature.class_decomposition(X4)    
class_hierarchical_decomposition
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3" valign="top">2</th>
      <th>1</th>
      <td>31</td>
      <td>39.0</td>
      <td>34</td>
      <td>25.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18</td>
      <td>24.0</td>
      <td>23</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15</td>
      <td>1.0</td>
      <td>7</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">4</th>
      <th>1</th>
      <td>8</td>
      <td>12.0</td>
      <td>9</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>3.0</td>
      <td>5</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1.0</td>
      <td>2</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">8</th>
      <th>1</th>
      <td>1</td>
      <td>4.0</td>
      <td>2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0.0</td>
      <td>2</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
</div>



从图中可以发现，随着层级的增加，各分类比例重分类的频数逐渐降低，在较低的层级一级索引为2下，其中分类2所在的(2,3)，即比例大于1/2的频数最少。


```python
usda_vis.histogram_3d(class_hierarchical_decomposition,xlabel='nested quadrants',ylabel='class',zlabel='frequency',roll=-30,figsize=(8,8),zoom=0.9)   
```


<img src="./imgs/3_5/output_56_0.png" height='auto' width='auto' title="caDesign">    


## 3.5.3 距离度量（Distance meatrics）

如果为了计算样方的分类、聚类和检索等模式识别问题，而样方的标记特征为直方图（Histogram），例如类/簇大小、共现关系或层级分解，则需要适用于比较两个概率密度函数（probability density functions，pdf）距离（distance）/（不）相似度（similarity）的计算方法。自20世纪以来，许多不同领域涌现处大量距离度量的方法，这里摘录Sung-Hyuk Cha <sup>[28]</sup> 对距离度量的分类方式如下表格（保留了该研究论文的公式编号）：

| 序号  | 名称  | 公式|  编号 |
|---|---|---|---|
| **1-类：** $L_{p}$ Minkowski family|   |   |   |
| 1  | Euclidean $L_{2}$  | $$\quad d_{E u c}=\sqrt{\sum_{i=1}^{d}\left\|P_{i}-Q_{i}\right\|^{2}}$$  |  (1) |
| 2 | City block $L_{1}$  | $$\quad d_{C B}=\sum_{i=1}^{d}\left\|P_{i}-Q_{i}\right\|$$  | (2)  |
| 3  |  Minkowski $L_{\mathrm{p}}$ | $$\quad d_{M k}=\sqrt[p]{\sum_{i=1}^{d}\left\|P_{i}-Q_{i}\right\|^{p}}$$ | (3)  |
| 4  | Chebyshev $L_{\infty}$  | $$ \quad d_{\text {Cheb }}=\max _{i}\left\|P_{i}-Q_{i}\right\|$$  |  (4) |
| **2-类：** $L_{1}$ family |   |   |   |
| 5  | Sørensen  | $$d_{s o r}=\frac{\sum_{i=1}^{d}\left\|P_{i}-Q_{i}\right\|}{\sum_{i=1}^{d}\left(P_{i}+Q_{i}\right)}$$  | (5)  |
|  6 | Gower  | $$\quad d_{\text {gow }}=\frac{1}{d} \sum_{i=1}^{d} \frac{\left\|P_{i}-Q_{i}\right\|}{R_{i}}$$ $$=\frac{1}{d} \sum_{i=1}^{d}\left\|P_{i}-Q_{i}\right\|$$  | (6)</br>(7)  |
|  7 | Soergel  | $$d_{s g}=\frac{\sum_{i=1}^{d}\left\|P_{i}-Q_{i}\right\|}{\sum_{i=1}^{d} \max \left(P_{i}, Q_{i}\right)}$$  | (8)  |
|  8 |  Kulczynski $d$ |  $$ \quad d_{k u l}=\frac{\sum_{i=1}^{d}\left\|P_{i}-Q_{i}\right\|}{\sum_{i=1}^{d} \min \left(P_{i}, Q_{i}\right)}$$ | (9)  |
| 9  | Canberra  | $$\quad d_{\text {Can }}=\sum_{i=1}^{d} \frac{\left\|P_{i}-Q_{i}\right\|}{P_{i}+Q_{i}}$$  |  (10) |
| 10  | Lorentzian  |  $$\quad d_{L o r}=\sum_{i=1}^{d} \ln \left(1+\left\|P_{i}-Q_{i}\right\|\right)$$ | (11)  |
|   |   | * $L_{1}$ family $\supset\{$ Intersectoin (13), Wave Hedges (15), Czekanowski (16), Ruzicka (21), Tanimoto (23), etc $\}$.  |   |
| **3-类：**  Intersection family  |   |   |   |
| 11  | Intersection   | $$ S_{IS} =\sum_{i=1}^d \min \left(P_i, Q_i\right)$$  |  (12) |
|   |   |  $$d_{\text {non }-I S}=1-S_{I S}=\frac{1}{2} \sum_{i=1}^d\left\|P_i-Q_i\right\|$$ |  (13) |
| 12  | Wave Hedges   | $$\quad d_{W H}=\sum_{i=1}^d\left(1-\frac{\min \left(P_i, Q_i\right)}{\max \left(P_i, Q_i\right)}\right)$$ $$=\sum_{i=1}^d \frac{\left\|P_i-Q_i\right\|}{\max \left(P_i, Q_i\right)} $$ | (14)</br>(15)  |
|  13 |  Czekanowski  |  $$S_{Cze}=\frac{2 \sum_{i=1}^d \min \left(P_i, Q_i\right)}{\sum_{i=1}^d\left(P_i+Q_i\right)}$$ |  (16) |
|   |   |  $$d_{Cze}=1-S_{Cze}=\frac{\sum_{i=1}^d\left\|P_i-Q_i\right\|}{\sum_{i=1}^d\left(P_i+Q_i\right)}$$ | (17)  |
|  14 |  Motyka   | $$S_{M o t}=\frac{\sum_{i=1}^d \min \left(P_i, Q_i\right)}{\sum_{i=1}^d\left(P_i+Q_i\right)}$$  | (18)  |
|   |   | $$d_{\text {Mot }}=1-S_{\text {Mot }}=\frac{\sum_{i=1}^d \max \left(P_i, Q_i\right)}{\sum_{i=1}^d\left(P_i+Q_i\right)} $$  | (19)  |
|  15 | Kulczynski $s$  |$$S_{Kul}=\frac{1}{d_{K ul}}=\frac{\sum_{i=1}^d \min \left(P_i, Q_i\right)}{\sum_{i=1}^d\left\|P_i-Q_i\right\|} $$   |  (20) |
| 16  | Ruzicka   |  $$S_{\text {Ruz }}=\frac{\sum_{i=1}^d \min \left(P_i, Q_i\right)}{\sum_{i=1}^d \max \left(P_i, Q_i\right)} $$ | (21)  |
|  17 | Tanimoto  |  $$d_{\text {Tani }}=\frac{\sum_{i=1}^d P_i+\sum_{i=1}^d Q_i-2 \sum_{i=1}^d \min \left(P_i, Q_i\right)}{\sum_{i=1}^d P_i+\sum_{i=1}^d Q_i-\sum_{i=1}^d \min \left(P_i, Q_i\right)}$$ |  (22) |
|   |   | $$=\frac{\sum_{i=1}^d\left(\max \left(P_i, Q_i\right)-\min \left(P_i, Q_i\right)\right)}{\sum_{i=1}^d \max \left(P_i, Q_i\right)}$$  | (23)  |
|  **4-类：**  Inner Product family |   |   |   |
| 18  | Inner Product   |  $$S_{I P}=P \bullet Q=\sum_{j=1}^d P_i Q_i$$ | (24)  |
| 19  |Harmonic  mean  | $$S_{H M}=2 \sum_{i=1}^d \frac{P_i Q_i}{P_i+Q_i}$$  | (25)  |
| 20  | Cosine  |  $$S_{\operatorname{Cos}}=\frac{\sum_{i=1}^d P_i Q_i}{\sqrt{\sum_{i=1}^d P_i^2} \sqrt{\sum_{i=1}^d Q_i^2}}$$ |  (26) |
| 21  |  Kumar-Hassebrook (PCE)  | $$S_{\text {Jac }}=\frac{\sum_{i=1}^{d} P_{i} Q_{i}}{\sum_{i=1}^{d} P_{i}^{2}+\sum_{i=1}^{d} Q_{i}^{2}-\sum_{i=1}^{d} P_{i} Q_{i}}$$  |  (27) |
| 22  | Jaccard  | $$S_{\text {Jac }}=\frac{\sum_{i=1}^{d} P_{i} Q_{i}}{\sum_{i=1}^{d} P_{i}^{2}+\sum_{i=1}^{d} Q_{i}^{2}-\sum_{i=1}^{d} P_{i} Q_{i}}$$  | (28)  |
|   |   | $$d_{\text {Jac }}=1-S_{\text {Jac }}=\frac{\sum_{i=1}^{d}\left(P_{i}-Q_{i}\right)^{2}}{\sum_{i=1}^{d} P_{i}^{2}+\sum_{i=1}^{d} Q_{i}^{2}-\sum_{i=1}^{d} P_{i} Q_{i}}$$  | (39)  |
| 23  | Dice  | $$S_{\text {Dice }}=\frac{2 \sum_{i=1}^d P_i Q_i}{\sum_{i=1}^d P_i^2+\sum_{i=1}^d Q_i^2} $$  | (40)  |
|   |   | $$d_{\text {Dice }}=1-S_{\text {Dice }}=\frac{\sum_{i=1}^d\left(P_i-Q_i\right)^2}{\sum_{i=1}^d P_i^2+\sum_{i=1}^d Q_i^2}$$  | (31)  |
| **5-类：**  Fidelity family or Squared-chord family  |   |   |   |
|  24 | Fidelity   | $$\quad S_{F i d}=\sum_{i=1}^{d} \sqrt{P_{i} Q_{i}}$$  | (32)  |
| 25  | Bhattacharyya  | $$d_{B}=-\ln \sum_{i=1}^{d} \sqrt{P_{i} Q_{i}}$$  |  (33) |
| 26  | Hellinger   |  $$d_{H}=\sqrt{2 \sum_{i=1}^{d}\left(\sqrt{P_{i}}-\sqrt{Q_{i}}\right)^{2}} $$  $$=2 \sqrt{1-\sum_{i=1}^{d} \sqrt{P_{i} Q_{i}}}$$| (34)</br>(35)  |
| 27  |   Matusita | $$d_{M}=\sqrt{\sum_{i=1}^{d}\left(\sqrt{P_{i}}-\sqrt{Q_{i}}\right)^{2}}$$ $$=\sqrt{2-2 \sum_{i=1}^{d} \sqrt{P_{i} Q_{i}}}$$ | (36)</br>(37)  |
| 28 | Squared-chord   | $$d_{s q c}=\sum_{i=1}^{d}\left(\sqrt{P_{i}}-\sqrt{Q_{i}}\right)^{2}$$  |  (38) |
|   |  $$S_{s q c}=1-d_{s q c} $$|  $$S_{s q c}=2 \sum_{i=1}^d \sqrt{P_i Q_i}-1$$ | (39)  |
|  **6-类：**  Squared $L_{2} $ family or $\chi^{2}$  family|   |   |   |
| 29  |Squared Euclidean   |  $$d_{s q e}=\sum_{i=1}^d\left(P_i-Q_i\right)^2$$ | (40)  |
| 30  | Pearson  $\chi^2$ | $$d_P(P, Q)=\sum_{i=1}^d \frac{\left(P_i-Q_i\right)^2}{Q_i}$$  | (41)  |
| 31  |Neyman  $\chi^2$ | $$d_N(P, Q)=\sum_{i=1}^d \frac{\left(P_i-Q_i\right)^2}{P_i}$$  | (42)  |
| 32  |  Squared $\chi^2$  |  $$d_{S q C h i}=\sum_{i=1}^d \frac{\left(P_i-Q_i\right)^2}{P_i+Q_i}$$ | (43)  |
| 33  | Probabilistic Symmetric $\chi^2$ |  $$ d_{P C h i i}=2 \sum_{i=1}^d \frac{\left(P_i-Q_i\right)^2}{P_i+Q_i}$$ |  (44) |
|  34 |  Divergence | $$ d_{D_{i v}}=2 \sum_{i=1}^d \frac{\left(P_i-Q_i\right)^2}{\left(P_i+Q_i\right)^2}$$  |  (45) |
| 35  | Clark  |  $$d_{C l k}=\sqrt{\sum_{i=1}^d\left(\frac{\left\|P_i-Q_i\right\|}{P_i+Q_i}\right)^2} $$ | (46)  |
| 36  |  Additive Symmetric $\chi^2$   |  $$d_{\text {AdChi }}=\sum_{i=1}^b \frac{\left(P_i-Q_i\right)^2\left(P_i+Q_i\right)}{P_i Q_i}$$ |  (47) |
|   |   | * Squared $L_{2} $ family ⊃ {Jaccard (29), Dice (31)}   |   |
|  **7-类：**  Shannon’s entropy family |   |   |   |
| 37  | Kullback-Leibler  | $$d_{K L}=\sum_{i=1}^d P_i \ln \frac{P_i}{Q_i} $$  |  (48) |
| 38  | Jeffreys   | $$d_J=\sum_{i=1}^d\left(P_i-Q_i\right) \ln \frac{P_i}{Q_i}$$  | (49)  |
| 39  | K divergence   |  $$d_{K d i v}=\sum_{i=1}^d P_i \ln \frac{2 P_i}{P_i+Q_i}$$ | (50)  |
| 40  | Topsøe   | $$d_{\text {Top }}=\sum_{i=1}^d\left(P_i \ln \left(\frac{2 P_i}{P_i+Q_i}\right)+Q_i \ln \left(\frac{2 Q_i}{P_i+Q_i}\right)\right)$$  | (51)  |
| 41 | Jensen-Shannon   | $$d_{J S}=\frac{1}{2}\left[\sum_{i=1}^d P_i \ln \left(\frac{2 P_i}{P_i+Q_i}\right)+\sum_{i=1}^d Q_i \ln \left(\frac{2 Q_i}{P_i+Q_i}\right)\right]$$  | (52)  |
| 42  |  Jensen difference  |  $$d_{J D}=\sum_{i=1}^b\left[\frac{P_i \ln P_i+Q_i \ln Q_i}{2}-\left(\frac{P_i+Q_i}{2}\right) \ln \left(\frac{P_i+Q_i}{2}\right)\right]$$ |  (53) |
|  **8-类：**   Combinations  |   |   |   |
| 43  | Taneja   | $$d_{T J}=\sum_{i=1}^{d}\left(\frac{P_{i}+Q_{i}}{2}\right) \ln \left(\frac{P_{i}+Q_{i}}{2 \sqrt{P_{i} Q_{i}}}\right)$$  | (54)  |
| 44  |   Kumar-Johnson |  $$d_{K J}=\sum_{i=1}^{d}\left(\frac{\left(P_{i}^{2}-Q_{i}^{2}\right)^{2}}{2\left(P_{i} Q_{i}\right)^{3 / 2}}\right)$$ | (55)  |
| 45  |  Avg $\left(L_{1}, L_{\infty}\right)$ |  $$d_{A C C}=\frac{\sum_{i=1}^{d}\left\|P_{i}-Q_{i}\right\|+\max _{i}\left\|P_{i}-Q_{i}\right\|}{2}$$ | (56)  |



`SciPy`库的`scipy.spatial.distance`模块集成了多种距离度量的方法，同时在`USDA`库中基本按照上述分类也集成了距离度量的方法，方便调用计算。

模式标记特征部分集成的算法返回值类型为DataFrame格式数据，且返回的为统计频数，距离度量的方法计算的为pdf之间的距离，因此需要对各值除以其总和得到表示概率分布的pdf。距离度量是比较两个样方的标记特征，因此生成同高宽的比较样方，并计算各个标记特征。


* 生成比较样本数据


```python
size=16
X_4comparison,_=usda_datasets.generate_categorical_2darray(size=size,seed=99)
X4_4comparison=mapclassify.FisherJenks(X_4comparison[0], k=4).yb.reshape(size,size)+1
usda_vis.imshow_label2darray(X4_4comparison,figsize=(7,7),random_seed=29,fontsize=10) 
```


<img src="./imgs/3_5/output_58_0.png" height='auto' width='auto' title="caDesign">    



```python
clump_labels_4comparison,_=cc3d.connected_components(X4_4comparison,connectivity=8,return_N=True,out_dtype=np.uint64) 
class_clumpSize_histogram_X4_4comparison=usda_signature.class_clumpSize_histogram(X4_4comparison,clump_labels_4comparison) 
class_pairs_frequency_4comparison=usda_signature.class_co_occurrence(X4_4comparison)
class_hierarchical_decomposition_4comparison=usda_signature.class_decomposition(X4_4comparison)   
```

* 类/簇大小距离计算

类/簇大小标记特征返回值，可能因为两组样方出现的分类值（包括分类（列），或计算特征所用分类（行））存在差异，使得矩阵大小不同，或者矩阵大小同，但列名或行索引不同，因此需要统一上述情况，定义`complete_dataframe_rowcols()`方法将差异的行列补全，并配置值为0后，再计算距离。例如该组实验数据样方`class_clumpSize_X_pdf`缺少行索引6。

对于类/簇大小标记特征距离计算，根据Jarosław Jasiewicz等人<sup>[20]</sup> 的发现，Jensen-Shannon距离具有较好的表现。


```python
print(class_clumpSize_histogram_X4.values.sum())
print(class_clumpSize_histogram_X4_4comparison.values.sum())

class_clumpSize_X_pdf=class_clumpSize_histogram_X4/class_clumpSize_histogram_X4.values.sum()
print(class_clumpSize_X_pdf.values.sum())
print(class_clumpSize_X_pdf)

class_clumpSize_X4_4comparison_pdf=class_clumpSize_histogram_X4_4comparison/class_clumpSize_histogram_X4_4comparison.values.sum()
print(class_clumpSize_X4_4comparison_pdf.values.sum())
print(class_clumpSize_X4_4comparison_pdf)


class_clumpSize_X_pdf,class_clumpSize_X4_4comparison_pdf=usda_utils.complete_dataframe_rowcols([class_clumpSize_X_pdf,class_clumpSize_X4_4comparison_pdf]) 
print('-'*50)
print(class_clumpSize_X_pdf,'\n',class_clumpSize_X4_4comparison_pdf)
```

    256
    256
    1.0
    class         1         2         3         4
    inds                                         
    1      0.011719  0.031250  0.007812  0.003906
    2      0.011719  0.027344  0.039062  0.000000
    3      0.000000  0.015625  0.023438  0.050781
    4      0.058594  0.070312  0.136719  0.031250
    5      0.222656  0.000000  0.000000  0.000000
    7      0.000000  0.000000  0.000000  0.257812
    1.0
    class         1         2         3         4
    inds                                         
    1      0.000000  0.019531  0.019531  0.000000
    2      0.000000  0.050781  0.046875  0.007812
    3      0.000000  0.062500  0.039062  0.019531
    4      0.031250  0.035156  0.031250  0.000000
    5      0.000000  0.000000  0.000000  0.242188
    6      0.132812  0.000000  0.000000  0.000000
    7      0.261719  0.000000  0.000000  0.000000
    --------------------------------------------------
    class         1         2         3         4
    1      0.011719  0.031250  0.007812  0.003906
    2      0.011719  0.027344  0.039062  0.000000
    3      0.000000  0.015625  0.023438  0.050781
    4      0.058594  0.070312  0.136719  0.031250
    5      0.222656  0.000000  0.000000  0.000000
    6      0.000000  0.000000  0.000000  0.000000
    7      0.000000  0.000000  0.000000  0.257812 
     class         1         2         3         4
    1      0.000000  0.019531  0.019531  0.000000
    2      0.000000  0.050781  0.046875  0.007812
    3      0.000000  0.062500  0.039062  0.019531
    4      0.031250  0.035156  0.031250  0.000000
    5      0.000000  0.000000  0.000000  0.242188
    6      0.132812  0.000000  0.000000  0.000000
    7      0.261719  0.000000  0.000000  0.000000
    


```python
class_clumpSize_pdf_shannon=usda_signature.Distances(class_clumpSize_X_pdf.to_numpy().flatten(),class_clumpSize_X4_4comparison_pdf.to_numpy().flatten())
class_clumpSize_pdf_shannon.shannon()
```




    {'Kull-Leiber': 24.192358176201616,
     'Jeffreys': 52.92032530411872,
     'Kdivergence': 0.4623230033047462,
     'Topsoe': 0.898443899156874,
     'Jensen-Shan': 0.449221949578437,
     'Jensen-Diff': 0.4492219495784369}



`SciPy`提供的`jensenshannon`(p, q[, base, axis, keepdims])方法计算pdf的Jensen-Shannon距离，为上述对应公式（52）的平方根，即$\sqrt{\frac{D(p \parallel m) + D(q \parallel m)}{2}}$，式中$D$为Kullback-Leibler divergence对应公式（50）。求`SciPy`计算结果的2次方值为0.449同上述`Jensen-Shan`对应的值。


```python
scipy_Jenson_Shan=distance.jensenshannon(class_clumpSize_X_pdf.to_numpy().flatten(),class_clumpSize_X4_4comparison_pdf.to_numpy().flatten())
print(scipy_Jenson_Shan)
print(math.pow(scipy_Jenson_Shan,2))
```

    0.6702402178162968
    0.44922194957843703
    

* 共现关系距离计算

共现关系标记特征根据Jarosław Jasiewicz等人发现，Wave Hedges距离度量具有较好的表现，其公式为（14）（15）。


```python
print(class_pairs_frequency.values.sum())
print(class_pairs_frequency_4comparison.values.sum())
class_pairs_frequency_pdf=class_pairs_frequency/class_pairs_frequency.values.sum()
class_pairs_frequency_4comparison_pdf=class_pairs_frequency_4comparison/class_pairs_frequency_4comparison.values.sum() 

class_pairs_frequency_pdf,class_pairs_frequency_4comparison_pdf=usda_utils.complete_dataframe_rowcols([class_pairs_frequency_pdf,class_pairs_frequency_4comparison_pdf])
print(class_pairs_frequency_pdf,'\n',class_pairs_frequency_4comparison_pdf)
```

    930.0
    930.0
                   0
    (1, 1)  0.195699
    (1, 2)  0.089247
    (1, 3)  0.041935
    (1, 4)  0.097849
    (2, 2)  0.036559
    (2, 3)  0.062366
    (2, 4)  0.052688
    (3, 3)  0.086022
    (3, 4)  0.119355
    (4, 4)  0.218280 
                    0
    (1, 1)  0.310753
    (1, 2)  0.125806
    (1, 3)  0.048387
    (1, 4)  0.061290
    (2, 2)  0.035484
    (2, 3)  0.063441
    (2, 4)  0.067742
    (3, 3)  0.026882
    (3, 4)  0.106452
    (4, 4)  0.153763
    


```python
class_pairs_pdf_wave_hedges=usda_signature.Distances(class_pairs_frequency_pdf.to_numpy().flatten(),class_pairs_frequency_4comparison_pdf.to_numpy().flatten())
class_pairs_pdf_wave_hedges.intersection()
```




    {'Intersection': 0.17419354838709675,
     'Wave Hedges': 2.52755796213251,
     'Czekanowski': 0.17419354838709677,
     'Motyka': 0.5870967741935486,
     'Ruzicka': 0.29670329670329676,
     'Tanimoto': 0.29670329670329665}



* 层级分解距离计算

层级分解标记特征根据Jarosław Jasiewicz等人发现，Jaccard距离度量具有较好的表现，其公式为（28）。


```python
print([class_hierarchical_decomposition.loc[[idx]].values.sum() for idx in [2,4,8]])

class_hierarchical_decomposition_pdf=class_hierarchical_decomposition/class_hierarchical_decomposition.values.sum()
class_hierarchical_decomposition_4comparison_pdf=class_hierarchical_decomposition_4comparison/class_hierarchical_decomposition_4comparison.values.sum()

class_hierarchical_decomposition_pdf,class_hierarchical_decomposition_4comparison_pdf=usda_utils.complete_dataframe_rowcols([class_hierarchical_decomposition_pdf,class_hierarchical_decomposition_4comparison_pdf])
print(class_hierarchical_decomposition_pdf,'\n',class_hierarchical_decomposition_4comparison_pdf)
```

    [256.0, 64.0, 16.0]
                1         2         3         4
    2 1  0.092262  0.116071  0.101190  0.074405
      2  0.053571  0.071429  0.068452  0.071429
      3  0.044643  0.002976  0.020833  0.044643
    4 1  0.023810  0.035714  0.026786  0.011905
      2  0.011905  0.008929  0.014881  0.026786
      3  0.011905  0.002976  0.005952  0.008929
    8 1  0.002976  0.011905  0.005952  0.000000
      2  0.008929  0.000000  0.005952  0.011905
      3  0.000000  0.000000  0.000000  0.000000 
                 1         2         3         4
    2 1  0.080357  0.104167  0.122024  0.104167
      2  0.035714  0.077381  0.065476  0.053571
      3  0.074405  0.008929  0.002976  0.032738
    4 1  0.017857  0.035714  0.035714  0.020833
      2  0.011905  0.011905  0.011905  0.017857
      3  0.017857  0.000000  0.000000  0.008929
    8 1  0.002976  0.011905  0.011905  0.002976
      2  0.005952  0.000000  0.000000  0.008929
      3  0.002976  0.000000  0.000000  0.000000
    


```python
class_hierarchical_decomposition_pdf_Jaccard=usda_signature.Distances(class_hierarchical_decomposition_pdf.to_numpy().flatten(),class_hierarchical_decomposition_4comparison_pdf.to_numpy().flatten())
class_hierarchical_decomposition_pdf_Jaccard.inner()
```




    {'Inner Product': 0.9352944302721089,
     'Harmonic Mean': 0.036283910083171333,
     'Cosine': 0.03048384304010754,
     'Jaccard': 0.060208413739868794,
     'Dice': 0.031038599283724627}



## 3.5.4 模式发现

### 3.5.4.1 [NLCD（National Land Cover Database）](https://www.mrlc.gov/data/legends/national-land-cover-database-class-legend-and-description)<sup>③</sup>美国土地覆盖类型数据集

NLCD是美国地质调查局（United States Geological Survey，USGS）与多分辨率土地特征联盟 (Multi-Resolution Land Characteristics Consortium，MRLC) 合作生产覆盖50 个州和波多黎各（Puerto Rico）全国一致的土地覆盖产品。目前包括2001、2004、2006、2008、2011、2013、2016和2019等多年土地覆盖可免费下载数据集，其高空分辨率为30m，包含20个分类，类别和对应的栅格值及说明如下：

|  Class/Value |   Classification Description  | recommended color value |
|---|---|---|
| **Water**  |   |   |
|  11 |  **Open Water**- areas of open water, generally with less than 25% cover of vegetation or soil.   | <span style="color:#5475a8;font-weight:bold;">#5475a8</span> |
| 12  |  **Perennial Ice/Snow**- areas characterized by a perennial cover of ice and/or snow, generally greater than 25% of total cover.   | <span style="color:#ffffff;font-weight:bold;">#ffffff</span>   |
| **Developed**  |   |   |
| 21  |  **Developed, Open Space**- areas with a mixture of some constructed materials, but mostly vegetation in the form of lawn grasses. Impervious surfaces account for less than 20% of total cover. These areas most commonly include large-lot single-family housing units, parks, golf courses, and vegetation planted in developed settings for recreation, erosion control, or aesthetic purposes.   | <span style="color:#e8d1d1;font-weight:bold;">#e8d1d1</span>   |
| 22  | **Developed, Low Intensity**- areas with a mixture of constructed materials and vegetation. Impervious surfaces account for 20% to 49% percent of total cover. These areas most commonly include single-family housing units.   | <span style="color:#e29e8c;font-weight:bold;">#e29e8c</span>  |
|  23 | **Developed, Medium Intensity** -areas with a mixture of constructed materials and vegetation. Impervious surfaces account for 50% to 79% of the total cover. These areas most commonly include single-family housing units.   |  <span style="color:#ff0000;font-weight:bold;">#ff0000</span>  |
| 24  |   **Developed High Intensity**-highly developed areas where people reside or work in high numbers. Examples include apartment complexes, row houses and commercial/industrial. Impervious surfaces account for 80% to 100% of the total cover.  | <span style="color:#b50000;font-weight:bold;">#b50000</span>   |
|  **Barren** |   |   |
| 31  | **Barren Land (Rock/Sand/Clay)** - areas of bedrock, desert pavement, scarps, talus, slides, volcanic material, glacial debris, sand dunes, strip mines, gravel pits and other accumulations of earthen material. Generally, vegetation accounts for less than 15% of total cover.   | <span style="color:#d2cdc0;font-weight:bold;">#d2cdc0</span>   |
| **Forest**   |   |   |
|  41 | **Deciduous Forest**- areas dominated by trees generally greater than 5 meters tall, and greater than 20% of total vegetation cover. More than 75% of the tree species shed foliage simultaneously in response to seasonal change.   | <span style="color:#85c77e;font-weight:bold;">#85c77e</span>   |
|  42 | **Evergreen Forest**- areas dominated by trees generally greater than 5 meters tall, and greater than 20% of total vegetation cover. More than 75% of the tree species maintain their leaves all year. Canopy is never without green foliage.   | <span style="color:#38814e;font-weight:bold;">#38814e</span>    |
|  43 |  **Mixed Forest**- areas dominated by trees generally greater than 5 meters tall, and greater than 20% of total vegetation cover. Neither deciduous nor evergreen species are greater than 75% of total tree cover.   |  <span style="color:#d4e7b0;font-weight:bold;">#d4e7b0</span>  |
|  **Shrubland** |   |   |
|  51 | **Dwarf Scrub**- Alaska only areas dominated by shrubs less than 20 centimeters tall with shrub canopy typically greater than 20% of total vegetation. This type is often co-associated with grasses, sedges, herbs, and non-vascular vegetation.    |  <span style="color:#af963c;font-weight:bold;">#af963c</span> |
|  52 |   **Shrub/Scrub**- areas dominated by shrubs; less than 5 meters tall with shrub canopy typically greater than 20% of total vegetation. This class includes true shrubs, young trees in an early successional stage or trees stunted from environmental conditions.  |   <span style="color:#dcca8f;font-weight:bold;">#dcca8f</span>  |
| **Herbaceous**  |   |   |
|  71 |  **Grassland/Herbaceous**- areas dominated by gramanoid or herbaceous vegetation, generally greater than 80% of total vegetation. These areas are not subject to intensive management such as tilling, but can be utilized for grazing.   |  <span style="color:#fde9aa;font-weight:bold;">#fde9aa</span>  |
| 72  | **Sedge/Herbaceous**- Alaska only areas dominated by sedges and forbs, generally greater than 80% of total vegetation. This type can occur with significant other grasses or other grass like plants, and includes sedge tundra, and sedge tussock tundra.    | <span style="color:#d1d182;font-weight:bold;">#d1d182</span>   |
|  73 |   **Lichens**- Alaska only areas dominated by fruticose or foliose lichens generally greater than 80% of total vegetation.  |  <span style="color:#a3cc51;font-weight:bold;">#a3cc51</span>  |
| 74  |  **Moss**- Alaska only areas dominated by mosses, generally greater than 80% of total vegetation.   | <span style="color:#82ba9e;font-weight:bold;">#82ba9e</span>    |
|  **Planted/Cultivated** |   |   |
|  81 |  **Pasture/Hay**-areas of grasses, legumes, or grass-legume mixtures planted for livestock grazing or the production of seed or hay crops, typically on a perennial cycle. Pasture/hay vegetation accounts for greater than 20% of total vegetation.   | <span style="color:#fbf65d;font-weight:bold;">#fbf65d</span>  |
| 82  |  **Cultivated Crops** -areas used for the production of annual crops, such as corn, soybeans, vegetables, tobacco, and cotton, and also perennial woody crops such as orchards and vineyards. Crop vegetation accounts for greater than 20% of total vegetation. This class also includes all land being actively tilled.  |  <span style="color:#ca9146;font-weight:bold;">#ca9146</span> |
|  **Wetlands**  |   |   |
| 90  |  **Woody Wetlands**- areas where forest or shrubland vegetation accounts for greater than 20% of vegetative cover and the soil or substrate is periodically saturated with or covered with water.  |   <span style="color:#c8e6f8;font-weight:bold;">#c8e6f8</span> |
| 95  |   **Emergent Herbaceous Wetlands**- Areas where perennial herbaceous vegetation accounts for greater than 80% of vegetative cover and the soil or substrate is periodically saturated with or covered with water.  |  <span style="color:#64b3d5;font-weight:bold;">#64b3d5</span>  |

下图为NLCD 2019年土地覆盖类型数据。

<img src="./imgs/3_5/3_5_01.jpg" height='auto' width='auto' title="caDesign">

提取NLCD 2019年土地覆盖类型旧金山（San Francisco）区域数据阐释基于样本标记特征模式的层次聚类、搜索、监测和分割。数据提取的过程为：

1. 读取遥感影像数据，获取epsg投影编号，用于经纬度提取坐标投影配置；
2. 在QGIS打开影像，或者在Googel Earth中拾取旧金山区域左下角坐标`pt_leftBottom`和右上角坐标`pt_rightTop`，用定义的`pt_coordi_transform()`函数，投影点坐标为NLCD影像投影；
3. 应用定义的`rio_read_subset()`函数，仅读取指定旧金山区域的数据；
4. 存储旧金山区域NLCD 2019年土地覆盖类型数据至本地磁盘，用于后续数据分析。


```python
nlcd_2019_lc_fn=r'E:\data\NLCD_landcover_2019_release_all_files_20210604\nlcd_2019_land_cover_l48_20210604\nlcd_2019_land_cover_l48_20210604.img'

with rio.open(nlcd_2019_lc_fn) as src:    
    landuse_array=src.read()
    transform=src.transform
    epsg_nlcd=src.crs
    
print(epsg_nlcd)
```

    PROJCS["Albers Conical Equal Area",GEOGCS["WGS 84",DATUM["WGS_1984",SPHEROID["WGS 84",6378137,298.257223563,AUTHORITY["EPSG","7030"]],AUTHORITY["EPSG","6326"]],PRIMEM["Greenwich",0,AUTHORITY["EPSG","8901"]],UNIT["degree",0.0174532925199433,AUTHORITY["EPSG","9122"]],AUTHORITY["EPSG","4326"]],PROJECTION["Albers_Conic_Equal_Area"],PARAMETER["latitude_of_center",23],PARAMETER["longitude_of_center",-96],PARAMETER["standard_parallel_1",29.5],PARAMETER["standard_parallel_2",45.5],PARAMETER["false_easting",0],PARAMETER["false_northing",0],UNIT["meters",1],AXIS["Easting",EAST],AXIS["Northing",NORTH]]
    


```python
pt_leftBottom=[-122.51764,37.46971]
pt_rightTop=[-122.06587,37.94047] 
pt_leftBottom_pj=usda_geodataProcess.pt_coordi_transform(4326,epsg_nlcd,pt_leftBottom)
pt_rightTop_pj=usda_geodataProcess.pt_coordi_transform(4326,epsg_nlcd,pt_rightTop)
```


```python
lu,transform,ras_meta=usda_geodataProcess.rio_read_subset(nlcd_2019_lc_fn,[pt_leftBottom_pj,pt_rightTop_pj])  
```


```python
san_francisco_nlcd_2019_lc_fn=r'E:\data\NLCD_landcover_2019_release_all_files_20210604\clipped\san_francisco_nlcd_2019_lc.tif'

ras_meta_=copy.deepcopy(ras_meta)
ras_meta_.update(   
        compress='lzw',
        )  
with rio.open(san_francisco_nlcd_2019_lc_fn,'w',**ras_meta_) as dst:
    dst.write(lu)
```

读取提取并存储到本地磁盘旧金山区域土地覆盖分类数据。


```python
san_francisco_nlcd_2019_lc=rxr.open_rasterio(san_francisco_nlcd_2019_lc_fn,masked=True).squeeze()
epsg_nlcd=san_francisco_nlcd_2019_lc.rio.crs
epsg_nlcd
```




    CRS.from_wkt('PROJCS["Albers_Conical_Equal_Area",GEOGCS["WGS 84",DATUM["WGS_1984",SPHEROID["WGS 84",6378137,298.257223563,AUTHORITY["EPSG","7030"]],AUTHORITY["EPSG","6326"]],PRIMEM["Greenwich",0],UNIT["Degree",0.0174532925199433]],PROJECTION["Albers_Conic_Equal_Area"],PARAMETER["latitude_of_center",23],PARAMETER["longitude_of_center",-96],PARAMETER["standard_parallel_1",29.5],PARAMETER["standard_parallel_2",45.5],PARAMETER["false_easting",0],PARAMETER["false_northing",0],UNIT["meters",1],AXIS["Easting",EAST],AXIS["Northing",NORTH]]')



定义样方网格，每一样方为计算标记特征的基本区域单元。


```python
cell_size=2000
quadrats_gdf=usda_geodataProcess.rec_quadrats_gdf(pt_leftBottom_pj,pt_rightTop_pj,cell_size,cell_size,crs=epsg_nlcd)
quadrats_gdf['coords']=quadrats_gdf['geometry'].apply(lambda x: x.representative_point().coords[:][0])
quadrats_gdf
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>geometry</th>
      <th>coords</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>POLYGON ((-2291093.439 1925411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1926411.0636174935)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>POLYGON ((-2291093.439 1927411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1928411.0636174935)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>POLYGON ((-2291093.439 1929411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1930411.0636174935)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>POLYGON ((-2291093.439 1931411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1932411.0636174935)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>POLYGON ((-2291093.439 1933411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1934411.0636174935)</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>489</th>
      <td>POLYGON ((-2241093.439 1953411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1954411.0636174935)</td>
    </tr>
    <tr>
      <th>490</th>
      <td>POLYGON ((-2241093.439 1955411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1956411.0636174935)</td>
    </tr>
    <tr>
      <th>491</th>
      <td>POLYGON ((-2241093.439 1957411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1958411.0636174935)</td>
    </tr>
    <tr>
      <th>492</th>
      <td>POLYGON ((-2241093.439 1959411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1960411.0636174935)</td>
    </tr>
    <tr>
      <th>493</th>
      <td>POLYGON ((-2241093.439 1961411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1962411.0636174935)</td>
    </tr>
  </tbody>
</table>
<p>494 rows × 2 columns</p>
</div>



配置NLCD土地覆盖分类颜色值，用于地图打印。同时叠加样方网格，观察样方覆盖情况。


```python
nlcd_class_idNcolor={
    '0':[0,"#64b3d5"],
    'Open_Water':[11,"#5475a8"],
    'Perennial_IceSnow':[12,"#ffffff"],
    'Developed_Open_Space':[21,"#e8d1d1"],
    'Developed_Low_Intensity':[22,"#e29e8c"],
    'Developed_Medium_Intensity':[23,"#ff0000"],
    'Developed_High_Intensity':[24,"#b50000"],
    'Barren_Land_RockSandClay) ':[31,"#d2cdc0"],
    'Deciduous_Forest':[41,'#85c77e'],
    'Evergreen_Forest':[42,"#38814e"],
    'Mixed Fores':[43,"#d4e7b0"],
    'Dwarf_Scrub':[51,"#af963c"],
    'Shrub_Scrub':[52,"#dcca8f"],
    'Grassland_Herbaceous':[71,"#fde9aa"],
    'Sedge_Herbaceous':[72,"#d1d182"],
    'Lichens':[73,"#a3cc51"],
    'Moss':[74,"#82ba9e"],
    'Pasture_Hay':[81,"#fbf65d"],
    'Cultivated_Crops':[82,"#ca9146"],
    'Woody_Wetlands':[90,"#c8e6f8"],
    'Emergent_Herbaceous_Wetlands':[95,"#64b3d5"],    
    }

nlcd_class_color={v[0]:v[1] for v in nlcd_class_idNcolor.values()}
cmap_LC, norm=matplotlib.colors.from_levels_and_colors(list(nlcd_class_color.keys()),list(nlcd_class_color.values()),extend='max')
cmap_LC
```

<img src="./imgs/3_5/3_5_02.png" height='auto' width='auto' title="caDesign">    



```python
f, ax=plt.subplots(figsize=(20,20))
san_francisco_nlcd_2019_lc.plot.imshow(add_colorbar=False,cmap=cmap_LC,norm=norm,ax=ax) # show(lu,ax=ax,transform=transform,cmap=cmap_LC,norm=norm)
quadrats_gdf.plot(color='none',edgecolor='k',linewidth=1,ax=ax,linestyle='--')
for idx, row in quadrats_gdf.iterrows():    
    ax.annotate(text=idx, xy=row['coords'], horizontalalignment='center')
plt.show()
```

<img src="./imgs/3_5/output_84_0.png" height='auto' width='auto' title="caDesign">
    


提取部分样方单独放大打印观察，并计算各个样方的标记特征。样方提取的方式使用`rioxarray`库提供的`clip`方法。


```python
scene_idx_lst=[63,80,81,84,96,97,109,186,315,409]
scenes={k:san_francisco_nlcd_2019_lc.rio.clip([quadrats_gdf.geometry.apply(mapping).iloc[k]],quadrats_gdf.crs) for k in scene_idx_lst}
```


```python
fig, axs=plt.subplots(nrows=2, ncols=5, figsize=(9*2, 4*2),subplot_kw={'xticks': [], 'yticks': []})
for ax, (idx,scene) in zip(axs.flat, scenes.items()):
    scene.plot.imshow(add_colorbar=False,cmap=cmap_LC,norm=norm,ax=ax)
    ax.set_title(idx)
    ax.set_axis_off()
                 
plt.tight_layout()
plt.show()                 
```

<img src="./imgs/3_5/output_87_0.png" height='auto' width='auto' title="caDesign">
    



定义`signature2distance_integrating()`方法计算两两样方的标记特征距离值。参数`scenes`为分类数据样方列表，`signatures_lst`为标记特征列表。通过`help`方法可以查看所定义函数的说明。


```python
help(usda_signature.signature2distance_integrating)
```

    Help on function signature2distance_integrating in module usda.pattern_signature._signature2distance_integration:
    
    signature2distance_integrating(scenes, signatures_lst=['class_hierarchical_decomposition'])
        批量计算signature（标记特征），并匹配siganature选择distance（距离）算法
        
        Parameters
        ----------
        scenes : list[2darray]
            多个2维数组（代表栅格或图像）.
        signatures_lst : list[str], optional
            signature名称列表，目前含有class_clumpSize、class_pairs_frequency和class_hierarchical_decomposition. The default is ['class_hierarchical_decomposition'].
        
        Returns
        -------
        pattern_distance_df : DataFrame
            signature标记特征对应distance距离返回值.
    
    


```python
pattern_distance_df=usda_signature.signature2distance_integrating(scenes,signatures_lst=['class_hierarchical_decomposition','class_pairs_frequency','class_clumpSize'])    
pattern_distance_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>class_hierarchical_decomposition</th>
      <th>class_pairs_frequency</th>
      <th>class_clumpSize</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="9" valign="top">63</th>
      <th>80</th>
      <td>0.818697</td>
      <td>36.114617</td>
      <td>0.629761</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.212229</td>
      <td>32.686012</td>
      <td>0.537336</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.023534</td>
      <td>19.777420</td>
      <td>0.370561</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.850479</td>
      <td>37.042946</td>
      <td>0.548986</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.803605</td>
      <td>40.512433</td>
      <td>0.625968</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.593157</td>
      <td>54.352800</td>
      <td>0.505033</td>
    </tr>
    <tr>
      <th>186</th>
      <td>0.796919</td>
      <td>36.825706</td>
      <td>0.659465</td>
    </tr>
    <tr>
      <th>315</th>
      <td>0.645925</td>
      <td>54.150345</td>
      <td>0.489721</td>
    </tr>
    <tr>
      <th>409</th>
      <td>0.396148</td>
      <td>35.773141</td>
      <td>0.435492</td>
    </tr>
    <tr>
      <th rowspan="8" valign="top">80</th>
      <th>81</th>
      <td>0.632294</td>
      <td>28.217521</td>
      <td>0.080077</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.788244</td>
      <td>37.246134</td>
      <td>0.654455</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.777978</td>
      <td>8.872845</td>
      <td>0.678953</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.647031</td>
      <td>12.049431</td>
      <td>0.688798</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.693220</td>
      <td>51.241613</td>
      <td>0.690256</td>
    </tr>
    <tr>
      <th>186</th>
      <td>1.000000</td>
      <td>19.000000</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>315</th>
      <td>1.000000</td>
      <td>49.000000</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>409</th>
      <td>0.807399</td>
      <td>35.011363</td>
      <td>0.627292</td>
    </tr>
    <tr>
      <th rowspan="7" valign="top">81</th>
      <th>84</th>
      <td>0.227968</td>
      <td>34.150054</td>
      <td>0.591152</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.891942</td>
      <td>32.640539</td>
      <td>0.679186</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.715356</td>
      <td>32.765645</td>
      <td>0.687659</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.477403</td>
      <td>51.604482</td>
      <td>0.613379</td>
    </tr>
    <tr>
      <th>186</th>
      <td>0.755293</td>
      <td>31.929292</td>
      <td>0.687450</td>
    </tr>
    <tr>
      <th>315</th>
      <td>0.660390</td>
      <td>60.627205</td>
      <td>0.649782</td>
    </tr>
    <tr>
      <th>409</th>
      <td>0.406146</td>
      <td>37.141794</td>
      <td>0.641089</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">84</th>
      <th>96</th>
      <td>0.827232</td>
      <td>37.749536</td>
      <td>0.647618</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.771575</td>
      <td>41.226368</td>
      <td>0.665225</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.554190</td>
      <td>52.827141</td>
      <td>0.417957</td>
    </tr>
    <tr>
      <th>186</th>
      <td>0.814807</td>
      <td>36.341972</td>
      <td>0.658514</td>
    </tr>
    <tr>
      <th>315</th>
      <td>0.671761</td>
      <td>53.718118</td>
      <td>0.497524</td>
    </tr>
    <tr>
      <th>409</th>
      <td>0.412026</td>
      <td>37.049261</td>
      <td>0.576006</td>
    </tr>
    <tr>
      <th rowspan="5" valign="top">96</th>
      <th>97</th>
      <td>0.572186</td>
      <td>12.929291</td>
      <td>0.065840</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.764689</td>
      <td>51.529262</td>
      <td>0.692933</td>
    </tr>
    <tr>
      <th>186</th>
      <td>1.000000</td>
      <td>19.000000</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>315</th>
      <td>0.941917</td>
      <td>49.000000</td>
      <td>0.692834</td>
    </tr>
    <tr>
      <th>409</th>
      <td>0.706103</td>
      <td>31.546109</td>
      <td>0.587782</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">97</th>
      <th>109</th>
      <td>0.702579</td>
      <td>50.532680</td>
      <td>0.691740</td>
    </tr>
    <tr>
      <th>186</th>
      <td>0.874830</td>
      <td>24.000000</td>
      <td>0.692838</td>
    </tr>
    <tr>
      <th>315</th>
      <td>0.935202</td>
      <td>54.000000</td>
      <td>0.692420</td>
    </tr>
    <tr>
      <th>409</th>
      <td>0.786699</td>
      <td>37.969910</td>
      <td>0.678243</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">109</th>
      <th>186</th>
      <td>0.663647</td>
      <td>45.767654</td>
      <td>0.639283</td>
    </tr>
    <tr>
      <th>315</th>
      <td>0.597354</td>
      <td>56.128743</td>
      <td>0.507708</td>
    </tr>
    <tr>
      <th>409</th>
      <td>0.494970</td>
      <td>50.475681</td>
      <td>0.663615</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">186</th>
      <th>315</th>
      <td>0.657728</td>
      <td>38.582079</td>
      <td>0.652029</td>
    </tr>
    <tr>
      <th>409</th>
      <td>0.763090</td>
      <td>34.200132</td>
      <td>0.687000</td>
    </tr>
    <tr>
      <th>315</th>
      <th>409</th>
      <td>0.442134</td>
      <td>55.589732</td>
      <td>0.615746</td>
    </tr>
  </tbody>
</table>
</div>



### 3.5.4.2 模式级聚——距离矩阵与层级聚类（distance matrix and hierarchical clustering）

基于样方标记特征和距离度量的一种应用是可以将相似特征的样方聚类归纳城市空间结构。首先是基于上述10个样方的层次聚类实验，观察集聚的过程。在应用`SciPy`库`scipy.cluster.hierarchy`模块提供的`linkage`层次聚类（hierarchical/agglomerative clustering）方法时，参数`y`是1-D压缩距离矩阵（ condensed distance matrix），或2-D观察向量矩阵。矩阵中不能有空值（NaNs）或无限值（infs）。前文定义`signature2distance_integrating()`方法的返回值是成对（层级索引）样方的DataFrame格式数据，定义`xy_to_matrix()`方法，将其转换为距离矩阵的形式，满足`linkage`参数输入的要求。

层次聚类每一层级形成新的聚类簇时，如聚类簇为$u$，其它各个簇表示为$v$，都需要更新簇$u$与所有簇$v$之间的距离值。这一距离值更新的方法由`linkage`参数`method`确定，其提供了`single`、`complete`、`average`、`weighted`、`centroid`、`median`和`ward`等方式，公式和解释如下表格<sup>[29]</sup>：

|  序号 |  方法名称 | 公式  | 解释说明  |
|---|---|---|---|
| 1  |  single |  $$d(u,v) = \min(dist(u[i],v[j]))$$ | 簇$u$和簇$v$距离值的最小值，即最近点算法（Nearest Point Algorithm）  |
| 2  |  complete | $$d(u, v) = \max(dist(u[i],v[j]))$$ | 簇$u$和簇$v$距离值的最大值，即最远点算法（Farthest Point Algorithm）或 Voor Hees 算法  |
| 3  |  average | $$d(u,v) = \sum_{ij} \frac{d(u[i], v[j])}{(\|u\|*\|v\|)}$$  | 式中$|u|$和$|v|$分别为簇$u$和$v$的基数（cardinalities），该算法也称之为UPGMA（unweighted pair group method with arithmetic mean）算法  |
| 4  | weighted  | $$d(u,v) = (dist(s,v) + dist(t,v))/2$$  | 簇$u$由簇$s$和簇$t$组成，计算簇$s$和簇$t$与簇$v$的距离值之和的一半为簇$u$和各簇$v$之间更新的距离值，称为WPGMA（Weighted Pair Group Method with Arithmetic Mean）算法  |
| 5  | centroid  |  $$dist(s,t) = \|\|c_s-c_t\|\|_2$$ | $c_s$和$c_t$分别为组成新簇$u$的簇$s$和簇$t$的质心。簇$u$与$v$距离值的更新则为簇$u$新质心到所有$v$质心之间的欧几里得距离（Euclidean distance），也称为 UPGMC（Unweighted Pair-Groups Method Centroid） 算法 |
| 6  | median  | $$dist(s,t) = \|\|w_s-w_t\|\|_2$$<sup>[Müllner, D. Modern hierarchical, agglomerative clustering algorithms. (2011).]</sup>  | 类似 centroid 方法，由质心变为中位数，也称为WPGMC算法 |
| 7  | ward  | $$d(u,v) = \sqrt{\frac{\|v\|+\|s\|}   {T}d(v,s)^2 + \frac{\|v\|+\|t\|} {T}d(v,t)^2 - \frac{\|v\|}{T}d(s,t)^2}$$  | 使用 Ward 方差最小化算法。 新簇$u$由簇$s$和簇$t$组成，且$$T=\|v\|+\|s\|+\|t\|$$ ，$$\|*\|$$为其参数的基数。该方法也称之为增量算法（incremental algorithm）|


```python
nlcd_decomposition_matrix=usda_utils.xy_to_matrix(pattern_distance_df[['class_hierarchical_decomposition']].reset_index())
nlcd_decomposition_matrix
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>level_0</th>
      <th>63</th>
      <th>80</th>
      <th>81</th>
      <th>84</th>
      <th>96</th>
      <th>97</th>
      <th>109</th>
      <th>186</th>
      <th>315</th>
    </tr>
    <tr>
      <th>level_0</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>63</th>
      <td>0.818697</td>
      <td>0.212229</td>
      <td>0.023534</td>
      <td>0.850479</td>
      <td>0.803605</td>
      <td>0.593157</td>
      <td>0.796919</td>
      <td>0.645925</td>
      <td>0.396148</td>
    </tr>
    <tr>
      <th>80</th>
      <td>0.212229</td>
      <td>0.632294</td>
      <td>0.788244</td>
      <td>0.777978</td>
      <td>0.647031</td>
      <td>0.693220</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.807399</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.023534</td>
      <td>0.788244</td>
      <td>0.227968</td>
      <td>0.891942</td>
      <td>0.715356</td>
      <td>0.477403</td>
      <td>0.755293</td>
      <td>0.660390</td>
      <td>0.406146</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.850479</td>
      <td>0.777978</td>
      <td>0.891942</td>
      <td>0.827232</td>
      <td>0.771575</td>
      <td>0.554190</td>
      <td>0.814807</td>
      <td>0.671761</td>
      <td>0.412026</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.803605</td>
      <td>0.647031</td>
      <td>0.715356</td>
      <td>0.771575</td>
      <td>0.572186</td>
      <td>0.764689</td>
      <td>1.000000</td>
      <td>0.941917</td>
      <td>0.706103</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.593157</td>
      <td>0.693220</td>
      <td>0.477403</td>
      <td>0.554190</td>
      <td>0.764689</td>
      <td>0.702579</td>
      <td>0.874830</td>
      <td>0.935202</td>
      <td>0.786699</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.796919</td>
      <td>1.000000</td>
      <td>0.755293</td>
      <td>0.814807</td>
      <td>1.000000</td>
      <td>0.874830</td>
      <td>0.663647</td>
      <td>0.597354</td>
      <td>0.494970</td>
    </tr>
    <tr>
      <th>186</th>
      <td>0.645925</td>
      <td>1.000000</td>
      <td>0.660390</td>
      <td>0.671761</td>
      <td>0.941917</td>
      <td>0.935202</td>
      <td>0.597354</td>
      <td>0.657728</td>
      <td>0.763090</td>
    </tr>
    <tr>
      <th>315</th>
      <td>0.396148</td>
      <td>0.807399</td>
      <td>0.406146</td>
      <td>0.412026</td>
      <td>0.706103</td>
      <td>0.786699</td>
      <td>0.494970</td>
      <td>0.763090</td>
      <td>0.442134</td>
    </tr>
  </tbody>
</table>
</div>




```python
Z=linkage(nlcd_decomposition_matrix, method='single') 
```

可视化层次聚类结构（树状图，dendrogram），可以直接由`SciPy`库提供的`dendrogram`方法打印（该方法重新排序了原以分类值标识的行列名，因此需要对位查看）；也可以使用`seaborn`的`clustermap`等方法实现。从层次聚类结果来看，96和97，109和186为距离最近的样方，首先执行了聚类，构成了新簇(96,97)和（109,186），比对前文打印的样方地图，该两对样方一对基本为植被，另一对基本为建设用地，复合实际土地覆盖类型相似结构。在下一层级，(96,97)进一步聚合了80，而(109,186)进一步聚合了84，通过观察打印的地图，二者是最为接近已聚样方簇的样方，从而一定程度上说明了样方的层级分解标记特征的Jaccard距离度量结果可以将相似土地覆盖类型结构的样方归类。


```python
fig=plt.figure(figsize=(5, 1.5))
dn=dendrogram(Z)
plt.show()
```


<img src="./imgs/3_5/output_95_0.png" height='auto' width='auto' title="caDesign">    



```python
sns.clustermap(nlcd_decomposition_matrix,method='single',figsize=(5,5));
```


<img src="./imgs/3_5/output_96_0.png" height='auto' width='auto' title="caDesign">   



cophenetic correlation coefficient（cophenetic correlation 系数）是衡量树状图保留原始未建模数据点之间成对距离程度的度量，将所有样本实际成对的距离与层次聚类所暗示的距离进行比较，该值越接近于1，聚类所保留原始距离的效果越好<sup>[30]</sup>。用`cophene`方法计算该系数，其结果为0.809，较好的保留了原始距离的信息。


```python
c, coph_dists=cophenet(Z, pdist(nlcd_decomposition_matrix))
print(c)
```

    0.8090297742076558
    

* 增加样方数量的模式级聚实验

通过上述10个样方的实验，初步判断样方标记特征距离度量方法的可行性，增加样方的数量，分析奥克兰（Oakland）区域样方聚类的特点。此次样方数为209个，但使用了类/簇大小标记特征。


```python
scene_idx_lst_2=list(range(285,494)) 
scenes_2={k:san_francisco_nlcd_2019_lc.rio.clip([quadrats_gdf.geometry.apply(mapping).iloc[k]],quadrats_gdf.crs) for k in scene_idx_lst_2}
```


```python
pattern_distance_df_2=usda_signature.signature2distance_integrating(scenes_2,signatures_lst=['class_hierarchical_decomposition','class_pairs_frequency','class_clumpSize'])    
pattern_distance_df_2
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>class_hierarchical_decomposition</th>
      <th>class_pairs_frequency</th>
      <th>class_clumpSize</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">285</th>
      <th>286</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>287</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>288</th>
      <td>0.808186</td>
      <td>12.038733</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>289</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>290</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">490</th>
      <th>492</th>
      <td>0.485143</td>
      <td>27.948164</td>
      <td>0.497090</td>
    </tr>
    <tr>
      <th>493</th>
      <td>0.529574</td>
      <td>25.737011</td>
      <td>0.507870</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">491</th>
      <th>492</th>
      <td>0.224449</td>
      <td>22.373891</td>
      <td>0.437367</td>
    </tr>
    <tr>
      <th>493</th>
      <td>0.219774</td>
      <td>18.494940</td>
      <td>0.299369</td>
    </tr>
    <tr>
      <th>492</th>
      <th>493</th>
      <td>0.291351</td>
      <td>14.616090</td>
      <td>0.448258</td>
    </tr>
  </tbody>
</table>
<p>21736 rows × 3 columns</p>
</div>




```python
nlcd_decomposition_matrix_2=usda_utils.xy_to_matrix(pattern_distance_df_2[['class_clumpSize']].reset_index())
nlcd_decomposition_matrix_2
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>level_0</th>
      <th>285</th>
      <th>286</th>
      <th>287</th>
      <th>288</th>
      <th>289</th>
      <th>290</th>
      <th>291</th>
      <th>292</th>
      <th>293</th>
      <th>294</th>
      <th>...</th>
      <th>483</th>
      <th>484</th>
      <th>485</th>
      <th>486</th>
      <th>487</th>
      <th>488</th>
      <th>489</th>
      <th>490</th>
      <th>491</th>
      <th>492</th>
    </tr>
    <tr>
      <th>level_0</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>285</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>...</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>286</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>...</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>287</th>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>...</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>288</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.240302</td>
      <td>0.456251</td>
      <td>...</td>
      <td>0.688475</td>
      <td>0.691116</td>
      <td>0.689746</td>
      <td>0.688871</td>
      <td>0.692257</td>
      <td>0.688102</td>
      <td>0.686559</td>
      <td>0.685458</td>
      <td>0.677197</td>
      <td>0.685163</td>
    </tr>
    <tr>
      <th>289</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>...</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>488</th>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.688102</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.591620</td>
      <td>...</td>
      <td>0.529922</td>
      <td>0.591126</td>
      <td>0.591070</td>
      <td>0.544601</td>
      <td>0.295189</td>
      <td>0.200730</td>
      <td>0.359049</td>
      <td>0.602958</td>
      <td>0.635073</td>
      <td>0.606241</td>
    </tr>
    <tr>
      <th>489</th>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.686559</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.575095</td>
      <td>...</td>
      <td>0.602111</td>
      <td>0.543992</td>
      <td>0.585181</td>
      <td>0.565167</td>
      <td>0.352403</td>
      <td>0.359049</td>
      <td>0.146109</td>
      <td>0.543257</td>
      <td>0.528007</td>
      <td>0.533614</td>
    </tr>
    <tr>
      <th>490</th>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.685458</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.595087</td>
      <td>...</td>
      <td>0.619539</td>
      <td>0.564215</td>
      <td>0.617371</td>
      <td>0.605859</td>
      <td>0.532397</td>
      <td>0.602958</td>
      <td>0.543257</td>
      <td>0.469357</td>
      <td>0.497090</td>
      <td>0.507870</td>
    </tr>
    <tr>
      <th>491</th>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.677197</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.591115</td>
      <td>...</td>
      <td>0.678681</td>
      <td>0.617198</td>
      <td>0.670476</td>
      <td>0.644937</td>
      <td>0.575092</td>
      <td>0.635073</td>
      <td>0.528007</td>
      <td>0.497090</td>
      <td>0.437367</td>
      <td>0.299369</td>
    </tr>
    <tr>
      <th>492</th>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.685163</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>0.584731</td>
      <td>...</td>
      <td>0.662676</td>
      <td>0.606291</td>
      <td>0.652848</td>
      <td>0.642557</td>
      <td>0.580909</td>
      <td>0.606241</td>
      <td>0.533614</td>
      <td>0.507870</td>
      <td>0.299369</td>
      <td>0.448258</td>
    </tr>
  </tbody>
</table>
<p>208 rows × 208 columns</p>
</div>




```python
sns.clustermap(nlcd_decomposition_matrix_2,method='single',figsize=(20,20));
```


<img src="./imgs/3_5/output_103_0.png" height='auto' width='auto' title="caDesign">    


计算cophenetic correlation 系数，其结果为0.769，也较好的保留了原始距离信息。


```python
Z_2=linkage(nlcd_decomposition_matrix_2, method='single')
c_2, coph_dists_2=cophenet(Z_2, pdist(nlcd_decomposition_matrix_2))
print(c_2)
```

    0.6306879297602356
    

`dendrogram`树状图打印，可以配置`p`参数，只显示最后给定数量簇的树状图。


```python
fig=plt.figure(figsize=(7,1))
dendrogram(
    Z_2,
    truncate_mode='lastp',  # show only the last p merged clusters
    p=12,  # show only the last p merged clusters
    show_leaf_counts=True,  # otherwise numbers in brackets are counts
    leaf_rotation=90.,
    leaf_font_size=12.,
    show_contracted=True,  # to get a distribution impression in truncated branches
)
plt.show()
```

<img src="./imgs/3_5/output_107_0.png" height='auto' width='auto' title="caDesign">
    


提取实验样方中心点坐标，用于绘制散点图（`scatter`），通过颜色区分层级聚类的簇。


```python
quadrat_pts=np.array(quadrats_gdf[285:493].coords.to_list())
quadrat_pts[:5]
```




    array([[-2262093.43904849,  1926411.06361749],
           [-2262093.43904849,  1928411.06361749],
           [-2262093.43904849,  1930411.06361749],
           [-2262093.43904849,  1932411.06361749],
           [-2262093.43904849,  1934411.06361749]])



`fcluster`方法提供了提取`linkage`层次聚类结果不同距离簇的多种方式，由参数`criterion`配置，包括`inconsistent`、`distance`、`maxclust`、`monocrit`和`maxclust_monocrit`等方法。观察树状图，选择`distance`方法，配置形成扁平簇（flat clusters）时应用的阈值`t`为1.2，使每个扁平簇中的原始观测值的共生距离（cophenetic distance）不大于 `t`，产生了27类簇。


```python
t=1.2

fig=plt.figure(figsize=(20,5))
dendrogram(Z_2)
plt.axhline(t, color='k', ls='--') # 参数t阈值位置
plt.show()
```


<img src="./imgs/3_5/output_111_0.png" height='auto' width='auto' title="caDesign">    



```python
clusters=fcluster(Z_2, t=t, criterion='distance')
print(len(np.unique(clusters)))

np.random.seed(7)
cmap=matplotlib.colors.ListedColormap (np.random.rand(256,3))

f, ax=plt.subplots(figsize=(20,20))
san_francisco_nlcd_2019_lc.plot.imshow(add_colorbar=False,cmap=cmap_LC,norm=norm,ax=ax) 
quadrats_gdf.plot(color='none',edgecolor='k',linewidth=1,ax=ax,linestyle='--')
for idx, row in quadrats_gdf.iterrows():    
    ax.annotate(text=idx, xy=row['coords'], horizontalalignment='center')
ax.scatter(quadrat_pts[:,0], quadrat_pts[:,1], c=clusters, cmap=cmap,s=500,marker='x',linewidth=5)
plt.show()
```

    27
    

<img src="./imgs/3_5/output_112_1.png" height='auto' width='auto' title="caDesign">
    


调整`t`参数可以获取不同阈值的距离簇，在$t=1.2$条件下，森林植被区域、海面、建成区多种模式结构能够有效的归类。`t`值越小，产生的簇越多，同一簇下的样方间距离越小。可以根据不同分析目的确定$t$值大小，同时前期调整分析区域范围，调试样方的宽度，使分析结果更趋于分析目的的要求，有效分辨模式结构的特点。

### 3.5.4.3 模式搜索 

Tomasz F. Stepinski等人基于类/簇大小标记特征和Jensen-Shannon距离度量，使用重叠滑动窗口方法对NLCD土地覆盖类别数据执行模式搜索查询。基于模式的查询和检索是解决景观之间结构相似性问题的方法途径之一<sup>[31]</sup>。如果选择了某一特征的空间模式，例如索引值为129的样方，其特点是含有绿色开放空间的城市建设用地，如要找出类似该特征的其它样方，就是要计算其它样方与该样方的标记特征距离，距离越小则越趋近于被搜索样方的模式结构，就是含有开放空间的城市建设用地。定义`pattern_search()`方法，实现该功能。


```python
san_francisco_nlcd_2019_lc=rxr.open_rasterio(san_francisco_nlcd_2019_lc_fn,masked=True).squeeze()
query_scene=san_francisco_nlcd_2019_lc.rio.clip([quadrats_gdf.geometry.apply(mapping).iloc[129]])
```


```python
fig, ax=plt.subplots(figsize=(5,5))
query_scene.plot.imshow(add_colorbar=False,cmap=cmap_LC,norm=norm,ax=ax)
ax.set_axis_off()
plt.show()    
```

<img src="./imgs/3_5/output_116_0.png" height='auto' width='auto' title="caDesign">
    


`pattern_search()`方法定义时，参数`query_scene_array`为一个2维数组，即土地覆盖分类样方矩阵；其它样方通过提供`quadrats_gdf`参数GeoDataFrame格式Polygon地理空间几何数据和土地覆盖分类栅格文件路径进行提取。标记特征参数`signature`目前配置了类/簇大小、共现关系和层级分解标记特征，对应选择的距离度量方法为Jensen-Shan、Wave Hedges和Jaccard算法。


```python
help(usda_signature.pattern_search)
```

    Help on function pattern_search in module usda.pattern_signature._pattern_module:
    
    pattern_search(query_scene_array, quadrats_gdf, tif_fn, signature='class_clumpSize_histogram', distance_metirc='Jensen-Shan')
        给定一个2d数组（栅格数据），指定signature和distance的方法，返回distance值
        
        Parameters
        ----------
        query_scene_array : 2darray
            2维数组， 参照栅格数据.
        quadrats_gdf : GeoDataFrame
            为比对的样方Polygon对象.
        tif_fn : str
            栅格数据文件路径.
        signature : str, optional
            signature标记方法名称. The default is 'class_clumpSize_histogram'.
        distance_metirc : str, optional
            distance距离测量方法名称. The default is 'Jensen-Shan'.
        
        Returns
        -------
        quadrats_gdf_copy : GeoDataFrame
            含distance结果的数据.
    
    


```python
query_distance=usda_signature.pattern_search(query_scene.data,quadrats_gdf,san_francisco_nlcd_2019_lc_fn,signature='class_clumpSize_histogram',distance_metirc='Jensen-Shan')
```

    100%|██████████| 494/494 [04:25<00:00,  1.86it/s]
    


```python
query_distance
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>geometry</th>
      <th>coords</th>
      <th>distance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>POLYGON ((-2291093.439 1925411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1926411.0636174935)</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>1</th>
      <td>POLYGON ((-2291093.439 1927411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1928411.0636174935)</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>2</th>
      <td>POLYGON ((-2291093.439 1929411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1930411.0636174935)</td>
      <td>0.622709</td>
    </tr>
    <tr>
      <th>3</th>
      <td>POLYGON ((-2291093.439 1931411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1932411.0636174935)</td>
      <td>0.651594</td>
    </tr>
    <tr>
      <th>4</th>
      <td>POLYGON ((-2291093.439 1933411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1934411.0636174935)</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>489</th>
      <td>POLYGON ((-2241093.439 1953411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1954411.0636174935)</td>
      <td>0.495392</td>
    </tr>
    <tr>
      <th>490</th>
      <td>POLYGON ((-2241093.439 1955411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1956411.0636174935)</td>
      <td>0.458482</td>
    </tr>
    <tr>
      <th>491</th>
      <td>POLYGON ((-2241093.439 1957411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1958411.0636174935)</td>
      <td>0.462042</td>
    </tr>
    <tr>
      <th>492</th>
      <td>POLYGON ((-2241093.439 1959411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1960411.0636174935)</td>
      <td>0.273874</td>
    </tr>
    <tr>
      <th>493</th>
      <td>POLYGON ((-2241093.439 1961411.064, -2243093.4...</td>
      <td>(-2242093.4390484886, 1962411.0636174935)</td>
      <td>0.465426</td>
    </tr>
  </tbody>
</table>
<p>494 rows × 3 columns</p>
</div>



打印分类地图，并用颜色变化的❌符号标注与被搜索样方标记特征距离的大小。图中越趋于深蓝色的样方与被搜索样方距离越近，即模式结构越类似；相反，海面区域为绿色，与被搜索样方的距离趋近于1，明显不相似。


```python
query_distance_pts=np.array(query_distance.coords.to_list())

f, ax=plt.subplots(figsize=(20,20))
san_francisco_nlcd_2019_lc.plot.imshow(add_colorbar=False,cmap=cmap_LC,norm=norm,ax=ax)  # show(san_francisco_nlcd_2019_lc_array,ax=ax,transform=san_francisco_nlcd_2019_lc_transform,cmap=cmap_LC,norm=norm)
query_distance.plot(color='none',edgecolor='k',linewidth=1,ax=ax,linestyle='--')
for idx, row in query_distance.iterrows():    
    ax.annotate(text=idx, xy=row['coords'], horizontalalignment='center')
ax.scatter(query_distance_pts[:,0], query_distance_pts[:,1], c=query_distance['distance'], cmap='winter',s=500,marker='x',linewidth=5)
plt.show()
```


<img src="./imgs/3_5/output_122_0.png" height='auto' width='auto' title="caDesign">    


### 3.5.4.4 模式监测

Pawel Netzel和Tomasz F. Stepinski应用类/簇大小标记特征和Jensen-Shannon距离度量对2001-2006年NLCD土地覆盖类型执行监测，以3km/cell（栅格单元）高空分辨率显示变化地图<sup>[32]</sup>。选择比较2008年和2019年NLCD土地覆盖类型旧金山区域变化度。2008年数据处理方法同2019年，并采用统一样方组数据，以计算不同时间同一样方标记特征的距离度量。

定义的`rio_read_subset()`方法可以返回栅格数据的元数据（meta/profile），包含投影等信息，可用于`rec_quadrats_gdf()`方法样方构建epsg投影编号参数输入；同时返回几何变换（transform）信息，可用于地图打印时，几何变换参数的输入。


```python
nlcd_2008_lc_fn=r'E:\data\NLCD_landcover_2019_release_all_files_20210604\nlcd_2008_land_cover_l48_20210604\nlcd_2008_land_cover_l48_20210604.img'
nlcd_2008_lc_lu,nlcd_2008_lc_transform,nlcd_2008_lc_ras_meta=usda_geodataProcess.rio_read_subset(nlcd_2008_lc_fn,[pt_leftBottom_pj,pt_rightTop_pj])  
```


```python
pt_leftBottom=[-122.51764,37.46971]
pt_rightTop=[-122.06587,37.94047] 
pt_leftBottom_pj=usda_geodataProcess.pt_coordi_transform(4326,nlcd_2008_lc_ras_meta['crs'],pt_leftBottom)
pt_rightTop_pj=usda_geodataProcess.pt_coordi_transform(4326,nlcd_2008_lc_ras_meta['crs'],pt_rightTop)
```


```python
cell_size=2000
quadrats_gdf=usda_geodataProcess.rec_quadrats_gdf(pt_leftBottom_pj,pt_rightTop_pj,cell_size,cell_size,crs=nlcd_2008_lc_ras_meta['crs'])
quadrats_gdf['coords']=quadrats_gdf['geometry'].apply(lambda x: x.representative_point().coords[:][0])
quadrats_gdf.head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>geometry</th>
      <th>coords</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>POLYGON ((-2291093.439 1925411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1926411.0636174935)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>POLYGON ((-2291093.439 1927411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1928411.0636174935)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>POLYGON ((-2291093.439 1929411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1930411.0636174935)</td>
    </tr>
  </tbody>
</table>
</div>




```python
f, ax=plt.subplots(figsize=(20,20))
show(nlcd_2008_lc_lu,ax=ax,transform=nlcd_2008_lc_transform,cmap=cmap_LC,norm=norm)
quadrats_gdf.plot(color='none',edgecolor='k',linewidth=1,ax=ax,linestyle='--')
for idx, row in quadrats_gdf.iterrows():    
    ax.annotate(text=idx, xy=row['coords'], horizontalalignment='center')
plt.show()
```


<img src="./imgs/3_5/output_127_0.png" height='auto' width='auto' title="caDesign">    



```python
san_francisco_nlcd_2008_lc_fn=r'E:\data\NLCD_landcover_2019_release_all_files_20210604\clipped\san_francisco_nlcd_2008_lc.tif'

nlcd_2008_lc_ras_meta_=copy.deepcopy(nlcd_2008_lc_ras_meta)
nlcd_2008_lc_ras_meta_.update(   
        compress='lzw',
        )  
with rio.open(san_francisco_nlcd_2008_lc_fn,'w',**nlcd_2008_lc_ras_meta_) as dst:
    dst.write(nlcd_2008_lc_lu)
```

定义`pattern_compare()`方法，实现模式监测。输如参数包括用于样方模式比较的两个分类栅格数据文件名`tif_a_fn`和`tif_b_fn`，样方组Polygon对象`quadrats_gdf`，和标记特征`signature`及距离度量`distance_metirc`。`pattern_compare()`方法目前嵌入的标记特征为类/簇大小、共现关系和层级分解，对应的距离度量算法为Jensen-Shannon、Wave Hedges和Jaccard。


```python
help(usda_signature.pattern_compare)
```

    Help on function pattern_compare in module usda.pattern_signature._pattern_module:
    
    pattern_compare(quadrats_gdf, tif_a_fn, tif_b_fn, signature='class_clumpSize_histogram', distance_metirc='Jensen-Shan')
        给两个栅格数据（2维数据），例如不同年份同一区域的LULC（土地利用/土地覆盖），指定样方，signature和distance方法，比较两个栅格的变化差异
        
        Parameters
        ----------
        quadrats_gdf :GeoDataFrame
            为比对的样方Polygon对象.
        tif_a_fn : str
            栅格数据1.
        tif_b_fn : str
            栅格数据2.
        signature : str, optional
            signature标记方法名称. The default is 'class_clumpSize_histogram'.
        distance_metirc : str, optional
            distance距离测量方法名称. The default is 'Jensen-Shan'.
        
        Returns
        -------
        quadrats_gdf_copy : GeoDataFrame
            含distance结果的数据.
    
    


```python
distance_compare=usda_signature.pattern_compare(quadrats_gdf,san_francisco_nlcd_2019_lc_fn,san_francisco_nlcd_2008_lc_fn,signature='class_clumpSize_histogram',distance_metirc='Jensen-Shan')   
```

    100%|██████████| 494/494 [08:31<00:00,  1.03s/it]
    


```python
distance_compare.head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>geometry</th>
      <th>coords</th>
      <th>distance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>POLYGON ((-2291093.439 1925411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1926411.0636174935)</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>POLYGON ((-2291093.439 1927411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1928411.0636174935)</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>POLYGON ((-2291093.439 1929411.064, -2293093.4...</td>
      <td>(-2292093.4390484886, 1930411.0636174935)</td>
      <td>0.010238</td>
    </tr>
  </tbody>
</table>
</div>



从计算结果打印图表可以发现，10年间旧金山区域土地覆盖类型基本没有较大的变化，城市发展处于成熟稳定期。在植被区域有少数几个样方有较大变化，例如488、489、40和42等几个样方；沿海区域的263、326和346有些许变化；建成区可以发现87样方有些许变化。


```python
distance_compare_pts=np.array(distance_compare.coords.to_list())

f, ax=plt.subplots(figsize=(20,20))
show(nlcd_2008_lc_lu,ax=ax,transform=nlcd_2008_lc_transform,cmap=cmap_LC,norm=norm)
distance_compare.plot(color='none',edgecolor='k',linewidth=1,ax=ax,linestyle='--')
for idx, row in distance_compare.iterrows():    
    ax.annotate(text=idx, xy=row['coords'], horizontalalignment='center')
ax.scatter(distance_compare_pts[:,0], distance_compare_pts[:,1], c=distance_compare['distance'], cmap='hot',s=500,marker='x',linewidth=5)
plt.show()
```


<img src="./imgs/3_5/output_134_0.png" height='auto' width='auto' title="caDesign">    



为了更清晰的查看土地覆盖类型连续变化情况，打印为纯粹连续网格颜色值变化，可以更明确上文指出具有明显变化的样方也伴随着邻近区域样方的变化浮动；且整个分析区域存在很多变化幅度微弱的区域。


```python
from mpl_toolkits.axes_grid1 import make_axes_locatable
fig, ax=plt.subplots(1, 1)
divider=make_axes_locatable(ax)
cax=divider.append_axes("right", size="5%", pad=0.1)
distance_compare.plot(column='distance',figsize=(5,5),legend=True,cax=cax, cmap='hot',ax=ax);
```


<img src="./imgs/3_5/output_136_0.png" height='auto' width='auto' title="caDesign">    


### 3.5.4.5 模式分割

通常土地覆盖类型在以一种类型为主导的区域内会夹杂着其它类型，如果其它类型在给定的距离度量阈值内，则将其归为主导类型，那么融合后的分类地图会比较容易进行区域化（区域集，regionalization）。Jacek Niesterowicz等人应用类/簇大小标记特征和Jensen-Shannon距离度量，基于对象的（object-bases）机器视觉图像分析技术找到2006年NLCD土地覆盖类型数据的区域集<sup>[33]</sup>。

本次实验模式分割算法采用了样方区域增长（Region Growing）算法。区域增长算法有多种方式，列表如下<sup>[34]</sup>：

|  序号 | 算法名称  | 日期和作者  | 算法描述  |
|---|---|---|---|
|  1 | Regional neighbor search  |  1968, Muerle & Allen  | Statistical description of unit cells; join if similar.   |
| 2  |   Multi-regional heuristics | 1970, Brice & Fennema  | Form connect~ed components of equal intensity; refine with phagocyte heuristic; refine with weakness heuristic.   |
| 3  |Functional approximation & merging    | 1972, Pavlidis  | Partition with 1-dimcnsionM strips; approximate strips into segments; merge segments with similar ap- proximation coefficients.   |
| 4  | Split-and-merge  | 1974, Horowtiz & Pavlidis   | Arbitrary initial partition in pyra- midal data s\~ructure; split regions with large approximation error; and merge adjacen\~ regions with similar approximation; group simi- lar squares into irregular regions.  |
| 5  |Semantics   |  1973, Feld- man & Yaldmovsky |  Brice-and-Fennema-like initializa- tion ; maximize probability all regions correctly interpreted ; maxi- mize probability all borders cor- rectly interpreted.  |

定义`Pattern_segment_regionGrow()`方法实现模式分割，采用区域增长具体的算法为区域邻居搜索（Regional neighbor search），该算法执行的逻辑为：

1. 将栅格数据分割为样方网格，每个样方包含的栅格单元数为$2 \times 2,3 \times 3, \ldots ,n \times n$等；
2. 计算所有样方的标记特征，或者后续迭代逐次计算；
3. 从左上角第一个样方开始，先标识为分类1，计算该样方与相邻样方标记特征的距离度量值，如果该距离值小于给定的阈值，则将邻接样方也标识为分类1；否则不标识。直至检查完所有邻接样方。
4. 如果邻接样方有被标识为1的样方，则逐个计算新增加为1样方邻接样方之间的特征标记距离。计算时，已检查的样方不再计算，包括左上角第一个样方和已邻接样方标识为1的样方；
5. 重复过程4，继续检查新增加标识为1的样方邻接样方的标记特征距离，直至不再有邻接样方满足阈值要求为止；
6. 从未被标识为1的单元继续重复3-5的过程，标识为分类2，以此类推，直至标识完所有样方；
7. 标识分类后的样方，按每一分类所有样方包含土地覆盖类型（或其它分类数据）数量最多的类型为该样方区域增长算法分类的最终土地覆盖类型。

* 使用小样本生成数据，编写`Pattern_segment_regionGrow()`模式分割方法


```python
size=16*3
X,_=usda_datasets.generate_categorical_2darray(size=size)
X4=mapclassify.JenksCaspall(X[0], k=4).yb.reshape(size,size)+1
```


```python
usda_vis.imshow_label2darray(X4,figsize=(7,7),random_seed=29) 
```


<img src="./imgs/3_5/output_139_0.png" height='auto' width='auto' title="caDesign">    




```python
X4_seg=usda_signature.Pattern_segment_regionGrow(X4,4,4,0.3) 
X4_seg.seg_region_growing()
X4_seg.quadrats_restore()
```

    100%|██████████| 12/12 [00:04<00:00,  2.61it/s]
    


```python
usda_vis.imshow_label2darray(X4_seg.quadrats_restored,figsize=(7,7),random_seed=29) 
```


<img src="./imgs/3_5/output_141_0.png" height='auto' width='auto' title="caDesign">    



定义`Pattern_segment_regionGrow()`方法时，初始化的参数包括分类数组`array`、一个样方的高度`scene_x_num`和宽度`scene_y_num`、标记特征的距离阈值`scene_y_num`，及标记特征`scene_y_num`和距离度量`distance_metirc`。


```python
help(usda_signature.Pattern_segment_regionGrow)
```

    Help on class Pattern_segment_regionGrow in module usda.pattern_signature._pattern_module:
    
    class Pattern_segment_regionGrow(builtins.object)
     |  Pattern_segment_regionGrow(array, scene_x_num, scene_y_num, threshold, signature='class_clumpSize_histogram', distance_metirc='Jensen-Shan')
     |  
     |  分类数据分割（segmentation），使用的是region growing algorithm，但是为signature标志特征和distance距离测量方式计算2维矩阵样方（scene）间的距离，根据指定阈值，判断是否合并
     |  
     |  Methods defined here:
     |  
     |  BFS(self, x0, y0)
     |      region growing algorithm
     |      
     |      Parameters
     |      ----------
     |      x0 : int
     |          目标样方行索引.
     |      y0 : int
     |          目标样方列索引.
     |      
     |      Returns
     |      -------
     |      None.
     |  
     |  PassedAll(self)
     |      是否超出样方数或这迭代次数
     |      
     |      Returns
     |      -------
     |      bool
     |          如果超出最大迭代次数，或者超出总样方数则停止计算.
     |  
     |  __init__(self, array, scene_x_num, scene_y_num, threshold, signature='class_clumpSize_histogram', distance_metirc='Jensen-Shan')
     |      分类数据分割初始化值
     |      
     |      Parameters
     |      ----------
     |      array : 2darray
     |          2维数组，为分类数据.
     |      scene_x_num : int
     |          一个样方（scene）的宽度.
     |      scene_y_num : int
     |          一个样方（scene）的高度.
     |      threshold : float
     |          signature的距离阈值.
     |      signature : str, optional
     |          signature标记方法名称. The default is 'class_clumpSize_histogram'.
     |      distance_metirc : str, optional
     |          distance距离测量方法名称. The default is 'Jensen-Shan'.
     |      
     |      Returns
     |      -------
     |      None.
     |  
     |  distance(self, x, y, x0, y0)
     |      由signature和distance的方法计算样方间的距离
     |      
     |      Parameters
     |      ----------
     |      x : int
     |          邻接样方行索引.
     |      y : int
     |          邻接样方列索引.
     |      x0 : int
     |          目标样方行索引.
     |      y0 : int
     |          目标样方列索引.
     |      
     |      Returns
     |      -------
     |      distance : float
     |          样方间的距离值.
     |  
     |  getNeighbour(self, x0, y0)
     |      计算给定坐标点的邻接样方（坐标点）
     |      
     |      Parameters
     |      ----------
     |      x0 : int
     |          行索引.
     |      y0 : int
     |          列索引.
     |      
     |      Returns
     |      -------
     |      neighbour : list[tuple]
     |          邻接样方索引对列表.
     |  
     |  limit(self, x, y)
     |      限制条件，邻接样方不能超过样方的行列数
     |      
     |      Parameters
     |      ----------
     |      x : int
     |          行索引.
     |      y : int
     |          列索引.
     |      
     |      Returns
     |      -------
     |      bool
     |          是否超出样方行列数.
     |  
     |  matrix2quadrat(self, array, x_num, y_num)
     |      将2维矩阵划分为多个样方，用于计算signature
     |      
     |      Parameters
     |      ----------
     |      array : 2darray
     |          2维矩阵，为分类数据.
     |      x_num : int
     |          一个样方（scene）的宽度.
     |      y_num : int
     |          一个样方（scene）的高度.
     |      
     |      Returns
     |      -------
     |      quadrats : list[2darray]
     |          样方列表.
     |      quadrat_x_num : int
     |          样方的列数.
     |      quadrat_y_num : int
     |          样方的行数.
     |  
     |  quadrats_restore(self)
     |      将样方分割值返回为原数据形状大小。样方按分割后个区域值频数最大值作为最终值
     |      
     |      Returns
     |      -------
     |      None.
     |  
     |  seg_region_growing(self)
     |      region growing algorithm，主程序
     |      
     |      Returns
     |      -------
     |      None.
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
    
    

* 使用旧金山区域数据执行模式分割

按新范围重新提取了旧金山区域。样方大小分别采用了$3 \times 3$和$6 \times 6$，从实验结果来看，随着样方大小的增加，合并的程度增大，且区域集均能较好的反映原有区域土地覆盖类型。


```python
pt_leftBottom_2=[-122.51073,37.50406]
pt_rightTop_2=[-122.28473,37.59128] 
pt_leftBottom_pj_2=usda_geodataProcess.pt_coordi_transform(4326,epsg_nlcd,pt_leftBottom_2)
pt_rightTop_pj_2=usda_geodataProcess.pt_coordi_transform(4326,epsg_nlcd,pt_rightTop_2)
```


```python
partial_lu,partial_transform,partial_ras_meta=usda_geodataProcess.rio_read_subset(nlcd_2019_lc_fn,[pt_leftBottom_pj_2,pt_rightTop_pj_2])  
```


```python
san_francisco_nlcd_2019_lc_partial_fn=r'E:\data\NLCD_landcover_2019_release_all_files_20210604\clipped\san_francisco_nlcd_2019_lc_partial.tif'

partial_ras_meta_=copy.deepcopy(partial_ras_meta)
partial_ras_meta_.update(   
        compress='lzw',
        )  
with rio.open(san_francisco_nlcd_2019_lc_partial_fn,'w',**partial_ras_meta_) as dst:
    dst.write(partial_lu)
```


```python
f, ax=plt.subplots(figsize=(20,20))
show(partial_lu,ax=ax,transform=partial_transform,cmap=cmap_LC,norm=norm)
plt.show()
```

<img src="./imgs/3_5/output_148_0.png" height='auto' width='auto' title="caDesign">
    



```python
san_francisco_nlcd_2019_lc_partial_segment_3=usda_signature.Pattern_segment_regionGrow(partial_lu[0],3,3,0.3) 
san_francisco_nlcd_2019_lc_partial_segment_3.seg_region_growing()
san_francisco_nlcd_2019_lc_partial_segment_3.quadrats_restore()
```

    100%|██████████| 44/44 [05:18<00:00,  7.24s/it]  
    


```python
f, ax=plt.subplots(figsize=(20,20))
show(san_francisco_nlcd_2019_lc_partial_segment_3.quadrats_restored,ax=ax,transform=partial_transform,cmap=cmap_LC,norm=norm)
plt.show()
```


<img src="./imgs/3_5/output_150_0.png" height='auto' width='auto' title="caDesign">    


```python
san_francisco_nlcd_2019_lc_partial_segment_6=usda_signature.Pattern_segment_regionGrow(partial_lu[0],6,6,0.3) 
san_francisco_nlcd_2019_lc_partial_segment_6.seg_region_growing()
san_francisco_nlcd_2019_lc_partial_segment_6.quadrats_restore()
```

    100%|██████████| 22/22 [01:27<00:00,  4.00s/it]
    


```python
f, ax=plt.subplots(figsize=(20,20))
show(san_francisco_nlcd_2019_lc_partial_segment_6.quadrats_restored,ax=ax,transform=partial_transform,cmap=cmap_LC,norm=norm)
plt.show()
```

<img src="./imgs/3_5/output_152_0.png" height='auto' width='auto' title="caDesign">



## 3.5.5 地表温度（Land Surface Temperature，LST）与土地覆盖类型模式标记特征

关于LST与土地覆盖类型模式的探索，Guanhua Guo等人使用面向对象的图像分割技术进一步融合相似的LST数据构建区域集后执行局部空间自相关指数（局部 Moran’s I指数）计算，并计算LST与NDVI（normalized differential vegetation index）、NDBI（normalized differential build-up index）和NDBal（normalized difference bareness index）指数之间的相关性，通过回归决策树（Regression Tree）构建模型探索LST与指数之间的非线性关系<sup>[3]</sup>；Jing Gao等人则提出了基于热源和汇斑块之间的空间连通性指数 (spatial connectivity between patches of the heat source and sink，SCSS)，以评估土地覆盖类型分布（景观格局）对中国武汉 LST 的异质性影响<sup>[35]</sup>；Chuyuan Wang等人选择了覆盖美国本土柯本气候区（Köppen climate zones）的多个城市，通过局部 Moran’s I指数实现空间聚类，使用相关性和多元回归分析NLCD土地覆盖类型和LST之间的关系，探索不同气候区的影响<sup>[36]</sup>。基于样方的模式标记特征和距离度量则考虑了区域环境的模式特征，是探索城市空间因素/要素（例如土地利用和土地覆盖（land use and land cover，LULC））组成、构成与某一监测指数（生境质量、碳储存和固持、LST等）之间关系的有效手段。下文浅尝辄止的实验，检查了LST冷热点区域对应样方的标记特征，具体实验步骤为：

1. 准备LST（MODIS/Aqua Land Surface Temperature/3-Band Emissivity Daily L3 Global 1 km SIN Grid Day，MYD21A1D v006）数据和NLCD土地覆盖类型数据，并同一投影，提取分析区域数据；
2. 以LST $1km \times 1km$ 高空分辨率栅格单元为样方大小，因此需要将LST栅格数据转为Polygon的SHP格式数据；
3. 计算LST的局部空间自相关指数（local indicators of spatial autocorrelation，LISA），获得热点（Hot Spots）和冷点（Cold Spots）；
4. 计算冷热点对应区域土地覆盖类型样方的标记特征，并对应土地覆盖类型分类和特征分类计算和，打印直方图观察标记特征。

### 3.5.5.1 LST与NLCD数据预处理

LST数据使用的是中分辨率成像光谱仪 (Moderate Resolution Imaging Spectroradiometer ，MODIS)的地表温度和辐射率（Emissivity ）产品（MYD21A1D v006）<sup>[37]</sup>。MYD21 LST算法基于 ASTER 温度/辐射率分离（ASTER Temperature/Emissivity Separation ，TES）技术，使用基于物理学的算法同时从 MODIS 热红外波段 29、31 和 32 中动态检索 LST 和光谱辐射率。TES 算法与改进的水蒸气比例 ( Water Vapor Scaling ，WVS) 大气校正相结合，可以稳定的检索非常温暖和潮湿的条件，并优化了相关计算方式，保证了LST的精度。该产品的高空分辨率为（栅格单一的大小）为1000m，时间分辨率为天，包括7层（栅格文件数），关键解释和指标如下：

|  文件名称（SDS Name） | 描述 (Description) | 单位（Units）  | 数据类型（Data Type）  |  有效值区间（Valid Factor） |
|---|---|---|---|---|
| LST_1KM  | Land surface temperature  |  Kelvin |  16-bit unsigned integer  |  7500 to 65535 |
| QC  |  Quality Control (QC) |  N/A | 16-bit unsigned integer  |  0 to 65535 |
| Emis_29  | Band 29 emissivity  | N/A  |  8-bit unsigned integer|  1 to 255 |
|  Emis_31 |  	Band 31 emissivity |  N/A |  8-bit unsigned integer |  1 to 255 |
| Emis_32  |  Band 32 emissivity |  N/A | 8-bit unsigned integer | 1 to 255  |
| View_Angle  | MODIS view zenith angle  |  Degree | 8-bit unsigned integer  |  0 to 130 |
|  View_Time |  Time of MODIS observation |  Hours |  8-bit unsigned integer |  0 to 240 |

土地覆盖类型为NLCD数据集。因为NLCD和LST数据投影系统不同，提取芝加哥区域数据时，如果不统一投影，提取的区域会发生偏差；而NLCD数据为$30m \times 30m$的高空分辨率，数据量较大，直接变换全美区域的数据投影会增加时间成本，因此提取分析区域数据后再变换投影。考虑到样方提取的范围，对NLCD数据提取时扩大了范围；而对LST数据提取时，缩小了范围，以避免样方超出NLCD提取区域范围。


```python
import geopandas as gpd
import libpysal.weights as LW
import esda
```

* 配置NLCD和LST数据文件路径，及分析区域坐标


```python
nlcd_2019_lc_fn=r'E:\data\NLCD_landcover_2019_release_all_files_20210604\nlcd_2019_land_cover_l48_20210604\nlcd_2019_land_cover_l48_20210604.img'
LST_temp_fn=r'E:\data\LST\LS_eMAH_TEMP.2022.254-263.1KM.COMPRES.006.2022267001804\LS_eMAH_TEMP.2022.254-263.1KM.LST_TEMP.006.2022266233217.tif'

pt_leftBottom=[-88.41018,41.45427]
pt_rightTop=[-87.15433,41.69524] 
```

* 读取扩大了分析范围的NLCD数据


```python
with rio.open(nlcd_2019_lc_fn) as src:    
    epsg_nlcd=src.crs
    transform=src.transform

nlcd_pt_leftBottom_pj=usda_geodataProcess.pt_coordi_transform(4326,epsg_nlcd,pt_leftBottom)
nlcd_pt_rightTop_pj=usda_geodataProcess.pt_coordi_transform(4326,epsg_nlcd,pt_rightTop)

buffer_dis=5000
nlcd_pt_leftBottom_pj=[i-buffer_dis for i in nlcd_pt_leftBottom_pj]
nlcd_pt_rightTop_pj=[i+buffer_dis for i in nlcd_pt_rightTop_pj]

Chicago_LC,Chicago_transform,Chicago_ras_meta=usda_geodataProcess.rio_read_subset(nlcd_2019_lc_fn,[nlcd_pt_leftBottom_pj,nlcd_pt_rightTop_pj])  

f, ax=plt.subplots(figsize=(20,20))
show(Chicago_LC,ax=ax,transform=Chicago_transform,cmap=cmap_LC,norm=norm)
plt.show()
```


<img src="./imgs/3_5/output_158_0.png" height='auto' width='auto' title="caDesign">    



* 保存扩大了分析范围的NLCD数据


```python
chicago_nlcd_2019_lc_fn=r'E:\data\NLCD_landcover_2019_release_all_files_20210604\clipped\chicago_nlcd_2019_lc.tif'

Chicago_ras_meta_=copy.deepcopy(Chicago_ras_meta)
Chicago_ras_meta_.update(   
        compress='lzw',
        )  
with rio.open(chicago_nlcd_2019_lc_fn,'w',**Chicago_ras_meta_) as dst:
    dst.write(Chicago_LC)
```

* 定义`raster_reprojection()`栅格投影变换函数，变换NLCD栅格数据投影（epsg=4326，同LST投影坐标系）


```python
chicago_nlcd_2019_lc_reproj_fn=r'E:\data\NLCD_landcover_2019_release_all_files_20210604\clipped\chicago_nlcd_2019_lc_4326.tif'
usda_geodataProcess.raster_reprojection(chicago_nlcd_2019_lc_fn,chicago_nlcd_2019_lc_reproj_fn,dst_crs=4326)
```

* 读取扩大了分析范围且变换了投影的NLCD数据


```python
with rio.open(chicago_nlcd_2019_lc_reproj_fn) as src:    
    epsg_chicago=src.crs
    transform_chicago=src.transform
    
chicago_pt_leftBottom_pj=usda_geodataProcess.pt_coordi_transform(4326,epsg_chicago,pt_leftBottom)
chicago_pt_rightTop_pj=usda_geodataProcess.pt_coordi_transform(4326,epsg_chicago,pt_rightTop)    
    
Chicago_LC_reproj,Chicago_reproj_transform,Chicago_reproj_ras_meta=usda_geodataProcess.rio_read_subset(chicago_nlcd_2019_lc_reproj_fn,[chicago_pt_leftBottom_pj,chicago_pt_rightTop_pj])  

f, ax=plt.subplots(figsize=(20,20))
show(Chicago_LC_reproj,ax=ax,transform=Chicago_reproj_transform,cmap=cmap_LC,norm=norm)
plt.show()    
```


<img src="./imgs/3_5/output_164_0.png" height='auto' width='auto' title="caDesign">    


* 读取缩小了分析范围的LST数据


```python
with rio.open(LST_temp_fn) as src:    
    epsg_LST=src.crs

LST_pt_leftBottom_pj=usda_geodataProcess.pt_coordi_transform(4326,epsg_LST,pt_leftBottom)
LST_pt_rightTop_pj=usda_geodataProcess.pt_coordi_transform(4326,epsg_LST,pt_rightTop)

shrink_dis=0.01
LST_pt_leftBottom_pj=[i+shrink_dis for i in LST_pt_leftBottom_pj]
LST_pt_rightTop_pj=[i-shrink_dis for i in LST_pt_rightTop_pj]

LST_temp,LST_transform,LST_ras_meta=usda_geodataProcess.rio_read_subset(LST_temp_fn,[LST_pt_leftBottom_pj,LST_pt_rightTop_pj])  

f, ax=plt.subplots(figsize=(20,20))
show(LST_temp,ax=ax,transform=LST_transform,cmap='hot')
plt.show()
```


<img src="./imgs/3_5/output_166_0.png" height='auto' width='auto' title="caDesign">    



* 保存缩小了分析范围的LST数据


```python
LST_Chicago_fn=r'E:\data\LST\LST_Chicago.tif'

LST_ras_meta_=copy.deepcopy(LST_ras_meta)
LST_ras_meta_.update(   
        compress='lzw',
        )  
with rio.open(LST_Chicago_fn,'w',**LST_ras_meta_) as dst:
    dst.write(LST_temp)
```

* 定义`raster2polygon()`方法，转换LST栅格数据为Polygon的SHP格式数据，作为分析样方组


```python
LST_shp_fn=r'E:\data\LST\LST_Chicago\LST_Chicago.shp'    
usda_geodataProcess.raster2polygon(LST_Chicago_fn,LST_shp_fn,dst_layer_name='LST',field_name='temp')
```

* 读取SHP格式的样方组数据为GeoDataFrame格式数据


```python
LST_gdf=gpd.read_file(LST_shp_fn)
LST_gdf=gpd.read_file(LST_shp_fn)
LST_gdf.to_crs(epsg_chicago,inplace=True)
LST_gdf['coords']=LST_gdf['geometry'].apply(lambda x: x.representative_point().coords[:][0])
LST_gdf.tail(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>geometry</th>
      <th>coords</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2782</th>
      <td>15177</td>
      <td>POLYGON ((-87.19602 41.47887, -87.18637 41.478...</td>
      <td>(-87.19119476340711, 41.474042316898704)</td>
    </tr>
    <tr>
      <th>2783</th>
      <td>15188</td>
      <td>POLYGON ((-87.18637 41.47887, -87.17672 41.478...</td>
      <td>(-87.1815427634865, 41.474042316898704)</td>
    </tr>
    <tr>
      <th>2784</th>
      <td>15165</td>
      <td>POLYGON ((-87.17672 41.47887, -87.16706 41.478...</td>
      <td>(-87.1718907635659, 41.474042316898704)</td>
    </tr>
  </tbody>
</table>
</div>



* 层叠打印分析区域的NLCD和LST样方数据，确保数据提取正确


```python
f, ax=plt.subplots(figsize=(20,20))
show(Chicago_LC_reproj,ax=ax,transform=Chicago_reproj_transform,cmap=cmap_LC,norm=norm)
LST_gdf.plot(color='none',edgecolor='k',linewidth=1,ax=ax,linestyle='--')
plt.show()
```


<img src="./imgs/3_5/output_174_0.png" height='auto' width='auto' title="caDesign">    


### 3.5.5.2 LST的LISA计算

LISA的计算方法可以查看“时空数据分析——空间自相关”部分内容。从计算结果打印地图可以查看具有统计显著性的冷点和热点分布情况。


```python
w_queen=LW.Queen.from_dataframe(LST_gdf)
LST_val=LST_gdf['temp']
LST_li=esda.moran.Moran_Local(LST_val,w_queen)

print(LST_li.q) # values indicate quandrant location 1 HH,  2 LH,  3 LL,  4 HL
print(LST_li.Is)
print(f"p_value<0.05 num: {(LST_li.p_sim<0.05).sum()}")

LST_gdf["li"]=LST_li.q
LST_gdf["p_value_li"]=LST_li.p_sim
LST_gdf["li_005"]=LST_gdf.apply(lambda row:row.li if row.p_value_li<0.05 else 0,axis=1)
spot_labels=[ '0 ns', '1 hot spot', '2 doughnut', '3 cold spot', '4 diamond']
LST_gdf['cl_li']=LST_gdf.apply(lambda row:spot_labels[row.li_005],axis=1)

LST_gdf.tail(3)
```

    [3 3 4 ... 3 3 3]
    [ 0.02635519  0.01931816 -0.01356252 ...  0.0514444   0.04165374
      0.08143038]
    p_value<0.05 num: 1153
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>geometry</th>
      <th>coords</th>
      <th>li</th>
      <th>p_value_li</th>
      <th>li_005</th>
      <th>cl_li</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2782</th>
      <td>15177</td>
      <td>POLYGON ((-87.19602 41.47887, -87.18637 41.478...</td>
      <td>(-87.19119476340711, 41.474042316898704)</td>
      <td>3</td>
      <td>0.261</td>
      <td>0</td>
      <td>0 ns</td>
    </tr>
    <tr>
      <th>2783</th>
      <td>15188</td>
      <td>POLYGON ((-87.18637 41.47887, -87.17672 41.478...</td>
      <td>(-87.1815427634865, 41.474042316898704)</td>
      <td>3</td>
      <td>0.233</td>
      <td>0</td>
      <td>0 ns</td>
    </tr>
    <tr>
      <th>2784</th>
      <td>15165</td>
      <td>POLYGON ((-87.17672 41.47887, -87.16706 41.478...</td>
      <td>(-87.1718907635659, 41.474042316898704)</td>
      <td>3</td>
      <td>0.254</td>
      <td>0</td>
      <td>0 ns</td>
    </tr>
  </tbody>
</table>
</div>




```python
from matplotlib import colors
cmap=colors.ListedColormap([ 'lightgrey', 'red', 'lightblue', 'blue', 'pink'])
usda_vis.gdf_plot_annotate(LST_gdf,"cl_li",annotate_fontsize=10,categorical=True,cmap=cmap,figsize=(40,10))
```


<img src="./imgs/3_5/output_177_0.png" height='auto' width='auto' title="caDesign">   



### 3.5.5.3 LST冷热点样方标记特征

* 提取冷点对应的土地覆盖类型样方（随机采样10个样方），观察样方内土地覆盖类型分布情况


```python
Chicago_LC_reproj_=rxr.open_rasterio(chicago_nlcd_2019_lc_reproj_fn,masked=True).squeeze()
```


```python
cold_spot_gdf=LST_gdf[LST_gdf['cl_li']=='3 cold spot']
print(cold_spot_gdf.shape)

cold_spot_sample=cold_spot_gdf.sample(n=10)
scenes_cold={idx:Chicago_LC_reproj_.rio.clip([mapping(df.geometry)],cold_spot_sample.crs) for idx,df in cold_spot_sample.iterrows() }

fig, axs=plt.subplots(nrows=2, ncols=5, figsize=(9*2, 4*2),subplot_kw={'xticks': [], 'yticks': []})
for ax, (idx,scene) in zip(axs.flat, scenes_cold.items()):
    scene.plot.imshow(add_colorbar=False,cmap=cmap_LC,norm=norm,ax=ax)
    ax.set_title(idx)
    ax.set_axis_off()
                 
plt.tight_layout()
plt.show() 
```

    (312, 7)
    


<img src="./imgs/3_5/output_180_1.png" height='auto' width='auto' title="caDesign">    



* 计算冷点类/簇大小标记特征

从计算结果来看，LST冷点分布对应的土地覆盖类型以分类编号11（对应开放水体（Open Water））和82（耕地（Cultivated Crops））为主，且二者的连通性区域大小（clump-size）主要位于为9，即$[ 2^{16},   2^{18} )$（$[65536, 262144)$）组距内，约占$1000m \times 1000m$样方的6.5536%~26.2144%，一定程度上表明，在该样方尺度下，开放水域或耕地大于6.5%比例时，具有形成冷点的潜在效应。


```python
scenes_cold_all={idx:Chicago_LC_reproj_.rio.clip([mapping(df.geometry)],cold_spot_gdf.crs) for idx,df in cold_spot_gdf.iterrows() }
```


```python
cold_spot_class_clumpSize=[]
for idx,scene in tqdm(scenes_cold_all.items()):
    clump_labels=cc3d.connected_components(scene.data,connectivity=8,out_dtype=np.uint64) 
    class_clumpSize_histogram=usda_signature.class_clumpSize_histogram(scene.data,clump_labels) 
    cold_spot_class_clumpSize.append(class_clumpSize_histogram)
```

    100%|██████████| 344/344 [00:01<00:00, 213.14it/s]
    


```python
cold_spot_class_clumpSize_concat=pd.concat(cold_spot_class_clumpSize).groupby(level=0).sum()
cold_spot_class_clumpSize_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>class</th>
      <th>11</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>41</th>
      <th>43</th>
      <th>71</th>
      <th>90</th>
      <th>95</th>
      <th>82</th>
      <th>31</th>
      <th>81</th>
      <th>52</th>
      <th>42</th>
    </tr>
    <tr>
      <th>inds</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>109.0</td>
      <td>665.0</td>
      <td>522.0</td>
      <td>513.0</td>
      <td>254.0</td>
      <td>121.0</td>
      <td>51.0</td>
      <td>154.0</td>
      <td>98.0</td>
      <td>113.0</td>
      <td>19.0</td>
      <td>82.0</td>
      <td>57.0</td>
      <td>12.0</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>155.0</td>
      <td>898.0</td>
      <td>798.0</td>
      <td>713.0</td>
      <td>224.0</td>
      <td>252.0</td>
      <td>155.0</td>
      <td>229.0</td>
      <td>276.0</td>
      <td>179.0</td>
      <td>46.0</td>
      <td>150.0</td>
      <td>156.0</td>
      <td>27.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>241.0</td>
      <td>1074.0</td>
      <td>1061.0</td>
      <td>892.0</td>
      <td>239.0</td>
      <td>626.0</td>
      <td>333.0</td>
      <td>534.0</td>
      <td>343.0</td>
      <td>282.0</td>
      <td>65.0</td>
      <td>180.0</td>
      <td>162.0</td>
      <td>100.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>330.0</td>
      <td>1335.0</td>
      <td>1288.0</td>
      <td>907.0</td>
      <td>257.0</td>
      <td>1121.0</td>
      <td>309.0</td>
      <td>669.0</td>
      <td>404.0</td>
      <td>262.0</td>
      <td>132.0</td>
      <td>296.0</td>
      <td>303.0</td>
      <td>98.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>674.0</td>
      <td>1387.0</td>
      <td>1665.0</td>
      <td>1058.0</td>
      <td>415.0</td>
      <td>1724.0</td>
      <td>126.0</td>
      <td>783.0</td>
      <td>1101.0</td>
      <td>497.0</td>
      <td>547.0</td>
      <td>237.0</td>
      <td>697.0</td>
      <td>91.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>835.0</td>
      <td>1640.0</td>
      <td>1753.0</td>
      <td>1264.0</td>
      <td>393.0</td>
      <td>2417.0</td>
      <td>84.0</td>
      <td>617.0</td>
      <td>1530.0</td>
      <td>114.0</td>
      <td>341.0</td>
      <td>504.0</td>
      <td>520.0</td>
      <td>93.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>735.0</td>
      <td>404.0</td>
      <td>2146.0</td>
      <td>1367.0</td>
      <td>1065.0</td>
      <td>3581.0</td>
      <td>0.0</td>
      <td>535.0</td>
      <td>2111.0</td>
      <td>100.0</td>
      <td>994.0</td>
      <td>1140.0</td>
      <td>260.0</td>
      <td>145.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>3638.0</td>
      <td>535.0</td>
      <td>2893.0</td>
      <td>1306.0</td>
      <td>1272.0</td>
      <td>4168.0</td>
      <td>0.0</td>
      <td>222.0</td>
      <td>4455.0</td>
      <td>0.0</td>
      <td>2708.0</td>
      <td>2229.0</td>
      <td>916.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>4332.0</td>
      <td>1048.0</td>
      <td>1791.0</td>
      <td>668.0</td>
      <td>2965.0</td>
      <td>4312.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2379.0</td>
      <td>0.0</td>
      <td>4039.0</td>
      <td>1306.0</td>
      <td>285.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>121508.0</td>
      <td>620.0</td>
      <td>1551.0</td>
      <td>0.0</td>
      <td>5791.0</td>
      <td>1205.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>28509.0</td>
      <td>606.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>14112.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
usda_vis.histogram_3d(cold_spot_class_clumpSize_concat,xlabel='clump-size',ylabel='class',zlabel='frequency',roll=-30,figsize=(8,8),zoom=0.9)  
```


<img src="./imgs/3_5/output_185_0.png" height='auto' width='auto' title="caDesign">    


* 计算冷点共现关系标记特征

从计算结果来看，以出现的土地覆盖类型自身邻接的频数居多，不同土地覆盖类型邻接中[21,22]、[22,23]、[23，24]和[41,90]有成对出现。

> 21（Developed, Open Space），22（Developed, Low Intensity），23（Developed, Medium Intensity），24（Developed High Intensity），41（ Deciduous Forest），42（Evergreen Forest）


```python
cold_spot_class_pairs=[]
for idx,scene in tqdm(scenes_cold_all.items()):
    class_pairs_frequency=usda_signature.class_co_occurrence(scene.data)
    cold_spot_class_pairs.append(class_pairs_frequency)
```

    100%|██████████| 312/312 [00:23<00:00, 13.51it/s]
    


```python
cold_spot_class_pairs_concat=pd.concat(cold_spot_class_pairs).groupby(level=0).sum()
cold_spot_class_pairs_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(11.0, 11.0)</th>
      <td>544178.0</td>
    </tr>
    <tr>
      <th>(11.0, 21.0)</th>
      <td>380.0</td>
    </tr>
    <tr>
      <th>(11.0, 22.0)</th>
      <td>743.0</td>
    </tr>
    <tr>
      <th>(11.0, 23.0)</th>
      <td>1244.0</td>
    </tr>
    <tr>
      <th>(11.0, 24.0)</th>
      <td>2024.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>(82.0, 90.0)</th>
      <td>179.0</td>
    </tr>
    <tr>
      <th>(82.0, 95.0)</th>
      <td>107.0</td>
    </tr>
    <tr>
      <th>(90.0, 90.0)</th>
      <td>31880.0</td>
    </tr>
    <tr>
      <th>(90.0, 95.0)</th>
      <td>2904.0</td>
    </tr>
    <tr>
      <th>(95.0, 95.0)</th>
      <td>2159.0</td>
    </tr>
  </tbody>
</table>
<p>120 rows × 1 columns</p>
</div>




```python
cold_spot_class_pairs_concat.reset_index().plot.bar(x='index', y=0, rot=0,figsize=(20,5),xlabel='class pairs');
plt.xticks(rotation=90)
plt.show()
```

<img src="./imgs/3_5/output_189_0.png" height='auto' width='auto' title="caDesign">
    


* 计算冷点层级分解标记特征

从计算结果来看，层级分解标记特征对LST冷点分布的解释不明显。通常随着层级的增加，各类土地覆盖类型占比随之减少，就对冷点其主导作用的开放水域而言，在第1层级（2），低于1/4和高于1/2比例的数量都比较显著；在第2层级（4）也有一定数量的占比。


```python
cold_spot_class_hierarchical_decomposition=[]
for idx,scene in tqdm(scenes_cold_all.items()):
    class_hierarchical_decomposition=usda_signature.class_decomposition(scene.data)  
    cold_spot_class_hierarchical_decomposition.append(class_hierarchical_decomposition)
```

    100%|██████████| 312/312 [00:08<00:00, 36.98it/s]
    


```python
cold_spot_class_hierarchical_decomposition_concat=pd.concat(cold_spot_class_hierarchical_decomposition).groupby(level=[0,1]).sum()
cold_spot_class_hierarchical_decomposition_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>11.0</th>
      <th>21.0</th>
      <th>22.0</th>
      <th>23.0</th>
      <th>24.0</th>
      <th>41.0</th>
      <th>43.0</th>
      <th>90.0</th>
      <th>95.0</th>
      <th>71.0</th>
      <th>82.0</th>
      <th>31.0</th>
      <th>81.0</th>
      <th>52.0</th>
      <th>42.0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3" valign="top">2</th>
      <th>1</th>
      <td>16732.0</td>
      <td>19537.0</td>
      <td>22733.0</td>
      <td>24212.0</td>
      <td>17818.0</td>
      <td>14473.0</td>
      <td>10461.0</td>
      <td>11336.0</td>
      <td>12955.0</td>
      <td>21194.0</td>
      <td>5453.0</td>
      <td>11140.0</td>
      <td>11801.0</td>
      <td>5632.0</td>
      <td>1165.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1070.0</td>
      <td>2812.0</td>
      <td>3869.0</td>
      <td>2709.0</td>
      <td>1293.0</td>
      <td>2786.0</td>
      <td>434.0</td>
      <td>2023.0</td>
      <td>611.0</td>
      <td>1072.0</td>
      <td>912.0</td>
      <td>888.0</td>
      <td>619.0</td>
      <td>163.0</td>
      <td>11.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>34138.0</td>
      <td>975.0</td>
      <td>2014.0</td>
      <td>1107.0</td>
      <td>2645.0</td>
      <td>3321.0</td>
      <td>81.0</td>
      <td>2321.0</td>
      <td>154.0</td>
      <td>470.0</td>
      <td>5591.0</td>
      <td>1300.0</td>
      <td>516.0</td>
      <td>85.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">4</th>
      <th>1</th>
      <td>4148.0</td>
      <td>5052.0</td>
      <td>5747.0</td>
      <td>6171.0</td>
      <td>4477.0</td>
      <td>3525.0</td>
      <td>2650.0</td>
      <td>2802.0</td>
      <td>3277.0</td>
      <td>5349.0</td>
      <td>1319.0</td>
      <td>2781.0</td>
      <td>2953.0</td>
      <td>1406.0</td>
      <td>293.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>289.0</td>
      <td>573.0</td>
      <td>875.0</td>
      <td>593.0</td>
      <td>275.0</td>
      <td>786.0</td>
      <td>87.0</td>
      <td>487.0</td>
      <td>130.0</td>
      <td>226.0</td>
      <td>187.0</td>
      <td>214.0</td>
      <td>158.0</td>
      <td>47.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8548.0</td>
      <td>206.0</td>
      <td>532.0</td>
      <td>243.0</td>
      <td>687.0</td>
      <td>834.0</td>
      <td>7.0</td>
      <td>631.0</td>
      <td>23.0</td>
      <td>109.0</td>
      <td>1483.0</td>
      <td>337.0</td>
      <td>123.0</td>
      <td>17.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">8</th>
      <th>1</th>
      <td>733.0</td>
      <td>955.0</td>
      <td>1099.0</td>
      <td>1155.0</td>
      <td>821.0</td>
      <td>656.0</td>
      <td>499.0</td>
      <td>495.0</td>
      <td>619.0</td>
      <td>1001.0</td>
      <td>239.0</td>
      <td>518.0</td>
      <td>549.0</td>
      <td>264.0</td>
      <td>54.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>63.0</td>
      <td>90.0</td>
      <td>140.0</td>
      <td>103.0</td>
      <td>65.0</td>
      <td>151.0</td>
      <td>5.0</td>
      <td>121.0</td>
      <td>9.0</td>
      <td>38.0</td>
      <td>33.0</td>
      <td>34.0</td>
      <td>31.0</td>
      <td>5.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1589.0</td>
      <td>26.0</td>
      <td>75.0</td>
      <td>29.0</td>
      <td>113.0</td>
      <td>138.0</td>
      <td>0.0</td>
      <td>104.0</td>
      <td>2.0</td>
      <td>5.0</td>
      <td>277.0</td>
      <td>60.0</td>
      <td>14.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
usda_vis.histogram_3d(cold_spot_class_hierarchical_decomposition_concat,xlabel='nested quadrants',ylabel='class',zlabel='frequency',roll=-30,figsize=(8,8),zoom=0.9) 
```

<img src="./imgs/3_5/output_193_0.png" height='auto' width='auto' title="caDesign">
    



* 计算热点的类/簇大小、共现关系和层级分解标记特征

同冷点的标记特征计算方法，热点的类/簇大小标记特征结果显示22（ Developed, Low Intensity）、23（ Developed, Medium Intensity）和24（Developed High Intensity）土地覆盖类型起主导作用，在各类层级上均有较高的比例，以$[ 2^{14},   2^{16} )$组距内数量为主；共现关系标记特征结果显示22、23自身及22和23对出现频率较高。

* 提取热点对应的土地覆盖类型样方（随机采样10个样方），观察样方内土地覆盖类型分布情况


```python
hot_spot_gdf=LST_gdf[LST_gdf['cl_li']=='1 hot spot']
print(hot_spot_gdf.shape)

hot_spot_sample=hot_spot_gdf.sample(n=10)
scenes_hot={idx:Chicago_LC_reproj_.rio.clip([mapping(df.geometry)],hot_spot_sample.crs) for idx,df in hot_spot_sample.iterrows() }

fig, axs=plt.subplots(nrows=2, ncols=5, figsize=(9*2, 4*2),subplot_kw={'xticks': [], 'yticks': []})
for ax, (idx,scene) in zip(axs.flat, scenes_hot.items()):
    scene.plot.imshow(add_colorbar=False,cmap=cmap_LC,norm=norm,ax=ax)
    ax.set_title(idx)
    ax.set_axis_off()
                 
plt.tight_layout()
plt.show() 
```

    (833, 7)
    

<img src="./imgs/3_5/output_195_1.png" height='auto' width='auto' title="caDesign">



* 计算热点类/簇大小标记特征


```python
scenes_hot_all={idx:Chicago_LC_reproj_.rio.clip([mapping(df.geometry)],hot_spot_gdf.crs) for idx,df in hot_spot_gdf.iterrows() }
```


```python
hot_spot_class_clumpSize=[]
for idx,scene in tqdm(scenes_hot_all.items()):
    clump_labels=cc3d.connected_components(scene.data,connectivity=8,out_dtype=np.uint64) 
    class_clumpSize_histogram=usda_signature.class_clumpSize_histogram(scene.data,clump_labels) 
    hot_spot_class_clumpSize.append(class_clumpSize_histogram)
```

    100%|██████████| 841/841 [00:04<00:00, 199.90it/s]
    


```python
hot_spot_class_clumpSize_concat=pd.concat(hot_spot_class_clumpSize).groupby(level=0).sum()
hot_spot_class_clumpSize_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>class</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>31</th>
      <th>41</th>
      <th>71</th>
      <th>95</th>
      <th>43</th>
      <th>52</th>
      <th>90</th>
      <th>11</th>
      <th>81</th>
      <th>82</th>
      <th>42</th>
      <th>-2147483648</th>
    </tr>
    <tr>
      <th>inds</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>3884.0</td>
      <td>5141.0</td>
      <td>4543.0</td>
      <td>3193.0</td>
      <td>259.0</td>
      <td>238.0</td>
      <td>427.0</td>
      <td>270.0</td>
      <td>104.0</td>
      <td>60.0</td>
      <td>259.0</td>
      <td>132.0</td>
      <td>241.0</td>
      <td>97.0</td>
      <td>17.0</td>
      <td>784.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5509.0</td>
      <td>8695.0</td>
      <td>7560.0</td>
      <td>3974.0</td>
      <td>367.0</td>
      <td>632.0</td>
      <td>715.0</td>
      <td>417.0</td>
      <td>196.0</td>
      <td>159.0</td>
      <td>610.0</td>
      <td>278.0</td>
      <td>608.0</td>
      <td>209.0</td>
      <td>13.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6913.0</td>
      <td>11495.0</td>
      <td>9296.0</td>
      <td>5644.0</td>
      <td>460.0</td>
      <td>1285.0</td>
      <td>1159.0</td>
      <td>734.0</td>
      <td>357.0</td>
      <td>306.0</td>
      <td>1030.0</td>
      <td>517.0</td>
      <td>910.0</td>
      <td>303.0</td>
      <td>24.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7633.0</td>
      <td>14251.0</td>
      <td>12140.0</td>
      <td>7630.0</td>
      <td>450.0</td>
      <td>1857.0</td>
      <td>1498.0</td>
      <td>837.0</td>
      <td>329.0</td>
      <td>278.0</td>
      <td>1355.0</td>
      <td>780.0</td>
      <td>1179.0</td>
      <td>806.0</td>
      <td>31.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>7963.0</td>
      <td>17729.0</td>
      <td>15655.0</td>
      <td>10186.0</td>
      <td>453.0</td>
      <td>2146.0</td>
      <td>1557.0</td>
      <td>764.0</td>
      <td>122.0</td>
      <td>289.0</td>
      <td>1875.0</td>
      <td>1826.0</td>
      <td>1324.0</td>
      <td>1928.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5572.0</td>
      <td>20390.0</td>
      <td>19156.0</td>
      <td>12712.0</td>
      <td>447.0</td>
      <td>2488.0</td>
      <td>1616.0</td>
      <td>685.0</td>
      <td>42.0</td>
      <td>348.0</td>
      <td>2848.0</td>
      <td>1749.0</td>
      <td>1444.0</td>
      <td>3277.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>4794.0</td>
      <td>23780.0</td>
      <td>28885.0</td>
      <td>16286.0</td>
      <td>410.0</td>
      <td>1984.0</td>
      <td>1060.0</td>
      <td>861.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2912.0</td>
      <td>2704.0</td>
      <td>1026.0</td>
      <td>4716.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>4754.0</td>
      <td>40370.0</td>
      <td>52149.0</td>
      <td>18555.0</td>
      <td>1198.0</td>
      <td>1762.0</td>
      <td>431.0</td>
      <td>966.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2991.0</td>
      <td>1573.0</td>
      <td>129.0</td>
      <td>6415.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2609.0</td>
      <td>60238.0</td>
      <td>78572.0</td>
      <td>22524.0</td>
      <td>300.0</td>
      <td>1400.0</td>
      <td>368.0</td>
      <td>374.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>877.0</td>
      <td>0.0</td>
      <td>264.0</td>
      <td>9580.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>516.0</td>
      <td>17462.0</td>
      <td>16368.0</td>
      <td>7311.0</td>
      <td>0.0</td>
      <td>1055.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3357.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.0</td>
      <td>1069.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
usda_vis.histogram_3d(hot_spot_class_clumpSize_concat,xlabel='clump-size',ylabel='class',zlabel='frequency',roll=-30,figsize=(8,8),zoom=0.9)  
```

<img src="./imgs/3_5/output_200_0.png" height='auto' width='auto' title="caDesign">
    



* 计算热点共现关系标记特征


```python
hot_spot_class_pairs=[]
for idx,scene in tqdm(scenes_hot_all.items()):
    try:
        class_pairs_frequency=usda_signature.class_co_occurrence(scene.data)
        hot_spot_class_pairs.append(class_pairs_frequency)
    except:
        pass
```

    100%|██████████| 833/833 [00:48<00:00, 17.00it/s]

    [1646]
    

    
    


```python
hot_spot_class_pairs_concat=pd.concat(hot_spot_class_pairs).groupby(level=0).sum()
hot_spot_class_pairs_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(11.0, 11.0)</th>
      <td>23872.0</td>
    </tr>
    <tr>
      <th>(11.0, 21.0)</th>
      <td>1417.0</td>
    </tr>
    <tr>
      <th>(11.0, 22.0)</th>
      <td>5027.0</td>
    </tr>
    <tr>
      <th>(11.0, 23.0)</th>
      <td>4120.0</td>
    </tr>
    <tr>
      <th>(11.0, 24.0)</th>
      <td>1870.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>(82.0, 90.0)</th>
      <td>1123.0</td>
    </tr>
    <tr>
      <th>(82.0, 95.0)</th>
      <td>1649.0</td>
    </tr>
    <tr>
      <th>(90.0, 90.0)</th>
      <td>35805.0</td>
    </tr>
    <tr>
      <th>(90.0, 95.0)</th>
      <td>7596.0</td>
    </tr>
    <tr>
      <th>(95.0, 95.0)</th>
      <td>11943.0</td>
    </tr>
  </tbody>
</table>
<p>120 rows × 1 columns</p>
</div>




```python
hot_spot_class_pairs_concat.reset_index().plot.bar(x='index', y=0, rot=0,figsize=(20,5),xlabel='class pairs');
plt.xticks(rotation=90)
plt.show()
```


<img src="./imgs/3_5/output_204_0.png" height='auto' width='auto' title="caDesign">    


* 计算热点层级分解标记特征


```python
hot_spot_class_hierarchical_decomposition=[]
for idx,scene in tqdm(scenes_hot_all.items()):
    try:
        class_hierarchical_decomposition=usda_signature.class_decomposition(scene.data)  
        hot_spot_class_hierarchical_decomposition.append(class_hierarchical_decomposition)
    except:
        pass
```

    100%|██████████| 833/833 [00:46<00:00, 18.02it/s]
    


```python
hot_spot_class_hierarchical_decomposition_concat=pd.concat(hot_spot_class_hierarchical_decomposition).groupby(level=[0,1]).sum()
hot_spot_class_hierarchical_decomposition_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>21.0</th>
      <th>22.0</th>
      <th>23.0</th>
      <th>24.0</th>
      <th>31.0</th>
      <th>41.0</th>
      <th>71.0</th>
      <th>95.0</th>
      <th>43.0</th>
      <th>52.0</th>
      <th>90.0</th>
      <th>11.0</th>
      <th>81.0</th>
      <th>82.0</th>
      <th>42.0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3" valign="top">2</th>
      <th>1</th>
      <td>135723.0</td>
      <td>75966.0</td>
      <td>68102.0</td>
      <td>120762.0</td>
      <td>43790.0</td>
      <td>61357.0</td>
      <td>77103.0</td>
      <td>43398.0</td>
      <td>26612.0</td>
      <td>22465.0</td>
      <td>54578.0</td>
      <td>50013.0</td>
      <td>62938.0</td>
      <td>42295.0</td>
      <td>5029.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17198.0</td>
      <td>51860.0</td>
      <td>55045.0</td>
      <td>21086.0</td>
      <td>1037.0</td>
      <td>3206.0</td>
      <td>2671.0</td>
      <td>1674.0</td>
      <td>536.0</td>
      <td>500.0</td>
      <td>3103.0</td>
      <td>1765.0</td>
      <td>2156.0</td>
      <td>2948.0</td>
      <td>57.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6035.0</td>
      <td>35050.0</td>
      <td>39729.0</td>
      <td>18284.0</td>
      <td>645.0</td>
      <td>2469.0</td>
      <td>1174.0</td>
      <td>792.0</td>
      <td>96.0</td>
      <td>163.0</td>
      <td>2099.0</td>
      <td>1534.0</td>
      <td>958.0</td>
      <td>5913.0</td>
      <td>10.0</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">4</th>
      <th>1</th>
      <td>35305.0</td>
      <td>18996.0</td>
      <td>16266.0</td>
      <td>30562.0</td>
      <td>11029.0</td>
      <td>15358.0</td>
      <td>19437.0</td>
      <td>10952.0</td>
      <td>6712.0</td>
      <td>5656.0</td>
      <td>13685.0</td>
      <td>12463.0</td>
      <td>15834.0</td>
      <td>10443.0</td>
      <td>1265.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3248.0</td>
      <td>12345.0</td>
      <td>13536.0</td>
      <td>4846.0</td>
      <td>187.0</td>
      <td>821.0</td>
      <td>561.0</td>
      <td>337.0</td>
      <td>91.0</td>
      <td>97.0</td>
      <td>775.0</td>
      <td>505.0</td>
      <td>488.0</td>
      <td>818.0</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1186.0</td>
      <td>9378.0</td>
      <td>10917.0</td>
      <td>4625.0</td>
      <td>152.0</td>
      <td>579.0</td>
      <td>239.0</td>
      <td>177.0</td>
      <td>8.0</td>
      <td>29.0</td>
      <td>485.0</td>
      <td>360.0</td>
      <td>191.0</td>
      <td>1528.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">8</th>
      <th>1</th>
      <td>6733.0</td>
      <td>3459.0</td>
      <td>2736.0</td>
      <td>5616.0</td>
      <td>2037.0</td>
      <td>2880.0</td>
      <td>3625.0</td>
      <td>2038.0</td>
      <td>1249.0</td>
      <td>1050.0</td>
      <td>2545.0</td>
      <td>2310.0</td>
      <td>2940.0</td>
      <td>1902.0</td>
      <td>233.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>439.0</td>
      <td>2465.0</td>
      <td>2894.0</td>
      <td>972.0</td>
      <td>33.0</td>
      <td>122.0</td>
      <td>72.0</td>
      <td>50.0</td>
      <td>2.0</td>
      <td>12.0</td>
      <td>145.0</td>
      <td>104.0</td>
      <td>81.0</td>
      <td>190.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>127.0</td>
      <td>1555.0</td>
      <td>1849.0</td>
      <td>765.0</td>
      <td>18.0</td>
      <td>76.0</td>
      <td>20.0</td>
      <td>18.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>55.0</td>
      <td>34.0</td>
      <td>12.0</td>
      <td>257.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
usda_vis.histogram_3d(hot_spot_class_hierarchical_decomposition_concat,xlabel='nested quadrants',ylabel='class',zlabel='frequency',roll=-30,figsize=(8,8),zoom=0.9) 
```

<img src="./imgs/3_5/output_208_0.png" height='auto' width='auto' title="caDesign">
    


通过LISA冷热点分布对应土地覆盖类型，计算样方标记特征，分析土地覆盖类型特征对冷热点的影响；也可以对连续的LST数据执行分割形成区域集，对区域集对应的土地覆盖类型样方计算标记特征，或者对区域集执行LISA计算获得冷热点后再对应计算样方土地覆盖类型的标记特征。此处仅定义了`Img_gray_regionGrow()`方法，基于区域增长（Region Growing）算法执行像素级别单波段的分割，获得区域集。


```python
LST_instance=usda_signature.Img_gray_regionGrow(im=LST_temp[0],th=23)
LST_instance.ApplyRegionGrow()
LST_segment=LST_instance.SEGS

f, ax=plt.subplots(figsize=(20,20))
cmap=matplotlib.colors.ListedColormap (np.random.rand(256,3))

show(LST_segment,ax=ax,transform=LST_transform,cmap=cmap)
plt.show()
```

    Iterations : 11031
    


<img src="./imgs/3_5/output_210_1.png" height='auto' width='auto' title="caDesign">    



```python
np.unique(LST_segment)
```




    array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=uint8)



参考样方标记特征和距离度量的模式级聚、搜索、监测和分割等方法，可以给定一个样方，计算该样方到热点、冷点或者其它分割区域集对应样方标记特征的距离，从而可以判断给定样方划分为冷热点或者其它LST分类的机会，这为规划土地在城市热环境方面的优化提供了量化的参考方式；可以分析冷热点对应样方标记特征距离的层级聚类，进一步提取土地覆盖类型的组成和结构，这包括冷热点或同一区域集内部样方距离的层级聚类，也包括冷热点或区域集间的距离比较；可以将LST值的变化对应到不同样方土地覆盖类型的变化上，分析样方标记特征的距离变化与LST温度变化的关系等都可以对城市土地覆盖类型规划评估、更新优化或者管理措施提供量化参照。样方标记特征和距离度量的模式特征方法，能够应用到更为广泛地理空间模式的探索内容上，这包括应对更高分辨率数据集的复杂空间模式分析。

---

注释（Notes）：

① scikit-learn，（<https://scikit-learn.org/stable>）。

② PySAL，（<https://pysal.org>）。

③ NLCD（National Land Cover Database），美国土地覆盖类型数据集，（<https://www.mrlc.gov/data/legends/national-land-cover-database-class-legend-and-description>）。

④ USDA，配套《城市空间数据方法》本书的Python库，（<https://richiebao.github.io/USDA_PyPI>）。

⑤ GeoPAT, (<https://github.com/Nowosad/geopat2>)。

⑥ GRASS GIS，（<https://grass.osgeo.org>）。

⑦ connected-components-3d，（<https://github.com/seung-lab/connected-components-3d>）。

参考文献（References）:

[1] Tian, Y., Tsendbazar, N. E., van Leeuwen, E., Fensholt, R. & Herold, M. A global analysis of multifaceted urbanization patterns using Earth Observation data from 1975 to 2015. Landsc Urban Plan 219, (2022).

[2] Hahus, I., Migliaccio, K., Douglas-Mankin, K., Klarenberg, G. & Muñoz-Carpena, R. Using Cluster Analysis to Compartmentalize a Large Managed Wetland Based on Physical, Biological, and Climatic Geospatial Attributes. Environ Manage 62, 571–583 (2018).

[3] Guo, G. et al. Impacts of urban biophysical composition on land surface temperature in urban heat island clusters. Landsc Urban Plan 135, 1–10 (2015).

[4] Talen, E., Anselin, L., Lee, S. & Koschinsky, J. Looking for logic: The zoning-land use mismatch. Landsc Urban Plan 152, 27–38 (2016).

[5] Gao, Y. et al. Clustering Urban Multifunctional Landscapes Using the Self-Organizing Feature Map Neural Network Model. J Urban Plan Dev 140, (2014).

[6] Schmiedel, I., Bergmeier, E. & Culmsee, H. Plant species richness patterns along a gradient of landscape modification intensity in Lower Saxony, Germany. Landsc Urban Plan 141, 41–51 (2015).

[7] Chen, Y. et al. Delineating urban functional areas with building-level social media data: A dynamic time warping (DTW) distance based k-medoids method. Landsc Urban Plan 160, 48–60 (2017).

[8] Yoshimura, Y., Santi, P., Arias, J. M., Zheng, S. & Ratti, C. Spatial clustering: Influence of urban street networks on retail sales volumes. Environ Plan B Urban Anal City Sci 48, 1926–1942 (2021).

[9] Kracalik, I. T. et al. Analysing the spatial patterns of livestock anthrax in Kazakhstan in relation to environmental factors: a comparison of local (G i *) and morphology cluster statistics. http://www.jennessent.com/arcgis/repeat_.

[10] Maïnassara, H. B., Molinari, N., Dematteï, C. & Fabbro-Peray, P. The relative risk of spatial cluster occurrence and spatio-temporal evolution of meningococcal disease in Niger, 2002-2008. http://vtopo.free.fr/.

[11] Hahus, I., Migliaccio, K., Douglas-Mankin, K. R., Klarenberg, G., & Muñoz-Carpena, R. (2018). Using cluster analysis to compartmentalize a large managed wetland based on physical, biological, and climatic geospatial attributes. Environmental Management, 62(3), 571–583. doi:10.1007/s00267-018-1050-5

[12] Vizzari, M. & Sigura, M. Landscape sequences along the urban-rural-natural gradient: A novel geospatial approach for identification and analysis. Landsc Urban Plan 140, 42–55 (2015).

[13] Kefalas, G., Kalogirou, S., Poirazidis, K. & Lorilla, R. S. Landscape transition in Mediterranean islands: The case of Ionian islands, Greece 1985–2015. Landsc Urban Plan 191, (2019).

[14] Lee, K. H., Dvorak, R. G., Schuett, M. A. & van Riper, C. J. Understanding spatial variation of physical inactivity across the continental United States. Landsc Urban Plan 168, 61–71 (2017).

[15] Stepinski, T. F., Niesterowicz, J. & Jasiewicz, J. Pattern-based regionalization of large geospatial datasets using complex object-based image analysis. in Procedia Computer Science vol. 51 2168–2177 (Elsevier B.V., 2015).

[16] Feature agglomeration vs. univariate selection，<https://scikit-learn.org/stable/auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py>.

[17] mapclassify: Classification Schemes for Choropleth Maps, <https://pypi.org/project/mapclassify/2.5.0/>.

[18] Duque, J. C., Anselin, L. & Rey, S. J. The Max-p-Regions Problem. (2010).

[19] Vatsavai, R. (2013). Gaussian multiple instance learning approach for mapping the slums of the world using very high resolution imagery. doi:10.1145/2487575.2488210

[20] Jasiewicz, J., Netzel, P. & Stepinski, T. GeoPAT: A toolbox for pattern-based information retrieval from large geospatial databases. Comput Geosci 80, 62–73 (2015).

[21] GeoPAT 2 user’s manual pawel netzel jakub nowosad jaroslaw jasiewicz jacek niesterowicz tomasz stepinski. http://sil.uc.eduCincinnati2018.

[22] A. Rosenfeld and J. Pfaltz. "Sequential Operations in Digital Picture Processing". Journal of the ACM. Vol. 13, Issue 4, Oct. 1966, Pg. 471-494. doi: 10.1145/321356.321357

[23] K. Wu, E. Otoo, K. Suzuki. "Two Strategies to Speed up Connected Component Labeling Algorithms". Lawrence Berkeley National Laboratory. LBNL-29102, 2005.

[24] Region-Growing, <https://github.com/Spinkoo/Region-Growing>.

[25] Jasiewicz, J. & Stepinski, T. F. Example-based retrieval of alike land-cover scenes from NLCD2006 database. IEEE Geoscience and Remote Sensing Letters 10, 155–159 (2013).

[26] Barnsley, M. J. & Barr, S. L. Inferring Urban Land Use from Satellite Sensor Images Using Kernel-Based Spatial Reclassification.

[27] Remmel, T. K. & Csillag, F. Mutual information spectra for comparing categorical maps. Int J Remote Sens 27, 1425–1452 (2006).

[28] Cha, S.-H. (2007). Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions. Int. J. Math. Model. Meth. Appl. Sci., 1.

[29] scipy.cluster.hierarchy.linkag, <https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html>.

[30] Cophenetic correlation, <https://en.wikipedia.org/wiki/Cophenetic_correlation>.

[31] Stepinski, T. F., Netzel, P. & Jasiewicz, J. LandEx - A geoweb tool for query and retrieval of spatial patterns in land cover datasets. IEEE J Sel Top Appl Earth Obs Remote Sens 7, 257–266 (2014).

[32] Netzel, P. & Stepinski, T. F. Pattern-based assessment of land cover change on continental scale with application to NLCD 2001-2006. IEEE Transactions on Geoscience and Remote Sensing 53, 1773–1781 (2015).

[33] Niesterowicz, J. & Stepinski, T. F. Regionalization of multi-categorical landscapes using machine vision methods. Applied Geography 45, 250–258 (2013).

[34] Zucker, S. W. (1976). Region growing: Childhood and adolescence. Computer Graphics and Image Processing, 5(3), 382–399. doi:10.1016/S0146-664X(76)80014-7.

[35] Gao, J., Gong, J., Yang, J., Li, J. & Li, S. Measuring Spatial Connectivity between patches of the heat source and sink (SCSS): A new index to quantify the heterogeneity impacts of landscape patterns on land surface temperature. Landsc Urban Plan 217, (2022).

[36] Wang, C., Li, Y., Myint, S. W., Zhao, Q. & Wentz, E. A. Impacts of spatial clustering of urban land cover on land surface temperature across Köppen climate zones in the contiguous United States. Landsc Urban Plan 192, (2019).

[37] MYD21A1D v006, <https://lpdaac.usgs.gov/products/myd21a1dv006/>.
