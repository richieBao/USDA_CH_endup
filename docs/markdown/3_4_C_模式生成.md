Created on Mon Apr 17 07:57:11 2023 @author: Richie Bao-caDesignè®¾è®¡(cadesign.cn)

# 3.4-C æ¨¡å¼ç”Ÿæˆï¼šcGANâ€”â€”æ¨¡å¼è½¬åŒ–å’ŒæœªçŸ¥åŒºåŸŸåœ°ç†ä¿¡æ¯ä¿®å¤


## 3.4.1 æ¡ä»¶å¯¹æŠ—ç½‘ç»œï¼ˆConditional Adversarial Networksï¼ŒcGANï¼‰ä¸åœ°ç†ç©ºé—´æ¨¡å¼æ½œåœ¨åº”ç”¨æ–¹å¼

åœ¨åœ°ç†ç©ºé—´æ•°æ®åˆ†æä¸­ï¼Œæœ€é‡è¦çš„ä¿¡æ¯æ¥æºä¹‹ä¸€æ˜¯é¥æ„Ÿï¼ˆèˆªæ‹ï¼‰å½±åƒæ•°æ®ï¼Œä¸”åŒ…å«æœ‰å¤šä¸ªæ³¢æ®µï¼Œæ ¹æ®ä¸åŒæ³¢æ®µçš„ç‰¹æ€§åˆ†æç›¸å…³å†…å®¹ï¼Œè¿™åœ¨Landsaté¥æ„Ÿå½±åƒã€Sentinel-2å’ŒNAIPèˆªæ‹å½±åƒç­‰ç« èŠ‚ä¸­å‡æœ‰é˜é‡Šã€‚é¥æ„Ÿå½±åƒï¼ˆRGBæ³¢æ®µï¼‰åˆæˆçš„è‡ªç„¶çœŸå½©è‰²å›¾åƒåº”ç”¨ä¸ºå«æ˜Ÿåœ°å›¾è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†æ˜¯ä¸ºäº†è¿›è¡ŒåŸå¸‚ç©ºé—´æ¨¡å¼çš„åˆ†æï¼Œéœ€è¦ä»å½±åƒåœ°å›¾ä¸­æå–ä¿¡æ¯ï¼Œä¾‹å¦‚åæ¼”åœ°è¡¨æ¸©åº¦ã€è®¡ç®—NDVIï¼ˆå½’ä¸€åŒ–æ¤è¢«æŒ‡æ•°ï¼‰ã€NDWIï¼ˆå½’ä¸€åŒ–æ°´ä½“æŒ‡æ•°ï¼‰å’ŒNDBIï¼ˆå½’ä¸€åŒ–å»ºç­‘æŒ‡æ•°ï¼‰ç­‰å„ç±»æŒ‡æ•°ç­‰ï¼Œå…¶ä¸­å½±åƒè§£è¯‘ä¸ºåœŸåœ°è¦†ç›–ç±»å‹ï¼ˆland coverï¼ŒLCï¼‰æ˜¯ç ”ç©¶åŸå¸‚ç©ºé—´æ¨¡å¼çš„åŸºç¡€ï¼Œèƒ½å¤Ÿåˆ†æåœ°ç‰©åˆ†å¸ƒç»“æ„ç­‰ç‰¹ç‚¹ï¼Œä¾‹å¦‚*æ ‡è®°è·ç¦»*ä¸€ç« å¯¹ LC çš„æ¨¡å¼æœç´¢ã€ç›‘æµ‹å’Œåˆ†å‰²ç­‰ã€‚æ ¹æ®æ³¢æ®µç‰¹å¾ï¼Œåº”ç”¨å½±åƒè§£è¯‘ç­‰æŠ€æœ¯ï¼Œä¾‹å¦‚[eCognition](https://geospatial.trimble.com/what-is-ecognition)<sup>â‘ </sup>é¢å‘å¯¹è±¡å½±åƒåˆ†å‰²åˆ†ç±»æ–¹æ³•ç­‰å·²ç»ç”Ÿäº§å¤§é‡è¦†ç›–è¦†ç›–å…¨çƒå’Œå¤šä¸ªè¿ç»­æ—¶é—´ç‚¹çš„ LC æ•°æ®ï¼Œä¸”ä»ç„¶åœ¨è¿›è¡Œä¸­ã€‚å› ä¸ºå…·æœ‰äº†å¤§é‡å·²ç»åˆ†ç±»çš„ LC ç­‰åœ°ç†ä¿¡æ¯æ•°æ®ï¼Œç”±æ¡ä»¶å¯¹æŠ—ç½‘ç»œï¼ˆcGANï¼‰å¯ä»¥æ„å»ºé¥æ„Ÿå½±åƒåˆ° LC æˆ– LC åˆ°é¥æ„Ÿå½±åƒçš„è½¬åŒ–æ¨¡å‹ï¼Œç”±æ­¤ä¼šä¸ºæ¨¡å¼çš„åˆ†æå¸¦æ¥æ½œåœ¨æ–°çš„ç ”ç©¶é€”å¾„ï¼Œè¿™åŒ…æ‹¬ï¼š

1. å°†$G$ç½‘ç»œï¼ˆä¾‹å¦‚ä½¿ç”¨ `U-Net`ï¼‰çš„ç¼–ç å™¨ï¼ˆencoderï¼‰ç»“æœç”¨äºæ¨¡å¼æ ‡è®°ç‰¹å¾ï¼Œåˆ†æåŸºäºå·²çŸ¥ LC åˆ†ç±»çš„æ ·æ–¹æ¨¡å¼ç‰¹å¾å’Œåˆ†å¸ƒåŠæ ·æ–¹é—´æ¨¡å¼ç»“æ„çš„æ¯”è¾ƒç­‰ï¼›
2. ç”¨$G$ç½‘ç»œè§£ç å™¨ï¼ˆdecoderï¼‰çš„ä¸åŒå±‚ï¼ˆé€†å·ç§¯å±‚ï¼‰åˆ†æå¯¹åº”å°ºåº¦ç©ºé—´æ·±åº¦åˆ†å¸ƒç‰¹å¾ï¼›
3. æ„å»ºå…·æœ‰å…³è”çš„å„ç±»å¯¹åº”åœ°ç†ç©ºé—´æ•°æ®é—´çš„æ˜ å°„ç½‘ç»œæ¨¡å‹ï¼Œä¾‹å¦‚ç”Ÿå¢ƒè´¨é‡ã€ç¢³å­˜å‚¨å’Œå›ºæŒã€å°æ°”å€™ç¯å¢ƒæ•°æ®ç­‰ï¼Œä½œ1ã€2æ¡çš„åˆ†æï¼›
4. è®¾è®¡å±‚é¢ï¼Œå°†ç»˜åˆ¶çš„ LC å®æ—¶ç”Ÿæˆå½±åƒå›¾ç‰‡ï¼ŒååŠ©è§„åˆ’è®¾è®¡<sup>[1]</sup>ï¼›
5. è®¾è®¡å±‚é¢ï¼Œæ ¹æ®å·²çŸ¥åœ°ç†ç©ºé—´æ•°æ®é¢„æµ‹æœªçŸ¥åŒºåŸŸä¿¡æ¯ï¼Œç”¨äºè§„åˆ’è®¾è®¡å‚è€ƒï¼Œå¹¶åˆ†æè¢«ä¿®å¤åŒºåŸŸæ½œåœ¨æ¨¡å¼çš„æ•°æ®åˆ†å¸ƒ;
6. è®¾è®¡å±‚é¢ï¼Œè¿ç§»ç‰¹å®šç±»å‹çš„åŸå¸‚ç©ºé—´æ¨¡å¼äºç»™å®šåŒºåŸŸï¼Œè§‚å¯Ÿæ¨¡å¼å˜åŒ–å½¢æ€ã€‚

å¯¹ cGAN çš„æ¢è®¨ä¸»è¦åŸºäºIsola, P., Zhu, J.-Y., Zhou, T. ç­‰äºº<sup>[2]</sup> å¯¹ cGAN å¤§é‡å·²æœ‰ç ”ç©¶å®ç°çš„ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬`Pix2pix GAN`å’Œ`CycleGAN`ä¸¤ä¸ªå›¾åƒï¼ˆè¾“å…¥ï¼‰åˆ°å›¾åƒï¼ˆè¾“å‡ºï¼‰çš„ç½‘ç»œæ¨¡å‹ï¼ˆè€Œ DCGANã€WGANå’ŒStyleGANç­‰è¾“å…¥ä¸ºå™ªå£°ï¼ˆéšè—ï¼‰å‘é‡ï¼‰ï¼Œå…¶ä¸­`Pix2pix GAN` ä¸ºæˆå¯¹çš„å›¾åƒäº’ç›¸åŒ¹é…ï¼Œä¾‹å¦‚å›¾åƒæˆ–é¥æ„Ÿå½±åƒçš„è¯­ä¹‰åˆ†å‰²å°†å½±åƒå’Œåˆ†ç±»å›¾å—å¯¹åº”èµ·æ¥ï¼Œæˆ–è€…å°†è½®å»“çº¿ä¸å¯¹è±¡å¯¹åº”èµ·æ¥ç­‰ï¼›`CycleGAN` æˆå¯¹å›¾åƒé—´æ²¡æœ‰åŒ¹é…å…³ç³»ï¼Œå¯ä»¥æŠŠä¸€ä¸ªå›¾åƒçš„ç‰¹å¾è¿ç§»è‡³å¦ä¸€ä¸ªå›¾åƒï¼Œä¸ºåŸŸè¿ç§»ï¼ˆDomain Adaptationï¼‰ã€‚ Isola, P. ç­‰ä½œè€…å¯¹åº”è®ºæ–‡å¼€å‘äº† [CycleGAN and pix2pix ](https://phillipi.github.io/pix2pix/)<sup>â‘¡</sup>å·¥å…·ï¼ŒåŒ…æ‹¬`Torch`ã€`Tensorflow`å’Œ`PyTorch`ç­‰ç‰ˆæœ¬ï¼Œè¿™é‡Œä½¿ç”¨[PyTorchç‰ˆæœ¬](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)<sup>[3]</sup>ï¼Œå¹¶å°†å…¶è¿ç§»è‡³`USDA`åº“è¿›è¡Œä½¿ç”¨ï¼Œä½äº`migrated_project.pix2pix`å­åŒ…ã€‚

åœ¨è¿ç§»æ—¶ï¼Œéœ€è¦æ³¨æ„æ¨¡å—ç›¸å¯¹è·¯å¾„è°ƒå…¥çš„æ–¹å¼ï¼Œå½“åœ¨æœ¬åœ°è°ƒè¯•æ—¶ï¼Œè°ƒå…¥æ¨¡å—æ–¹å¼é€šå¸¸ä¸º`from options import cfg_train`ï¼ˆä½äºåŒä¸€æ–‡ä»¶å¤¹ä¸‹ï¼‰ï¼›ä½†æ˜¯å¯¹äºPythonåŒ…ä¸€èˆ¬è¦åœ¨å‰é¢åŠ å…¥ç‚¹ï¼ˆåŒæ ·ä½äºåŒä¸€æ–‡ä»¶å¤¹ä¸‹ï¼‰ï¼Œä¸º`from .options import cfg_train `ï¼Œå› æ­¤ä¸ºäº†åœ¨æœ¬åœ°è°ƒè¯•ï¼Œä¸€èˆ¬å¯ä»¥ç”¨ä¸‹è¿°æ–¹å¼è°ƒå…¥ï¼ŒåŒæ—¶æ»¡è¶³æœ¬åœ°è°ƒè¯•è°ƒç”¨å’Œä½œä¸ºå®‰è£…åŒ…è°ƒç”¨ã€‚

```python
if __package__:    
    from .options import cfg_train 
    from .data import create_dataset
    from .models import create_model
    from .util._visualizer import Visualizer
    
else:
    from options import cfg_train 
    from data import create_dataset
    from models import create_model
    from util._visualizer import Visualizer
```

## 3.4.1.1 cGAN æ–¹æ³•

GANs ç”Ÿæˆç½‘ç»œ$G$æ˜¯ä»éšæœºå™ªå£°å‘é‡$z$è¾“å…¥åˆ°å›¾åƒ$y$è¾“å‡ºçš„æ˜ å°„ï¼Œ$ G: z \rightarrow y$ï¼Œè€ŒcGANs å­¦ä¹ ä»è§‚æµ‹åˆ°çš„è¾“å…¥å›¾åƒ$x$å’Œéšæœºå™ªå£°å‘é‡$z$åˆ°$y$çš„æ˜ å°„ï¼Œ$G:\{x, z\} \rightarrow y$ã€‚$G$è¢«è®­ç»ƒä¸ºäº§ç”Ÿæ— æ³•ä¸çœŸå®å›¾åƒåŒºåˆ†çš„è¾“å‡ºï¼Œè€Œåˆ¤åˆ«ç½‘ç»œ$D$è¢«è®­ç»ƒæˆå°½å¯èƒ½æ£€æµ‹å‡ºç”Ÿæˆå™¨çš„â€œä¼ªé€ â€è¾“å‡ºï¼ˆç”Ÿæˆå›¾åƒï¼‰ã€‚cGANçš„ç›®æ ‡ï¼ˆæŸå¤±å‡½æ•°ï¼‰å¯ä»¥è¡¨ç¤ºä¸ºï¼š$\mathcal{L}_{c G A N}(G, D)= \mathbb{E}_{x, y}[\log D(x, y)]+ \mathbb{E}_{x, z}[\log (1-D(x, G(x, z))] $ï¼Œå¼ä¸­ï¼Œ$G$è¯•å›¾æœ€å°åŒ–è¿™ä¸ªç›®æ ‡ï¼Œè€Œ$D$è¯•å›¾æœ€å¤§åŒ–è¿™ä¸ªç›®æ ‡ï¼Œå³$G^*=\arg \min _G \max _D \mathcal{L}_{c G A N}(G, D)$ã€‚æ ¹æ®ä½œè€…ç”±å·²æœ‰çš„ç ”ç©¶å‘ç°ï¼Œå°†cGANçš„ä¸Šè¿°ç›®æ ‡ä¸ä¼ ç»Ÿçš„æŸå¤±ï¼ˆLossï¼‰ï¼Œä¾‹å¦‚$L1$æˆ–$L2$è·ç¦»æ··åˆåœ¨ä¸€èµ·ä½¿ç”¨ä¼šä¼˜åŒ–ç½‘ç»œã€‚

å¯¹äº$L1$æŸå¤±å‡½æ•°ï¼Œä¹Ÿç§°ä¹‹ä¸ºæœ€å°åŒ–ç»å¯¹è¯¯å·®ï¼ˆLeast Abosulote Errorï¼ŒLAEï¼‰ï¼Œä¸ºæœ€å°åŒ–çœŸå®å€¼$y_i$å’Œé¢„æµ‹å€¼$f\left(x_i\right)$ä¹‹é—´å·®å€¼ç»å¯¹å€¼çš„å’Œï¼Œå…¬å¼ä¸ºï¼š$D_{L 1}=\sum_{i=1}^n\left|y_i-f\left(x_i\right)\right|$ã€‚$D_{L 1}$å®é™…ä¸ºå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean absolute errorï¼ŒMAEï¼‰ã€‚ä½¿ç”¨$L1$æŸå¤±å‡½æ•°å³è¦æ±‚ $min D_{L 1}$ï¼›$L2$æŸå¤±å‡½æ•°ï¼Œä¹Ÿç§°ä¸ºæœ€å°åŒ–å¹³æ–¹è¯¯å·®ï¼ˆLeast Square Errorï¼ŒLSEï¼‰ï¼Œä¸ºæœ€å°åŒ–çœŸå®å€¼$y_i$å’Œé¢„æµ‹å€¼$f\left(x_i\right)$ä¹‹é—´å·®å€¼å¹³æ–¹å’Œï¼Œå…¬å¼ä¸ºï¼š$D_{L 2}=\sum_{i=1}^n\left(y_i-f\left(x_i\right)\right)^2$ã€‚ï¼š$D_{L 2}$å®é™…ä¸ºå‡æ–¹è¯¯å·®ï¼ˆmean-square errorï¼ŒMSEï¼‰ã€‚ä½¿ç”¨$L2$æŸå¤±å‡½æ•°å³è¦æ±‚ $min D_{L 2}$ã€‚

åœ¨ cGANs ä¸­ï¼Œä½¿ç”¨$L1$è·ç¦»ï¼Œå…¬å¼è¡¨è¿°ä¸ºï¼š$\mathcal{L}_{L 1}(G)= \mathbb{E}_{x, y, z}\left[\|y-G(x, z)\|_1\right]$ï¼Œå› æ­¤æ›´æ–° cGANs çš„æŸå¤±å‡½æ•°ä¸ºï¼š$G^*=\arg \min _G \max _D \mathcal{L}_{c G A N}(G, D)+\lambda \mathcal{L}_{L 1}(G)$ã€‚ 

å¦‚æœæ²¡æœ‰å™ªå£°å‘é‡$z$ï¼Œç½‘ç»œä»ç„¶å¯ä»¥å­¦ä¹ ä»$x$åˆ°$y$çš„æ˜ å°„ï¼Œä½†ä¼šäº§ç”Ÿç¡®å®šæ€§è¾“å‡ºï¼Œå› æ­¤æ— æ³•åŒ¹é…é™¤ç‹„æ‹‰å…‹å‡½æ•°ï¼ˆdelta functionï¼‰ä»¥å¤–çš„ä»»ä½•åˆ†å¸ƒï¼Œå› æ­¤åœ¨$G$ç½‘ç»œä¸­åº”ç”¨`dropout`çš„å½¢å¼æä¾›å™ªå£°ã€‚

## 3.4.1.2 ç½‘ç»œä½“ç³»ç»“æ„

ä½¿ç”¨è¿ç§»åˆ°`USDA`åº“ä¸­çš„ cGAN é€šç”¨æ¡†æ¶ï¼Œä»¥`pix2pix`ç½‘ç»œä¸ºä¾‹ï¼Œé…ç½®å‚æ•°è°ƒå…¥$G$ç½‘ç»œã€‚è®­ç»ƒæ•°æ®é›†ä½¿ç”¨äº†cGAN é€šç”¨æ¡†æ¶ä½œè€…æä¾›çš„[maps](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)<sup>â‘¢</sup>ï¼Œä¸ºé¥æ„Ÿå½±åƒå’Œåœ°å›¾å¯¹åº”çš„æ•°æ®é›†ï¼Œå¯¹åº”é…ç½®å‚æ•°`p2p.opt.basic.dataroot`ï¼Œé»˜è®¤æ–‡ä»¶å¤¹åä¸º`train`ï¼›ä¸”åŠ è½½äº†é¢„è®­ç»ƒæ¨¡å‹[map2satæˆ–sat2map](http://efrosgans.eecs.berkeley.edu/pix2pix/models-pytorch/)<sup>â‘£</sup>ï¼Œå¯¹åº”å‚æ•°`p2p.opt.basic.checkpoints_dir`ï¼Œé»˜è®¤é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶åä¸º`latest_net_G.pth`ï¼ˆ$G$ï¼‰å’Œ`latest_net_D.pth`($D$)ã€‚æ•°æ®é›†çš„è¡¨ç°å½¢å¼ä½¿ç”¨äº†`aligned`æ–¹å¼ï¼Œå°†é¥æ„Ÿå½±åƒå’Œå¯¹åº”çš„åœ°å›¾å›¾åƒå¹¶åœ¨ä¸€ä¸ªæ–‡ä»¶ï¼Œå³å·¦å³æ‹¼æ¥äº†å¯¹åº”çš„ä¸¤å¹…å›¾åƒï¼Œæœ‰å‚æ•°`p2p.opt.dataset.dataset_mode`é…ç½®ã€‚å¦‚æœç”±é¥æ„Ÿå½±åƒç”Ÿæˆåœ°å›¾ï¼Œåˆ™é…ç½®`p2p.opt.dataset.direction`ä¸º`A2B`ï¼Œåä¹‹ï¼Œç”¨åœ°å›¾ç”Ÿæˆå¯¹åº”çš„é¥æ„Ÿå½±åƒé…ç½®ä¸º`B2A`ã€‚


```python
# IPython extension to reload modules before executing user code.
%load_ext autoreload 
# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.
%autoreload 2 

from usda.migrated_project.pix2pix import train
from usda.migrated_project.pix2pix import test
from usda.migrated_project.stylegan import adjust_dynamic_range

import matplotlib.pyplot as plt
import numpy as np
import torch
from torchshape import tensorshape
from fastai.vision.gan import basic_critic,partial
```

    The autoreload extension is already loaded. To reload it, use:
      %reload_ext autoreload
    

é…ç½®åŸºæœ¬å‚æ•°ï¼Œå¹¶ç”¨`p2p.create_dataset()`åˆ›å»ºæ•°æ®åŠ è½½å™¨ã€‚


```python
p2p=train.Pix2pix_train()
p2p.opt.basic.dataroot=r'I:\data\pix2pix_dataset\maps'
p2p.opt.dataset.dataset_mode='aligned'   
p2p.opt.dataset.direction='BtoA'
p2p.opt.basic.isTrain=True
p2p.opt.model.model='pix2pix'

p2p.create_dataset()
```

    dataset [AlignedDataset] was created
    The number of training images = 1096
    

ç”¨`p2p.create_model()`æ–¹æ³•æ„å»ºæ¨¡å‹ã€‚


```python
p2p.opt.basic.checkpoints_dir=r'I:\model_ckpts\pix2pix_02'
p2p.create_model()
p2p_net=p2p.model
```

    initialize network with normal
    initialize network with normal
    model [Pix2PixModel] was created
    ---------- Networks initialized -------------
    [Network G] Total number of parameters : 54.414 M
    [Network D] Total number of parameters : 2.769 M
    -----------------------------------------------
    

æŸ¥çœ‹$G$ç½‘ç»œã€‚


```python
p2p_net.netG
```




    DataParallel(
      (module): UnetGenerator(
        (model): UnetSkipConnectionBlock(
          (model): Sequential(
            (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
            (1): UnetSkipConnectionBlock(
              (model): Sequential(
                (0): LeakyReLU(negative_slope=0.2, inplace=True)
                (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): UnetSkipConnectionBlock(
                  (model): Sequential(
                    (0): LeakyReLU(negative_slope=0.2, inplace=True)
                    (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (3): UnetSkipConnectionBlock(
                      (model): Sequential(
                        (0): LeakyReLU(negative_slope=0.2, inplace=True)
                        (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        (3): UnetSkipConnectionBlock(
                          (model): Sequential(
                            (0): LeakyReLU(negative_slope=0.2, inplace=True)
                            (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                            (3): UnetSkipConnectionBlock(
                              (model): Sequential(
                                (0): LeakyReLU(negative_slope=0.2, inplace=True)
                                (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                                (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                (3): UnetSkipConnectionBlock(
                                  (model): Sequential(
                                    (0): LeakyReLU(negative_slope=0.2, inplace=True)
                                    (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                                    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                    (3): UnetSkipConnectionBlock(
                                      (model): Sequential(
                                        (0): LeakyReLU(negative_slope=0.2, inplace=True)
                                        (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                                        (2): ReLU(inplace=True)
                                        (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                                        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                      )
                                    )
                                    (4): ReLU(inplace=True)
                                    (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                                    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                  )
                                )
                                (4): ReLU(inplace=True)
                                (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                                (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                              )
                            )
                            (4): ReLU(inplace=True)
                            (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                            (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          )
                        )
                        (4): ReLU(inplace=True)
                        (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (4): ReLU(inplace=True)
                    (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  )
                )
                (4): ReLU(inplace=True)
                (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
                (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): ReLU(inplace=True)
            (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (4): Tanh()
          )
        )
      )
    )



#### 1) å¸¦è·³è·ƒï¼ˆskipsï¼‰çš„$G$ç½‘ç»œ

cGAN ç½‘ç»œçš„è®¾è®¡ä¸ StyleGAN ç½‘ç»œè®¾è®¡å…·æœ‰ç›¸ä¼¼ä¸€è‡´çš„é€»è¾‘å†…æ ¸ï¼ŒStyleGAN é€šè¿‡è¿ç»­ä¸åŒæ·±åº¦åˆ†åˆ«å¯¹æ¥ç”±å™ªå£°å‘é‡ç»è¿‡å¤šå±‚æ„ŸçŸ¥æœºéçº¿æ€§æ˜ å°„ç½‘ç»œå¾—åˆ°çš„æ§åˆ¶å‘é‡ï¼Œè§£ç¼ è¿ç»­æ·±åº¦ç‰¹å¾å½±å“ï¼›è€Œ cGAN å°†ç¼–ç å™¨å’Œè§£ç å™¨å¯¹åº”çš„å·ç§¯å±‚å’Œé€†å·ç§¯å±‚è¿æ¥èµ·æ¥ï¼ŒåŒä¸€â€œæ·±åº¦â€äº’ç›¸å¯¹åº”ã€‚å¦‚å›¾<sup>[]</sup>ï¼š

<img src="./imgs/3_4_c/3_4_c_01.png" height='auto' width=700 title="caDesign"> 

ä¸Šå›¾å·¦ï¼Œä¸€èˆ¬ç¼–ç å™¨å’Œè§£ç å™¨ç½‘ç»œæ˜¯é€šè¿‡ä¸€ç³»åˆ—é€æ­¥ä¸‹é‡‡æ ·çš„å±‚åï¼Œå°†è¯¥è¿‡ç¨‹å¯¹åº”åè½¬ä¸ºä¸Šé‡‡æ ·çš„è¿ç»­è¿‡ç¨‹ï¼Œé‚£ä¹ˆä¸åŒæ·±åº¦ä¹‹é—´çš„åˆ†å¸ƒç‰¹å¾å‘ç”Ÿäº†ä¼ é€’å…±äº«ï¼Œè¿™ç±»ä¼¼äº PGGAN  ç½‘ç»œè¿ç»­ä¸Šé‡‡æ ·åå†ç»§ç»­ä¸‹é‡‡æ ·çš„è¿‡ç¨‹ã€‚ä¸Šå›¾å³ï¼ŒcGAN è·³è·ƒè¿æ¥çš„æ–¹æ³•å°†ç¬¬$i$å±‚å’Œç¬¬$n-i$å±‚å¯¹åº”è¿æ¥èµ·æ¥ï¼Œå…¶ä¸­$n$ä¸ºå±‚çš„æ€»æ•° ï¼Œé‚£ä¹ˆå°±å®ç°äº†å¯¹åº”æ·±åº¦å·ç§¯å±‚ç‰¹å¾çš„å¯¹æ¥ï¼Œé¿å…äº†å…¶å®ƒæ·±åº¦ç‰¹å¾çš„å½±å“ã€‚æ ¸å¿ƒä»£ç å¦‚ä¸‹ï¼š

```python 
class UnetGenerator(nn.Module):
    """Create a Unet-based generator"""

    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):
        """Construct a Unet generator
        Parameters:
            input_nc (int)  -- the number of channels in input images
            output_nc (int) -- the number of channels in output images
            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,
                                image of size 128x128 will become of size 1x1 # at the bottleneck
            ngf (int)       -- the number of filters in the last conv layer
            norm_layer      -- normalization layer

        We construct the U-Net from the innermost layer to the outermost layer.
        It is a recursive process.
        """
        super(UnetGenerator, self).__init__()
        # construct unet structure
        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer
        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters
            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)
        # gradually reduce the number of filters from ngf * 8 to ngf
        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)
        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)
        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)
        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer

    def forward(self, input):
        """Standard forward"""
        return self.model(input)


class UnetSkipConnectionBlock(nn.Module):
    """Defines the Unet submodule with skip connection.
        X -------------------identity----------------------
        |-- downsampling -- |submodule| -- upsampling --|
    """

    def __init__(self, outer_nc, inner_nc, input_nc=None,
                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):
        """Construct a Unet submodule with skip connections.

        Parameters:
            outer_nc (int) -- the number of filters in the outer conv layer
            inner_nc (int) -- the number of filters in the inner conv layer
            input_nc (int) -- the number of channels in input images/features
            submodule (UnetSkipConnectionBlock) -- previously defined submodules
            outermost (bool)    -- if this module is the outermost module
            innermost (bool)    -- if this module is the innermost module
            norm_layer          -- normalization layer
            use_dropout (bool)  -- if use dropout layers.
        """
        super(UnetSkipConnectionBlock, self).__init__()
        self.outermost = outermost
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d
        if input_nc is None:
            input_nc = outer_nc
        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,
                             stride=2, padding=1, bias=use_bias)
        downrelu = nn.LeakyReLU(0.2, True)
        downnorm = norm_layer(inner_nc)
        uprelu = nn.ReLU(True)
        upnorm = norm_layer(outer_nc)

        if outermost:
            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,
                                        kernel_size=4, stride=2,
                                        padding=1)
            down = [downconv]
            up = [uprelu, upconv, nn.Tanh()]
            model = down + [submodule] + up
        elif innermost:
            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,
                                        kernel_size=4, stride=2,
                                        padding=1, bias=use_bias)
            down = [downrelu, downconv]
            up = [uprelu, upconv, upnorm]
            model = down + up
        else:
            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,
                                        kernel_size=4, stride=2,
                                        padding=1, bias=use_bias)
            down = [downrelu, downconv, downnorm]
            up = [uprelu, upconv, upnorm]

            if use_dropout:
                model = down + [submodule] + up + [nn.Dropout(0.5)]
            else:
                model = down + [submodule] + up

        self.model = nn.Sequential(*model)

    def forward(self, x):
        if self.outermost:
            return self.model(x)
        else:   # add skip connections
            return torch.cat([x, self.model(x)], 1)
```

`UnetGenerator`ç±»å®ç°äº†åŸºäº U-Net ç½‘ç»œ$G$çš„æ„å»ºï¼Œæ„å»ºçš„é¡ºåºç”±æœ€å†…å±‚å¼€å§‹ï¼Œç„¶åæˆå¯¹æ„å»ºï¼Œç›´è‡³æœ€å¤–å±‚ã€‚ä½¿ç”¨` torch.cat([x, self.model(x)], 1)`å®ç°å¯¹åº”å±‚çš„è¿æ¥ã€‚

#### 2) é©¬å°”å¯å¤«åˆ¤åˆ«å™¨$D$ï¼ˆPatchGANï¼‰

å¦‚æœç”¨$L1$æŸå¤±ï¼Œç”Ÿæˆçš„å›¾åƒä¼šäº§ç”Ÿæ¨¡ç³Šçš„æ•ˆæœï¼ˆæœªæ•è·åˆ°é«˜é¢‘ä¿¡æ¯ï¼ˆhigh-frequencyï¼‰ï¼‰ï¼Œä½†å›¾åƒçš„æ•´ä½“ç»“æ„æ˜¯è¶‹äºå»åˆçš„ï¼ˆèƒ½å¤Ÿæ•è·åˆ°ä½é¢‘ä¿¡æ¯ï¼ˆlow-frequencyï¼‰ï¼‰ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé™åˆ¶$D$ç½‘ç»œä»…å¯¹é«˜é¢‘ç»“æ„å»ºæ¨¡ï¼Œä¾é $L1$é¡¹å¼ºåˆ¶ä½é¢‘çš„æ­£ç¡®æ€§ï¼Œåªéœ€å°†$D$ä½œç”¨äºåˆ†å‰²çš„$N \times N$å›¾åƒå•å…ƒä¸Šï¼Œç§°ä¹‹ä¸º PatchGANã€‚é€šè¿‡å¯¹ç”Ÿæˆå›¾åƒä¸­æ¯ä¸ªåˆ†å‰²å›¾åƒå•å…ƒåˆ¤æ–­çœŸå‡ï¼Œå¹³å‡æ‰€æœ‰å“åº”ï¼Œä½œä¸º$D$çš„è¾“å‡ºã€‚è¯¥ç§æ–¹æ³•æœ‰æ•ˆçš„å°†å›¾åƒè§†ä¸ºé©¬å°”ç§‘å¤«éšæœºåœºï¼ˆMarkov random ï¬eldï¼‰ï¼Œç”Ÿæˆå›¾åƒ$G(x,z)$çš„åƒç´ ç±»å‹å¯¹åº”ç›®æ ‡å›¾åƒï¼ˆ$y$ï¼‰åƒç´ ç±»å‹åœ¨å›¾åƒåˆ†å‰²å•å…ƒå¤§å°å†…ï¼ˆé‚»åŸŸï¼‰çš„é‚»é‡Œå…³ç³»å¾—ä»¥æ¨æ–­ï¼Œå³åƒç´ åªä¸é‚»åŸŸçš„åƒç´ ç‚¹ä¿¡æ¯æœ‰å…³ï¼Œè€Œå’Œé‚»åŸŸå¤–çš„åƒç´ ç‚¹æ— å…³ã€‚

å®ç° PatchGAN çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯” cGAN å’Œ WGAN $D$çš„è¾“å‡ºå¯ä»¥è§‚å¯Ÿåˆ°å¯¹äºcGANï¼Œè¾“å‡ºçš„æ•°æ®å½¢çŠ¶ä¸º`(1, 1, 30, 30)`ï¼Œè€Œå¯¹äº WGAN è¾“å‡ºå½¢çŠ¶ä¸º`(1, 1, 1, 1) `ï¼Œå› æ­¤å¯ä»¥åˆ¤æ–­ä½œè€…ä»…ä»…é€šè¿‡å·ç§¯çš„æ–¹æ³•å®ç°äº† PatchGAN åˆ†å‰²å›¾åƒå•å…ƒåˆ¤æ–­çœŸå‡çš„ç›®çš„ã€‚åŒæ—¶ï¼Œåœ¨æŸå¤±å‡½æ•°è®¡ç®—ä¸­ï¼Œå¦‚æœé…ç½®`opt.train.gan_mode = 'vanilla'` ï¼Œåˆ™è°ƒç”¨`loss = nn.BCEWithLogitsLoss()`ï¼Œå…¶ä¸­å‚æ•°`reduction='mean'`ï¼›å¦‚æœé…ç½®`opt.train.gan_mode = 'wgangp'`ï¼Œåˆ™é€šè¿‡å¦‚ä¸‹æ–¹æ³•è®¡ç®—æŸå¤±ï¼š

```python
elif self.gan_mode == 'wgangp':
    if target_is_real:
        loss = -prediction.mean()
    else:
        loss = prediction.mean()
```

cGAN $D$ ç½‘ç»œæ‰“å°å¦‚ä¸‹ï¼š


```python
p2p_net.netD
```




    DataParallel(
      (module): NLayerDiscriminator(
        (model): Sequential(
          (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): LeakyReLU(negative_slope=0.2, inplace=True)
          (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): LeakyReLU(negative_slope=0.2, inplace=True)
          (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (7): LeakyReLU(negative_slope=0.2, inplace=True)
          (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)
          (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (10): LeakyReLU(negative_slope=0.2, inplace=True)
          (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
        )
      )
    )



ä½¿ç”¨[torchshape](https://github.com/yuezuegu/torchshape)<sup>â‘¤</sup>åº“æä¾›çš„æ–¹æ³•è®¡ç®—è¾“å‡ºå½¢çŠ¶ï¼Œç›®å‰æ”¯æŒçš„æ“ä½œæœ‰ï¼š

* nn.Conv1d
* nn.Conv2d
* nn.Linear
* nn.MaxPool1d
* nn.MaxPool2d
* nn.AvgPool1d
* nn.AvgPool2d
* nn.Flatten
* nn.BatchNorm1d
* nn.BatchNorm2d

è®¡ç®— cGAN çš„$D$ç½‘ç»œå„å·ç§¯å±‚è¾“å‡ºå½¢çŠ¶ã€‚


```python
shape=(1,6,256,256)
for op in p2p_net.netD.module.model:
    if isinstance(op, torch.nn.modules.conv.Conv2d):
        shape=tensorshape(op,shape)
        print(f'{shape}   \t{op}')
```

    (1, 64, 128, 128)   	Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1, 128, 64, 64)   	Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1, 256, 32, 32)   	Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1, 512, 31, 31)   	Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)
    (1, 1, 30, 30)   	Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
    

è®¡ç®— WGAN çš„ $ğ·$ ç½‘ç»œå„å·ç§¯å±‚è¾“å‡ºå½¢çŠ¶ã€‚


```python
critic=basic_critic(in_size=256, n_channels=3,n_features=64,act_cls=partial(torch.nn.LeakyReLU, negative_slope=0.2))
flattened_critic=[module for module in critic.modules() if not isinstance(module, torch.nn.Sequential)]
shape=(1,3,256,256)
for op in flattened_critic:
    if isinstance(op, torch.nn.modules.conv.Conv2d):
        shape=tensorshape(op,shape)
        print(f'{shape}   \t{op}')
```

    (1, 64, 128, 128)   	Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1, 128, 64, 64)   	Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1, 256, 32, 32)   	Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1, 512, 16, 16)   	Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1, 1024, 8, 8)   	Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1, 2048, 4, 4)   	Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1, 1, 1, 1)   	Conv2d(2048, 1, kernel_size=(4, 4), stride=(1, 1))
    

* å…¶å®ƒè®­ç»ƒæŠ€å·§

1. ä¼˜åŒ–$D$çš„åŒæ—¶ï¼Œå°†ç›®æ ‡é™¤ä»¥2ï¼Œé™ä½$D$ç›¸å¯¹äº$G$çš„å­¦ä¹ é€Ÿåº¦ï¼Œå¯¹åº”ä»£ç ä¸º`self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5`ï¼›
2. ä½¿ç”¨ minibatch SGDï¼Œå¹¶åº”ç”¨ Adam è§£é‡Šå™¨ï¼Œé…ç½®å­¦ä¹ ç‡ä¸º2e-4ï¼ŒåŠ¨é‡å‚æ•°ï¼ˆmomentum parametersï¼‰ä¸ºï¼Œ$ \beta _{1} =0.5ï¼› \beta _{2} =0.999$ï¼›
3. é…ç½®æ‰¹é‡å¤§å°åœ¨1åˆ°10ä¹‹é—´ï¼Œå¦‚æœé…ç½®ä¸º1ï¼Œç§°ä¸ºâ€œå®ä¾‹å½’ä¸€åŒ–ï¼ˆinstance normal-izationï¼‰â€ã€‚

#### 3) æŸ¥çœ‹æ•°æ®ï¼Œè®­ç»ƒå’Œæµ‹è¯•ç”Ÿæˆ

cGAN é€šç”¨æ¡†æ¶å«æœ‰å¯¹æ‹¼æ¥å›¾åƒçš„æ‹†åˆ†å¤„ç†ï¼Œæ„å»ºå®Œæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨åï¼Œå›¾åƒå®ç°æ‹†åˆ†ï¼Œä»¥`A`å’Œ`B`ä¸ºé”®ï¼Œæ ‡è¯†Aå’ŒBæ•°æ®ã€‚æ ¹æ®è½¬åŒ–çš„æ–¹å‘ï¼Œä¸€ä¸ªç”¨äº$x$ï¼Œä¸€ä¸ªç”¨äº$y$ã€‚


```python
images=next(iter(p2p.dataset))
fig, axs = plt.subplots(1,2,figsize=(10, 10))
axs[0].imshow(adjust_dynamic_range(images['A'].permute(0,2,3,1)[0]))
axs[1].imshow(adjust_dynamic_range(images['B'].permute(0,2,3,1)[0]))
plt.show()
```


<img src="./imgs/3_4_c/output_18_0.png" height='auto' width='auto' title="caDesign">   



`visualizer`æ–¹æ³•ç”¨äºå®ç°è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¡ç®—é˜¶æ®µæ€§è®­ç»ƒæ¨¡å‹é¢„æµ‹ï¼ˆç¿»è¯‘ï¼‰çš„ç»“æœå›¾åƒï¼Œå¹¶å­˜å‚¨è‡³`p2p.opt.basic.checkpoints_dir`é…ç½®å­˜å‚¨è·¯å¾„ä¸‹ã€‚


```python
p2p.opt.train.visual.display_id=-1
p2p.visualizer()
```

    create web directory I:\model_ckpts\pix2pix_02\web...
    

* è®­ç»ƒæ¨¡å‹


```python
p2p.train()
```

```
learning rate 0.0002000 -> 0.0002000
(epoch: 1, iters: 100, time: 0.067, data: 7.584) G_GAN: 1.612 G_L1: 20.842 D_real: 0.013 D_fake: 0.471 
(epoch: 1, iters: 200, time: 0.068, data: 0.001) G_GAN: 1.667 G_L1: 27.958 D_real: 0.003 D_fake: 0.424 
(epoch: 1, iters: 300, time: 0.066, data: 0.001) G_GAN: 1.009 G_L1: 8.658 D_real: 1.523 D_fake: 0.310 
(epoch: 1, iters: 400, time: 0.173, data: 0.001) G_GAN: 1.162 G_L1: 26.299 D_real: 0.463 D_fake: 0.128 
(epoch: 1, iters: 500, time: 0.070, data: 0.001) G_GAN: 2.325 G_L1: 19.845 D_real: 0.102 D_fake: 0.118 
(epoch: 1, iters: 600, time: 0.072, data: 0.001) G_GAN: 2.664 G_L1: 14.551 D_real: 0.007 D_fake: 2.713 
(epoch: 1, iters: 700, time: 0.067, data: 0.001) G_GAN: 1.895 G_L1: 18.732 D_real: 0.244 D_fake: 0.405 
(epoch: 1, iters: 800, time: 0.068, data: 0.001) G_GAN: 3.193 G_L1: 22.693 D_real: 0.039 D_fake: 0.132 
(epoch: 1, iters: 900, time: 0.066, data: 0.001) G_GAN: 2.161 G_L1: 16.417 D_real: 0.475 D_fake: 0.319 
(epoch: 1, iters: 1000, time: 0.068, data: 0.001) G_GAN: 1.669 G_L1: 20.868 D_real: 0.142 D_fake: 0.216 
End of epoch 1 / 200 	 Time Taken: 73 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 2, iters: 4, time: 0.059, data: 0.000) G_GAN: 2.092 G_L1: 20.703 D_real: 0.050 D_fake: 0.532 
(epoch: 2, iters: 104, time: 0.177, data: 0.000) G_GAN: 2.218 G_L1: 27.503 D_real: 0.001 D_fake: 0.537 
(epoch: 2, iters: 204, time: 0.070, data: 0.001) G_GAN: 0.933 G_L1: 28.514 D_real: 1.289 D_fake: 0.113 
(epoch: 2, iters: 304, time: 0.068, data: 0.001) G_GAN: 1.923 G_L1: 21.235 D_real: 0.117 D_fake: 0.171 
(epoch: 2, iters: 404, time: 0.066, data: 0.001) G_GAN: 2.134 G_L1: 21.554 D_real: 0.600 D_fake: 0.058 
(epoch: 2, iters: 504, time: 0.075, data: 0.001) G_GAN: 1.511 G_L1: 14.303 D_real: 0.232 D_fake: 0.148 
(epoch: 2, iters: 604, time: 0.076, data: 0.001) G_GAN: 2.297 G_L1: 23.186 D_real: 0.609 D_fake: 0.049 
(epoch: 2, iters: 704, time: 0.069, data: 0.001) G_GAN: 1.207 G_L1: 13.543 D_real: 0.620 D_fake: 0.989 
(epoch: 2, iters: 804, time: 0.069, data: 0.001) G_GAN: 2.229 G_L1: 23.092 D_real: 0.144 D_fake: 0.162 
(epoch: 2, iters: 904, time: 0.194, data: 0.000) G_GAN: 1.966 G_L1: 22.935 D_real: 0.467 D_fake: 0.087 
(epoch: 2, iters: 1004, time: 0.066, data: 0.001) G_GAN: 2.268 G_L1: 24.545 D_real: 0.003 D_fake: 0.245 
End of epoch 2 / 200 	 Time Taken: 66 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 3, iters: 8, time: 0.084, data: 0.001) G_GAN: 1.096 G_L1: 15.798 D_real: 0.325 D_fake: 0.656 
```

* æµ‹è¯•å’Œç”Ÿæˆæ•°æ®

ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆå›¾åƒï¼ŒæŸ¥çœ‹é¢„æµ‹ç»“æœã€‚å›¾åƒå­˜å‚¨äº`p2p.opt.test.results_dir`é…ç½®å­˜å‚¨è·¯å¾„ä¸‹ï¼Œå¦‚ä¸‹å¯¹åº”Aå’ŒBçš„éƒ¨åˆ†ç”Ÿæˆå›¾åƒã€‚


```python
p2p=test.Pix2pix_test()

p2p.opt.basic.dataroot=r'I:\data\pix2pix_dataset\maps'
p2p.opt.test.results_dir=r'I:\model_ckpts\pix2pix_02'
p2p.opt.dataset.dataset_mode='aligned'   
p2p.opt.dataset.direction='BtoA'
p2p.opt.train.saveload.phase='val'
p2p.opt.basic.isTrain=False
p2p.opt.model.model='pix2pix'

p2p.create_dataset()

p2p.opt.basic.checkpoints_dir=r'I:\model_ckpts\pix2pix_02'
p2p.create_model()
m=p2p.model    

p2p.test()
```

    dataset [AlignedDataset] was created
    initialize network with normal
    model [Pix2PixModel] was created
    loading the model from I:\model_ckpts\pix2pix_02\latest_net_G.pth
    ---------- Networks initialized -------------
    [Network G] Total number of parameters : 54.414 M
    -----------------------------------------------
    creating web directory I:\model_ckpts\pix2pix_02\val_latest
    processing (0000)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1.jpg']
    processing (0005)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1002.jpg']
    processing (0010)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1007.jpg']
    processing (0015)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1011.jpg']
    processing (0020)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1016.jpg']
    processing (0025)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1020.jpg']
    processing (0030)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1025.jpg']
    processing (0035)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\103.jpg']
    processing (0040)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1034.jpg']
    processing (0045)-th image... ['I:\\data\\pix2pix_dataset\\maps\\val\\1039.jpg']
    

| read-A  | real-B  | fake-b  |
|---|---|---|
| <img src="./imgs/3_4_c/1018_real_A.png" height='auto' width=700 title="caDesign">   | <img src="./imgs/3_4_c/1018_real_B.png" height='auto' width=700 title="caDesign">   |  <img src="./imgs/3_4_c/1018_fake_B.png" height='auto' width=700 title="caDesign">  |
|  <img src="./imgs/3_4_c/1033_real_A.png" height='auto' width=700 title="caDesign">  | <img src="./imgs/3_4_c/1033_real_B.png" height='auto' width=700 title="caDesign">   | <img src="./imgs/3_4_c/1033_fake_B.png" height='auto' width=700 title="caDesign">   |
| <img src="./imgs/3_4_c/1036_real_A.png" height='auto' width=700 title="caDesign">   | <img src="./imgs/3_4_c/1036_real_B.png" height='auto' width=700 title="caDesign">   | <img src="./imgs/3_4_c/1036_fake_B.png" height='auto' width=700 title="caDesign">   |

## 3.4.2 NAIP é¥æ„Ÿå½±åƒå’ŒåœŸåœ°è¦†ç›–ç±»å‹ï¼ˆland coverï¼ŒLCï¼‰ä¹‹é—´çš„ç¿»è¯‘è½¬åŒ–

### 3.4.2.1 æ„å»ºè®­ç»ƒæ ·æœ¬æ•°æ®

ä¸»è¦ä½¿ç”¨[TorchGeo](https://github.com/microsoft/torchgeo)<sup>â‘¥</sup>å’Œ[PILï¼ˆPillowï¼‰](https://pillow.readthedocs.io/en/stable/)<sup>â‘¦</sup>åº“æ„å»ºè®­ç»ƒæ•°æ®é›†ã€‚å…³äº`TorchGeo`æ ·æœ¬æå–å¯ä»¥å‚è€ƒ*NAIPèˆªæ‹å½±åƒä¸åˆ†å‰²æ¨¡å‹åº“åŠColaboratoryå’ŒPlanetary Computer Hub*ä¸€ç« çš„é˜é‡Šã€‚


```python
# IPython extension to reload modules before executing user code.
%load_ext autoreload 
# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.
%autoreload 2 
import usda.geodata_process as usda_geoprocess
import usda.imgs_process as usda_imgs

from yacs.config import CfgNode as CN
import os
from torchgeo.datasets import NAIP,ChesapeakeDE,stack_samples 
from torchgeo.samplers import RandomGeoSampler
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torchvision.transforms as T
from torchgeo.datasets import unbind_samples
import numpy as np
from PIL import Image
from tqdm import tqdm
```

é…ç½®åŸå§‹é¥æ„Ÿå½±åƒå’Œ LC æ•°æ®æ–‡ä»¶æ‰€åœ¨å­˜å‚¨è·¯å¾„ï¼ŒåŠæå–åæ ·æœ¬æ•°æ®å­˜å‚¨ä½ç½®ï¼ŒåŒ…æ‹¬å•ç‹¬çš„å½±åƒæ–‡ä»¶å¤¹`cfg.sample_imgs_dir`ï¼ŒLC æ–‡ä»¶å¤¹`cfg.sample_lc_dir`åŠå½±åƒå’ŒLCæ‹¼æ¥å›¾åƒçš„å­˜å‚¨ä½ç½®`cfg.sample_img_lc_dir`ã€‚


```python
cfg=CN()
cfg.Chesapeake_root=r'E:\data\Delaware'
cfg.Chesapeake_LC=os.path.join(cfg.Chesapeake_root,'LC')
cfg.Chesapeake_imagery=os.path.join(cfg.Chesapeake_root,'imagery')
cfg.sample_imgs_dir=r'I:\data\naip_lc4pix2pix\imgs'
cfg.sample_lc_dir=r'I:\data\naip_lc4pix2pix\lc'
cfg.sample_img_lc_dir=r'I:\data\naip_lc4pix2pix\img_lc'
```

æ„å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ã€‚


```python
naip=NAIP(cfg.Chesapeake_imagery)
chesapeake=ChesapeakeDE(cfg.Chesapeake_LC, crs=naip.crs, res=naip.res, download=False)
dataset=naip & chesapeake
```

é‡‡æ ·çš„å›¾åƒå¤§å°é…ç½®ä¸º`size=512`ï¼Œæ€»å…±äº§ç”Ÿå½±åƒå’ŒLCå¯¹åº”çš„`length=10000)`ä¸ªéšæœºæ ·æœ¬ã€‚


```python
sampler=RandomGeoSampler(dataset, size=512, length=10000)
dataloader=DataLoader(dataset, sampler=sampler, collate_fn=stack_samples)
for batch in dataloader:
    image=batch["image"]
    target=batch["mask"]
    break
    
print(f'sample length={len(sampler)};\nimage shape:{image.shape};\ntarget shape:{target.shape}')
```

    sample length=10000;
    image shape:torch.Size([1, 4, 512, 512]);
    target shape:torch.Size([1, 1, 512, 512])
    

é…ç½® LC çš„é¢œè‰²ï¼Œå°† LC åˆ†ç±»æ•°å€¼è½¬åŒ–ä¸ºé¢œè‰²å€¼ï¼ˆ3ä¸ªé€šé“ï¼‰ã€‚


```python
LC_color_dict={
    0: (0, 0, 0),
    1: (0, 197, 255),
    2: (0, 168, 132),
    3: (38, 115, 0),
    4: (76, 230, 0),
    5: (163, 255, 115),
    6: (255, 170, 0),
    7: (255, 0, 0,),
    8: (156, 156, 156),
    9: (0, 0, 0),
    10: (115, 115, 0),
    11: (230, 230, 0),
    12: (255, 255, 115),
    13: (197, 0, 255),
    }

target_T=target[0].permute(1,2,0)
target_img=torch.tensor([LC_color_dict[i.item()] for i in torch.flatten(target_T)]).reshape(target_T.shape[0],target_T.shape[1],3)
img=np.transpose(image[0][:3],(1,2,0))

fig, axs = plt.subplots(1,2,figsize=(15, 15))
axs[0].imshow(img/255)
axs[1].imshow(target_img)
plt.show()
```

<img src="./imgs/3_4_c/output_36_0.png" height='auto' width='auto' title="caDesign">
    


ç¤ºä¾‹å½±åƒå’ŒLCæ‹¼æ¥åçš„å›¾åƒã€‚


```python
img_pil=Image.fromarray(img.numpy().astype('uint8'),'RGB')
target_pil=Image.fromarray(target_img.numpy().astype('uint8'),'RGB')
img_target=usda_imgs.imgs_concat_h(img_pil,target_pil)
img_target
```



<img src="./imgs/3_4_c/output_38_0.png" height='auto' width='auto' title="caDesign">
    



æ‰¹é‡ç”Ÿæˆæ ·æœ¬æ•°æ®ã€‚

> å¦‚æœè¦ç”Ÿæˆå¤§æ•°é‡çš„æ ·æœ¬ï¼Œå¯ä½¿ç”¨å¤šçº¿ç¨‹ã€‚ä¸‹è¿°ä»£ç æœªä½¿ç”¨ã€‚


```python
suffix='.jpg'

for i,batch in enumerate(dataloader):
    image=batch["image"]
    target=batch["mask"]
    
    target_T=target[0].permute(1,2,0)
    target_img=torch.tensor([LC_color_dict[i.item()] for i in torch.flatten(target_T)]).reshape(target_T.shape[0],target_T.shape[1],3)
    img=np.transpose(image[0][:3],(1,2,0))    

    img_pil=Image.fromarray(img.numpy().astype('uint8'),'RGB')
    target_pil=Image.fromarray(target_img.numpy().astype('uint8'),'RGB')
    img_target=usda_imgs.imgs_concat_h(img_pil,target_pil)

    img_pil.save(os.path.join(cfg.sample_imgs_dir,f'{i}{suffix}'))
    target_pil.save(os.path.join(cfg.sample_lc_dir,f'{i}{suffix}'))
    img_target.save(os.path.join(cfg.sample_img_lc_dir,f'{i}{suffix}'))
    
    print(f'---{i}',end='\r')
```

    ---9999

### 3.4.2.2 è®­ç»ƒå½±åƒåˆ°LCå’ŒLCåˆ°å½±åƒçš„ cGAN ç½‘ç»œæ¨¡å‹

ä½¿ç”¨ cGAN çš„ pix2pix æ–¹æ³•æ„å»º NAIP é«˜åˆ†è¾¨ç‡ï¼ˆ1mï¼‰èˆªæ‹å½±åƒå’Œ LCï¼ˆ13ç±»ï¼‰ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œæ¨¡å‹è®­ç»ƒçš„æ–¹æ³•åŒä¸Šã€‚è°ƒç”¨è®­ç»ƒåçš„æ¨¡å‹ç”¨äºå½±åƒå’ŒLCä¹‹é—´çš„äº’ç›¸è½¬åŒ–ï¼Œä¸‹è¿°ä½¿ç”¨äº†èŠåŠ å“¥åŒºåŸŸçš„ NAIP èˆªæ‹å½±åƒï¼Œåº”ç”¨è®­ç»ƒçš„$G$æ¨¡å‹å®éªŒã€‚


```python
# IPython extension to reload modules before executing user code.
%load_ext autoreload 
# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.
%autoreload 2 

from usda.migrated_project.pix2pix import A2B
from usda.migrated_project.pix2pix import sketch_A2B
import usda.imgs_process as usda_imgs
from yacs.config import CfgNode as CN
```

è½¬æ¢èŠåŠ å“¥åŒºåŸŸ NAIP çœŸå®å½±åƒåˆ° LC åˆ†ç±»ï¼Œèƒ½å¤Ÿå¤§æ¦‚åˆ†ç±»ä¸»è¦çš„åœ°ç‰©å…³ç³»ï¼Œä½†æ˜¯ç»†èŠ‚ï¼Œä¾‹å¦‚å»ºç­‘ã€ä¸é€æ°´åœ°é¢ç­‰è¾¹ç¼˜æ¯”è¾ƒæ¨¡ç³Šã€‚ä¸€æ–¹é¢ç”±äºè®­ç»ƒçš„ LC æ•°æ®é›†æœ¬èº«è¾¹ç¼˜å¹¶ä¸æ¸…æ™°ï¼›å¦ä¸€æ–¹é¢ç”±äº pix2pix æœ¬èº«ç½‘ç»œç»“æ„çš„å½±å“ã€‚åœ¨è¿›ä¸€æ­¥çš„æ¢ç´¢ä¸­ï¼Œé™¤äº†ä½¿ç”¨ç²¾åº¦é«˜çš„ LC æ•°æ®é›†å¤–ï¼Œå¯ä»¥å€Ÿé‰´ StyleGAN çš„$G$ç½‘ç»œã€‚


```python
pretrained_model_Img2LC_fn=r'I:\model_ckpts\pix2pix\pix2pix4Img2LC\latest_net_G.pth'
img_fn=r'I:\data\NAIP4StyleGAN\naip_512\0_167.jpg'

AorB,BorA=A2B.A2B_generator(pretrained_model_Img2LC_fn,img_fn)
AnB=usda_imgs.imgs_concat_h(AorB,BorA)
AnB
```

    initialize network with normal
    




<img src="./imgs/3_4_c/output_44_1.png" height='auto' width='auto' title="caDesign">    



LC åˆ†ç±»åˆ°å½±åƒçš„è½¬åŒ–ï¼Œäººå·¥å»ºç­‘éƒ¨åˆ†å¯ä»¥å¤§æ¦‚â€œç¿»è¯‘â€ï¼Œé€šå¸¸æ¨¡ç³Šä¸”è¾¹ç•Œä¸æ¸…æ™°ã€‚å› ä¸ºå¹¶ä¸å…³æ³¨æ¤è¢«éƒ¨åˆ†çš„ç»†èŠ‚ï¼Œå› æ­¤æ¤è¢«çš„â€œç¿»è¯‘â€ç›¸å¯¹æ›´ç†æƒ³ã€‚


```python
pretrained_mode_LC2Img_fn=r'I:\model_ckpts\pix2pix\pix2pix4LC2IMG\latest_net_G.pth'
img_fn=r'I:\model_ckpts\pix2pix\pix2pix4LC2IMG\web\images\epoch001_real_A.png'

AorB,BorA=A2B.A2B_generator(pretrained_mode_LC2Img_fn,img_fn,cfg)
AnB=usda_imgs.imgs_concat_h(AorB,BorA)
AnB
```

    initialize network with normal
    




<img src="./imgs/3_4_c/output_46_1.png" height='auto' width='auto' title="caDesign">    



ä¸‹é¢è¿›è¡Œçš„å®éªŒæ˜¯å®Œå…¨äººä¸ºæ¶‚é¸¦ä¸€ä¸ª LC åˆ†ç±»ï¼ˆåˆ†ç±»é¢œè‰²åŒè®­ç»ƒæ‰€ç”¨ LCï¼‰ï¼Œå°†å…¶è½¬æ¢ä¸ºå½±åƒï¼Œå¹¶ç”¨ç”Ÿæˆçš„å½±åƒå†è½¬åŒ–å› LC åˆ†ç±»ã€‚ä»ç»“æœå¯ä»¥åˆ¤æ–­ï¼Œå›¾åƒç¿»è¯‘çš„æ–¹å¼å¯ä»¥è¾…åŠ©è§„åˆ’è®¾è®¡ï¼Œé€šè¿‡è‰å›¾åˆ°â€œçœŸå®â€å½±åƒçš„è½¬åŒ–ï¼Œç†è§£åˆ†ç±»å¸ƒå±€æ½œåœ¨çœŸå®çš„å½¢æ€ï¼Œä»è€Œè¿›ä¸€æ­¥è°ƒæ•´åˆ†ç±»å¸ƒå±€ã€‚


```python
img_fn=r'C:\Users\richi\omen_richiebao\omen_github\USDA_special_study\imgs\3_4_c\3_4_c_02.png'

AorB,BorA=A2B.A2B_generator(pretrained_mode_LC2Img_fn,img_fn)
AnB=usda_imgs.imgs_concat_h(AorB,BorA)
AnB
```

    initialize network with normal
    




<img src="./imgs/3_4_c/output_48_1.png" height='auto' width='auto' title="caDesign">   



```python
BorA_save_fn=r'C:\Users\richi\omen_richiebao\omen_github\USDA_special_study\imgs\3_4_c\3_4_c_02.jpg'
BorA.save(BorA_save_fn)
```

å°†æ¶‚é¸¦åˆ†ç±»ç”Ÿæˆçš„å½±åƒå†è½¬åŒ–å› LC åˆ†ç±»ã€‚è™½ç„¶ç›®å‰è®­ç»ƒçš„æ¨¡å‹è¿˜æœªè¾¾åˆ°ç†æƒ³çš„â€œç¿»è¯‘â€è¦æ±‚ï¼Œä½†æ˜¯è½¬åŒ–åçš„ LC è¡¨è¾¾æ˜¾ç„¶è¦æ¯”äººä¸ºæ¶‚é¸¦æ›´ä¸ºè‡ªç„¶ï¼Œé‚£ä¹ˆä»LCæ¶‚é¸¦åˆ°å½±åƒï¼Œå†è½¬åŒ–å›LCçš„æ–¹å¼ï¼Œå¯ä»¥è¾…åŠ©è§„åˆ’è®¾è®¡ï¼Œæ¨æ–­å¸ƒå±€çš„åˆç†æ€§ï¼Œå¹¶ä¼˜åŒ–è¡¨è¾¾æå‡å·¥ä½œçš„æ•ˆç‡ã€‚


```python
img_fn=r'I:\data\NAIP4StyleGAN\naip_512\0_167.jpg'

AorB,BorA=A2B.A2B_generator(pretrained_model_Img2LC_fn,BorA_save_fn)
AnB=usda_imgs.imgs_concat_h(AorB,BorA)
AnB
```

    initialize network with normal
    




<img src="./imgs/3_4_c/output_51_1.png" height='auto' width='auto' title="caDesign">    



## 3.4.3 è¾…åŠ©è§„åˆ’è®¾è®¡å·¥å…·çš„æ„å»º

ä¸Šè¿°â€œç¿»è¯‘â€å®éªŒéœ€è¦åœ¨ç»˜å›¾è½¯ä»¶ä¸­ç»˜åˆ¶ LCåï¼Œå†è°ƒå…¥åˆ°ç¨‹åºæ‰§è¡Œè½¬åŒ–ï¼Œæ“ä½œæ¨¡å¼è„±èŠ‚ï¼›å¹¶ä¸”éœ€è¦å¸å–åˆ†ç±»é¢œè‰²åç»˜å›¾ï¼Œä»¤å·¥ä½œç¹çã€‚ä½¿ç”¨[Tkinter](https://www.pythonguis.com/tkinter/)<sup>â‘§</sup>åº“æ„å»ºè¾…åŠ©è§„åˆ’è®¾è®¡å·¥å…·çš„é›å½¢ï¼Œå®ç° LC æ¶‚é¸¦å’Œç¿»è¯‘ä¸ºå½±åƒã€‚è¯¥éƒ¨åˆ†å®ç°ä½äº`usda.migrated_project.pix2pix`ä¸‹çš„`sketch_A2B`æ¨¡å—ä¸­ï¼Œç•Œé¢å¦‚ä¸‹å›¾ï¼š

<img src="./imgs/3_4_c/3_4_c_03.png" height='auto' width='auto' title="caDesign"> 

åœ¨`Tkinter`ä¸­ä½¿ç”¨äº†`canvas`çš„`postscript`æ–¹æ³•ä¿å­˜ä¸ºEPSæ ¼å¼å›¾åƒï¼Œå¹¶é…åˆ`PIL`åº“å°†EPSå›¾åƒè½¬åŒ–ä¸ºå¸¸è§„çš„PNGæˆ–è€…JPGå›¾åƒï¼Œå› æ­¤ä½¿ç”¨åˆ°[Ghostscript](https://ghostscript.com/releases/gsdnld.html)<sup>â‘¨</sup>ï¼Œéœ€è¦ä¸‹è½½å®‰è£…ï¼ŒåŒæ—¶ï¼Œé€šè¿‡ä¸‹è¿°è¯­å¥è°ƒç”¨ï¼Œ

```python
from PIL import EpsImagePlugin
EpsImagePlugin.gs_windows_binary =r'C:\Program Files\gs\gs10.01.1\bin\gswin64c'
```

ä¸Šè¿°`Ghostscript`è·¯å¾„é…ç½®é»˜è®¤å†™åœ¨äº†`Sketch_A2B`æ¨¡å—ä¸­ï¼Œåœ¨å®éªŒ`Sketch_A2B`å·¥å…·æ—¶ï¼Œå¯ä»¥å°†`Ghostscript`é»˜è®¤å®‰è£…åˆ°ä¸Šè¿°è·¯å¾„ï¼Œæˆ–è€…ä¿®æ”¹`USDA`åº“`Sketch_A2B`æ¨¡å—ä¸­å¯¹åº”çš„è°ƒå…¥è·¯å¾„ä»£ç ã€‚

è°ƒç”¨`Sketch_A2B`å·¥å…·ï¼Œæ¶‚é¸¦ LCå¹¶ç¿»è¯‘ã€‚å…¶ä¸­ï¼Œ`load model`æŒ‰é’®ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„$G$ç½‘ç»œæ¨¡å‹ï¼›`update G_img`æŒ‰é’®ç”¨äºå½±åƒè½¬åŒ–ï¼Œå®éªŒç»“æœå¦‚ä¸‹ã€‚


```python
app=sketch_A2B.Sketch_A2B()
app.mainloop()
```

    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    initialize network with normal
    

<img src="./imgs/3_4_c/3_4_c_04.png" height='auto' width='auto' title="caDesign"> 

## 3.4.4 ç¼ºå¤±åŒºåŸŸçš„ä¿®å¤å®éªŒ

### 3.4.4.1 å»ºç«‹éšæœºå½¢çŠ¶é®ç½©çš„å½±åƒæ•°æ®é›†

ä½¿ç”¨[cv2ï¼ˆOpenCV-Pythonï¼‰](https://opencv24-python-tutorials.readthedocs.io/en/latest/index.html)<sup>â‘©</sup>åº“ååŠ©å¤„ç†å…·æœ‰éšæœºå½¢çŠ¶è¦†ç›–ï¼ˆé®ç½©ï¼‰çš„å½±åƒï¼Œè¯¥æ–¹æ³•å·²å†™å…¥`USDA`åº“ï¼Œäº`usda_imgs`æ¨¡å—ä¸­ã€‚åŸå§‹å½±åƒä½¿ç”¨å‰æ–‡å·²ç»å¤„ç†çš„å¤§å°ä¸º 512 çš„å½±åƒæ•°æ®é›†ã€‚


```python
# IPython extension to reload modules before executing user code.
%load_ext autoreload 
# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.
%autoreload 2 
import usda.imgs_process as usda_imgs
import glob, os
import cv2
from PIL import Image
from tqdm import tqdm
```


```python
naip_512_path=r'I:\data\naip_lc4pix2pix\imgs'
img_fns=glob.glob(naip_512_path+"/*.jpg")
img=cv2.imread(img_fns[0])

_,masked_img=usda_imgs.random_shape_onImage(img,thresh1=130) # thresh2=255ï¼ˆé»˜è®¤å€¼ï¼‰

img_pil=Image.fromarray(img.astype('uint8'),'RGB')
img_maskedImg=usda_imgs.imgs_concat_h(img_pil,masked_img)
img_maskedImg
```




<img src="./imgs/3_4_c/output_58_0.png" height='auto' width='auto' title="caDesign">    




```python
img_maskedImg_path=r'I:\data\naip_lc4pix2pix\img_maskedImg'
suffix='.jpg'
i=0
for fn in tqdm(img_fns):
    img=cv2.imread(fn)
    _,masked_img=usda_imgs.random_shape_onImage(img,thresh1=130)
    img_pil=Image.fromarray(img.astype('uint8'),'RGB')
    img_maskedImg=usda_imgs.imgs_concat_h(img_pil,masked_img)    
    img_maskedImg.save(os.path.join(img_maskedImg_path,f'{i}{suffix}'))
    i+=1
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [12:08<00:00, 13.72it/s]
    

### 3.4.4.2 è®­ç»ƒä¸ç»“æœ

è®­ç»ƒæ–¹æ³•åŒä¸Šï¼Œç»“æœå¦‚ä¸‹ï¼š

| read-A  | real-B  | fake-b  |
|---|---|---|
| <img src="./imgs/3_4_c/epoch012_real_A.png" height='auto' width=700 title="caDesign">   | <img src="./imgs/3_4_c/epoch012_real_B.png" height='auto' width=700 title="caDesign">   |  <img src="./imgs/3_4_c/epoch012_fake_B.png" height='auto' width=700 title="caDesign">  |
|  <img src="./imgs/3_4_c/epoch022_real_A.png" height='auto' width=700 title="caDesign">  | <img src="./imgs/3_4_c/epoch022_real_B.png" height='auto' width=700 title="caDesign">   | <img src="./imgs/3_4_c/epoch022_fake_B.png" height='auto' width=700 title="caDesign">   |
| <img src="./imgs/3_4_c/epoch033_real_A.png" height='auto' width=700 title="caDesign">   | <img src="./imgs/3_4_c/epoch033_real_B.png" height='auto' width=700 title="caDesign">   | <img src="./imgs/3_4_c/epoch033_fake_B.png" height='auto' width=700 title="caDesign">   |

è®­ç»ƒç»“æœè´¨é‡ä¸éšæœºå½¢çŠ¶é®ç½©å¤§å°ï¼Œå’Œè®­ç»ƒçš„æ•°æ®é›†å½±åƒå¤§å°å’Œå†…å®¹ç­‰æœ‰å…³ã€‚é®ç½©å¤§å°æ™®éé«˜äºå»ºç­‘å°ºåº¦ï¼Œå½±åƒå¤šä»¥è‡ªç„¶æ™¯è§‚ä¸ºä¸»ï¼Œä»ç»“æœèƒ½å¤Ÿå‘ç°ï¼Œè‡ªç„¶æ™¯è§‚éƒ¨åˆ†çš„ä¿®å¤è¾ƒå¥½ï¼Œä½†æ˜¯äººå·¥å»ºç­‘éƒ¨åˆ†å¹¶ä¸ç†æƒ³ã€‚

---

æ³¨é‡Šï¼ˆNotesï¼‰ï¼š

â‘  eCognitionï¼Œï¼ˆ<https://geospatial.trimble.com/what-is-ecognition>ï¼‰ã€‚

â‘¡ CycleGAN and pix2pixï¼Œï¼ˆ<https://phillipi.github.io/pix2pix/>ï¼‰ã€‚

â‘¢ cGAN é€šç”¨æ¡†æ¶mapsæ•°æ®é›†ï¼Œï¼ˆ<http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/>ï¼‰ã€‚

â‘£ cGAN é€šç”¨æ¡†æ¶é¢„è®­ç»ƒæ¨¡å‹map2satæˆ–sat2mapï¼Œï¼ˆ<http://efrosgans.eecs.berkeley.edu/pix2pix/models-pytorch/>ï¼‰ã€‚

â‘¤ torchshapeï¼Œï¼ˆ<https://github.com/yuezuegu/torchshape>ï¼‰ã€‚

â‘¥ TorchGeoï¼Œï¼ˆ<https://github.com/microsoft/torchgeo>ï¼‰ã€‚

â‘¦ PILï¼ˆPillowï¼‰ï¼Œï¼ˆ<https://pillow.readthedocs.io/en/stable/>ï¼‰ã€‚

â‘§ Tkinterï¼Œï¼ˆ<https://www.pythonguis.com/tkinter/>ï¼‰ã€‚

â‘¨ Ghostscriptï¼Œï¼ˆ<https://ghostscript.com/releases/gsdnld.html>ï¼‰ã€‚

â‘© cv2ï¼ˆOpenCV-Pythonï¼‰ï¼Œï¼ˆ<https://opencv24-python-tutorials.readthedocs.io/en/latest/index.html>ï¼‰ã€‚

å‚è€ƒæ–‡çŒ®ï¼ˆReferencesï¼‰:

[1] Raman, T. A., Kollar, J. & Penman, S. Chapter 17 - SASAKI: Filling the design gapâ€”Urban impressions with AI. in Artificial Intelligence in Urban Planning and Design (eds. As, I., Basu, P. & Talwar, P.) 339â€“362 (Elsevier, 2022). doi:https://doi.org/10.1016/B978-0-12-823941-4.00002-0.

[2] Isola, P., Zhu, J.-Y., Zhou, T. & Efros, A. A. Image-to-Image Translation with Conditional Adversarial Networks. (2016).

[3] Pix2pixçš„PyTorchç‰ˆæœ¬, <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix>.
