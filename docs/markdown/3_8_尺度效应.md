> Created on Sat Jul 29 08:06:41 2023 @author: Richie Bao-caDesign设计(cadesign.cn)

# 3.8 尺度效应

## 3.8.1 数据下载及预处理

### 3.8.1.1 芝加哥 NAIP（0.6m）高分辨率航拍影像生成土地覆盖（land cover，LC）数据

NAIP 航拍影像下载和解译的具体解释可以参考*NAIP航拍影像与分割模型库及Colaboratory和Planetary Computer Hub*部分，本章使用包含芝加哥城的伊利诺伊州库克郡（Illinois | Cook County）区域的 NAIP 影像。语义分割直接使用参考部分已经训练好的分割模型。

对大范围高分辨率影像数据处理需要注意内存管理，避免内存溢出，一方面，语义分割生成 LC 时，逐个瓦片的数据处理方式，避免一次性读取整个区域的数据；另一方面，合并 LC 所有瓦片时，使用[GDAL](https://gdal.org/drivers/raster/vrt.html)<sup>①</sup>库的虚拟格式 VRT（Virtual Format）。VRT 驱动是 GDAL 的格式驱动程序，允许通过重新定位，应用潜在的算法，及修改或增加各种元数据，从其它 GDAL 数据集组成虚拟 GDAL 数据集。虚拟数据集的 VRT 描述可以以 XML 格式保存，其扩展名通常为 .vrt。VRT 支持栅格数据集（GDAL）也支持矢量数据集（OGR）。虚拟栅格 VRT 可以将多个栅格组合到一个文件中，实现整个栅格数据集的可视化。VRT 文件列出了数据集中每个栅格的元数据，例如文件位置、波段数、数据类型和栅格及栅格单元大小等。根据请求通过对源栅格的引用创建栅格单元值，使得源数据只有在需要时才参与计算，延迟了计算性的密集处理，因此减小了生成的中间文件大小，这有利于创建大型数据集<sup>[1]</sup>。

#### 1） 下载库克区域的 NAIP 航拍影像

在[Google Colaboratory，CoLab](https://colab.research.google.com/?utm_source=scs-index)<sup>②</sup>中，结合使用 [Google Drive](https://www.google.com/drive/)<sup>③</sup> 完成 NAIP 航拍影像数据的下载。必要的边界文件预先传上云端。

* 首先在 CoLab 中安装需要的库，并调入所需模块。


```python
%pip install torchgeo pystac_client planetary_computer
%pip install usda rioxarray rasterstats
```


```python
import usda.utils as usda_utils
import usda.geodata_process as usda_geodataProcess

import os
import geopandas as gpd
from torchgeo.datasets.utils import download_url
from IPython.display import Image
import pystac_client
import planetary_computer
from shapely.geometry import shape
import numpy as np
```

* 链接 Google Drive，用于将数据下载到云端。


```python
from google.colab import drive
drive.mount('/content/gdrive')
```

* 定义必要的参数，例如文件读写路径等。


```python
__C=usda_utils.AttrDict()
args=__C

__C.workspace='/content/gdrive/MyDrive/dataset/naip_cook'

__C.data=usda_utils.AttrDict()
__C.data.cook_border=os.path.join(args.workspace,'Cook_County_Border.geojson') # 库克郡边界
__C.data.chicago_boundary=os.path.join(args.workspace,'Boundaries - City.geojson') # 芝加哥城边界
__C.data.naip_dir=os.path.join(args.workspace,'imgs') # 航拍影像下载保存文件夹
__C.data.items_info_fn=os.path.join(args.workspace,'items_info.geojson') # 航拍影像下载，包括索引、ID、时间、下载地址和对应的瓦片边界等
```

* 在[Planetary Computer，PC](https://planetarycomputer.microsoft.com/)<sup>④</sup>上，根据给定的边界搜索数据。

因为库克郡边界相对复杂，如果直接用于影像检索，会发生错误，因此先提取其外接矩形，用该矩形检索后，再根据库克郡边界提取内部及与其相交的影像编号用于下载。

-提取外接矩形


```python
cookcounty_border=gpd.read_file(args.data.cook_border)
cookcounty_border_envelope=cookcounty_border.envelope
cookcounty_border_envelope
```




    0    POLYGON ((-88.26365 41.46971, -87.52414 41.469...
    dtype: geometry



-转化为 JSON 数据格式


```python
AOI_json=cookcounty_border_envelope.geometry.to_json()
AOI_json
```




    '{"type": "FeatureCollection", "features": [{"id": "0", "type": "Feature", "properties": {}, "geometry": {"type": "Polygon", "coordinates": [[[-88.26365102252663, 41.469712978017064], [-87.52414271436139, 41.469712978017064], [-87.52414271436139, 42.15430113371412], [-88.26365102252663, 42.15430113371412], [-88.26365102252663, 41.469712978017064]]]}, "bbox": [-88.26365102252663, 41.469712978017064, -87.52414271436139, 42.15430113371412]}], "bbox": [-88.26365102252663, 41.469712978017064, -87.52414271436139, 42.15430113371412]}'



-提取可用于检索的边界数据格式


```python
AOI=eval(AOI_json)['features'][0]['geometry']
AOI
```




    {'type': 'Polygon',
     'coordinates': [[[-88.26365102252663, 41.469712978017064],
       [-87.52414271436139, 41.469712978017064],
       [-87.52414271436139, 42.15430113371412],
       [-88.26365102252663, 42.15430113371412],
       [-88.26365102252663, 41.469712978017064]]]}



-给定时间范围，检索数据。总共检索到 145 个瓦片对象


```python
data_range="2021-01-01/2022-01-01"

catalog = pystac_client.Client.open(
    "https://planetarycomputer.microsoft.com/api/stac/v1",
     modifier=planetary_computer.sign_inplace,
     )

catalog
```

    type "Catalog"
    id "microsoft-pc"
    stac_version "1.0.0"
    description "Searchable spatiotemporal metadata describing Earth science datasets hosted by the Microsoft Planetary Computer"
    links [] 130 items
    conformsTo [] 15 items
    title "Microsoft Planetary Computer STAC API"




```python
search_results= catalog.search(collections=["naip"], intersects=AOI, datetime=data_range)
items=search_results.item_collection()
items
```

    type "FeatureCollection"
    features [] 145 items

查看其中一个瓦片的信息。


```python
items[0]
```

    type "Feature"
    stac_version "1.0.0"
    id "il_m_4208864_sw_16_060_20210928"
    properties
    geometry
    links [] 5 items
    assets
    bbox [] 4 items
    stac_extensions [] 2 items
    collection "naip"





查看一个网片的几何边界。


```python
item_polygon=shape(items[0].geometry)
item_polygon_gdf=gpd.GeoDataFrame({'geometry':[item_polygon]},crs=4326)
item_polygon_gdf.crs
```




    <Geographic 2D CRS: EPSG:4326>
    Name: WGS 84
    Axis Info [ellipsoidal]:
    - Lat[north]: Geodetic latitude (degree)
    - Lon[east]: Geodetic longitude (degree)
    Area of Use:
    - name: World.
    - bounds: (-180.0, -90.0, 180.0, 90.0)
    Datum: World Geodetic System 1984 ensemble
    - Ellipsoid: WGS 84
    - Prime Meridian: Greenwich



定义`planetary_computer_items_filter4download()`方法，给定参数`border`，下载指定范围内的影像数据。


```python
items_gdf=usda_geodataProces.planetary_computer_items_filter4download(items,border=cookcounty_border.geometry[0])
items_gdf=usda_geodataProces.drop_overlapping_polygons(items_gdf)
```

下载时，记录了如下各影像瓦片的信息，方便查看。


```python
items_gdf=gpd.read_file(args.data.items_info_fn)
items_gdf.tail(2)
```






  <div id="df-3ef906a4-f974-4b35-b1aa-e9ce3e2d552d">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>idx</th>
      <th>id</th>
      <th>datetime</th>
      <th>url</th>
      <th>geometry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>96</th>
      <td>143</td>
      <td>il_m_4208749_sw_16_060_20210905</td>
      <td>2021-09-05 16:00:00+00:00</td>
      <td>https://naipeuwest.blob.core.windows.net/naip/...</td>
      <td>POLYGON ((-87.93239 42.12233, -87.93340 42.191...</td>
    </tr>
    <tr>
      <th>97</th>
      <td>144</td>
      <td>il_m_4208749_se_16_060_20210905</td>
      <td>2021-09-05 16:00:00+00:00</td>
      <td>https://naipeuwest.blob.core.windows.net/naip/...</td>
      <td>POLYGON ((-87.87046 42.12233, -87.87141 42.191...</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-3ef906a4-f974-4b35-b1aa-e9ce3e2d552d')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>





叠合打印库克郡边界、芝加哥城边界和下载的各个影像边界信息，核实下载的影像是否正确。


```python
chicago_boundary_gdf=gpd.read_file(args.data.chicago_boundary)

ax=items_gdf.boundary.plot()
cookcounty_border.boundary.plot(color='red',ax=ax)
chicago_boundary_gdf.boundary.plot(color='k',ax=ax);
```

将下载的信息写入至本地磁盘。


```python
items_gdf.to_file(args.data.items_info_fn)
```

在 QGIS 中查看下载的库克区域 NAIP 航拍影像。

<img src="./imgs/3_8/3_8_02.jpg" height='auto' width='auto' title="caDesign"> 

#### 2） 用已训练的分割模型解译

调用*分割模型库及Colaboratory和Planetary Computer Hub*部分已经训练的分割模型参数，

1. 首先需要对应训练时的数据配置处理影像；
2. 并建立神经网络模型后加载预训练的模型参数；
3. 最后进行分割预测；
4. 将预测的各个影像瓦片转化为 GeoTIFF 格式栅格文件；
5. 合并分割图像。

* 构建数据集和数据加载器

> [GDAL](https://gdal.org/index.html)<sup>⑤</sup>库安装时，如果直接`pip install gdal`或者`conda install -c conda-forge gdal`安装后调用出现报错， 可以从[Archived: Unofficial Windows Binaries for Python Extension Packages](https://www.lfd.uci.edu/~gohlke/pythonlibs/)<sup>⑥</sup>下载对应版本的安装包至本地安装。


```python
from torchvision.transforms import Compose
from torchgeo.transforms import AugmentationSequential
from torchgeo.samplers import PreChippedGeoSampler
from torchgeo.datasets import stack_samples 
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt
import torchvision.transforms as T
from torchgeo.trainers import SemanticSegmentationTask
import numpy as np
from tqdm import tqdm
import itertools
import rioxarray as rxr
import glob
import os
from tqdm import tqdm
from rasterio.plot import show
```

影像数据预处理的图像变换部分，有数据标准化、增加 NDVI 信息等。


```python
ndvi=usda_geodataProcess.AppendNDVI(index_nir=3, index_red=0)
ndwi=usda_geodataProcess.AppendNDWI(index_green=1, index_nir=3)

naip_transforms=Compose([
    usda_geodataProces.naip_preprocess,
    usda_geodataProces.remove_bbox,
    AugmentationSequential(ndvi,data_keys=["image"])
    ])
```

构建数据集和数据加载器。[TorchGeo](https://torchgeo.readthedocs.io/en/stable/api/samplers.html)<sup>⑦</sup>库提供了多种数据采样器，训练时通常使用`RandomGeoSampler`随机采样方法，最大化数据集尽可能返回更多的随机样本；预测时如果单一影像瓦片数据量大造成内存溢出，通常用`GridGeoSampler`方法，将单一影像数据按照指定网格大小切分后逐一预测后再拼接；本次实验 NAIP 单个影像瓦片的大小约为 $12564 \times 9683$，使用 64G 内存基本可以完成单一瓦片的预测，因此使用`PreChippedGeoSampler`方法构建数据加载器，每次一次性预测整幅影像。


```python
naip_cook_dir=r'D:\datasets\naip_cook'
X_naip=usda_geodataProcess.NAIP(naip_cook_dir,transforms=naip_transforms)
X_sampler=PreChippedGeoSampler(X_naip)
X_dataloader=DataLoader(X_naip,sampler=X_sampler, collate_fn=stack_samples,batch_size=1,shuffle=False)
```

因为相对 64G 内存，未切分的整幅影像压在内存溢出线上，有时会造成内存溢出停止循环预测，因此需要从停止的影像索引位置继续预测。切分数据加载器的方法可以用`itertools`库提供的`islice`方法实现。


```python
idx_restart=70
X_dataloader=itertools.islice(X_dataloader, idx_restart, None)
```

可以用返回的数据集属性`filepath_lst`查看对应影像的文件路径。


```python
naip_fns=(X_naip.filepath_lst)
print(f'{len(naip_fns)}\n{naip_fns[:3]}')
```

查看数据加载器中的一幅影像。


```python
for batch in X_dataloader:
    img=batch['image']    
    break   
    
X_,pad_ex=usda_geodataProcess.img_size_expand_topNright(img,base=32)
print(f'{img.shape}\n{X_.shape}\n{pad_ex}')
plt.imshow(T.ToPILImage()(X_[0][:3])); 
```

* 构建分割网络模型

配置参数同训练时的参数，直接迁移训练时的定义，并加载训练的模型参数。


```python
aux_params=dict(
    pooling='avg',             # one of 'avg', 'max'
    dropout=0.5,               # dropout ratio, default is None
    activation='sigmoid',      # activation function, default is None
    )

task=SemanticSegmentationTask(
    model='unet',
    backbone='resnet34',
    weights='imagenet',
    pretrained=True,
    in_channels=5, 
    num_classes=8,
    ignore_index=0,
    loss='ce', # 'jaccard'
    learning_rate=0.1,
    learning_rate_schedule_patience=5,
    aux_params=aux_params,
    )

trained_model_fn=r'C:\Users\richie\omen_richiebao\omen_github\model_trained\naipNDelawareSeg_last.ckpt'
unet_model=task.load_from_checkpoint(trained_model_fn)
unet_model.freeze()
```

* 预测并转化为栅格数据保存

试验性预测其中一幅图像，确定无误后循环预测全部影像。


```python
y_probs=unet_model(X_)
print(y_probs.shape)
y_pred=np.argmax(y_probs,axis=1)
print(y_pred.shape,'\n',y_pred,'\n',y_pred.unique())
```


```python
LC_color_dict=usda_geodataProcess.Seg_config.LC_color_dict
cmap_LC,norm=usda_geodataProcess.cmap4LC(LC_color_dict,list(np.unique(y_pred))) 
cmap_LC
```

查看预测结果。


```python
fig,ax=plt.subplots(figsize=(20,20))
ax.imshow(np.squeeze(y_pred),cmap=cmap_LC)
plt.show()
```

循环预测所有影像，同时定义`segarray2tiff()`方法将预测结果转化为栅格保存至本地磁盘。


```python
seg_save_dir=r'D:\datasets\naip_seg' 

completed_fn_lst=[]
i=0
for batch in tqdm(X_dataloader):
    img=batch['image']        
    X_,_=usda_geodataProcess.img_size_expand_topNright(img,base=32)
    y_probs=unet_model(X_)
    y_pred=np.argmax(y_probs,axis=1)
    img_fn=X_naip.filepath_lst[idx_restart:][i]
    usda_geodataProcess.segarray2tiff(y_pred, img_fn, seg_save_dir) 
    completed_fn_lst.append(img_fn)
    i+=1
    try:
        del img, X_, y_pred, y_probs
    except:
        pass
```

* 合并预测的分割栅格数据

因为分割栅格数据类型为 `int32`，如果`GDAL`库的版本不支持读取该类型，则需要将其降低为`int8`。


```python
seg_dir=r'D:\datasets\naip_seg' 
seg_int8_dir=r'D:\datasets\naip_seg_int8'

fn_lst=glob.glob(os.path.join(seg_dir,'*.tif'))
for fn in tqdm(fn_lst):
    src=rxr.open_rasterio(fn)
    src.rio.to_raster(os.path.join(seg_int8_dir,os.path.basename(fn)),driver='GTiff',dtype='int8')
```

定义`raster_mosaic_vrt`方法合并分割数据。


```python
seg_merged_fn=r'D:\datasets\cook_seg_merged.tif'    
usda_geodataProces.raster_mosaic_vrt(seg_int8_dir,seg_merged_fn,xRes=0.6,yRes=0.6)  
```

在 QGIS 中查看合并后的分割数据。

<img src="./imgs/3_8/3_8_03.jpg" height='auto' width='auto' title="caDesign"> 

### 3.8.1.2 观测鸟分布数据——eBird Basic Dataset (EBD)

[eBird](https://ebird.org/home)<sup>⑧</sup> 项目从每个观鸟者都有其独特的知识和经验的简单想法出发，以鸟类清单的形式收集这些信息为目标，存档并分享，推动面向科学、保护和教育新的数据驱动方法。eBird 开发有配套的工具，使观测鸟更具有价值，例如管理列表、照片和录音，到物种分布的实时地图，及观察到鸟类时的通报，提供最新和有用的信息。eBird 是世界上最大的与生物多样性相关的可续项目之一，全球每年有超过 1 亿的鸟类目击事件由 eBird 的贡献者提供，平均参与增长率约为每年 20%。eBird 是有数百个合作伙伴组织、数千名地区专家和数十万用户的协作企业，由康奈尔鸟类实验室（Cornell Lab of Ornithology）管理。通过在一个简单、科学的框架内收集的清单数据，eBird 记录鸟类的分布、丰富和生境及趋势。观测者输入他们观鸟的时间、地点和方式，然后填写一份清单，列出在郊游期间看到和听到的所有鸟类。eBird 提供的免费移动应用程序允许在世界任何地方进行离线数据收集，并提供了许多方法来探索和总结来自全球 eBird 社区的数据及其观测结果。数据质量至关重要，当进入目击时，观察者会看到一份该日期和该地区可能出现的鸟类名录，这些名录是由世界上最有知识的鸟类分布专家开发。当看到不寻常的鸟类，或报告了高数量的鸟类时，地区专家会审查这些记录<sup>[2]</sup>。

从 [EBD](https://www.gbif.org/occurrence/download?dataset_key=4fa7b334-ce0d-4e88-aaae-2e0c138d049e) <sup>⑨</sup>下载观测数据时，使用前文库克郡外接矩形边界`AOI_json`值，为 GeoJSON 数据格式，复制值到下载页面 Location 标签下的 Geometry 子标签下，下载所有时间数据，压缩文件大小为 896MB，解压后的 CSV 格式文件为 4.17GB。下图为 EBD 下载页面。

<img src="./imgs/3_8/3_8_05.png" height='auto' width='auto' title="caDesign">


```python
import usda.utils as usda_utils

import geopandas as gpd
import pandas as pd
```

读取和转化 eBird 数据为 GeoDataFrame 数据格式。首先读取为 DataFrame 数据格式。试验时所用内存大小为 64GB，满足读写时内存需求，如果内存较小，可以配合使用`pd.read_csv`方法提供的`chunksize`参数分批读取查看，或者用于拆分数据。


```python
s_t=usda_utils.start_time()

ebird_fn=r'D:\datasets\ebird\eBird_cook.csv'
ebird_df=pd.read_csv(ebird_fn,on_bad_lines='skip',sep='\t')
ebird_df.head(2)
```

    start time: 2023-08-13 11:36:54.534666
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gbifID</th>
      <th>datasetKey</th>
      <th>occurrenceID</th>
      <th>kingdom</th>
      <th>phylum</th>
      <th>class</th>
      <th>order</th>
      <th>family</th>
      <th>genus</th>
      <th>species</th>
      <th>...</th>
      <th>identifiedBy</th>
      <th>dateIdentified</th>
      <th>license</th>
      <th>rightsHolder</th>
      <th>recordedBy</th>
      <th>typeStatus</th>
      <th>establishmentMeans</th>
      <th>lastInterpreted</th>
      <th>mediaType</th>
      <th>issue</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1169395868</td>
      <td>4fa7b334-ce0d-4e88-aaae-2e0c138d049e</td>
      <td>URN:catalog:CLO:EBIRD:OBS251600139</td>
      <td>Animalia</td>
      <td>Chordata</td>
      <td>Aves</td>
      <td>Passeriformes</td>
      <td>Icteridae</td>
      <td>Agelaius</td>
      <td>Agelaius phoeniceus</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>CC_BY_4_0</td>
      <td>NaN</td>
      <td>obsr223391</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2023-06-07T15:02:30.183Z</td>
      <td>NaN</td>
      <td>CONTINENT_DERIVED_FROM_COORDINATES</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1172197891</td>
      <td>4fa7b334-ce0d-4e88-aaae-2e0c138d049e</td>
      <td>URN:catalog:CLO:EBIRD:OBS254731795</td>
      <td>Animalia</td>
      <td>Chordata</td>
      <td>Aves</td>
      <td>Passeriformes</td>
      <td>Mimidae</td>
      <td>Dumetella</td>
      <td>Dumetella carolinensis</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>CC_BY_4_0</td>
      <td>NaN</td>
      <td>obsr261364</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2023-06-07T15:02:38.836Z</td>
      <td>NaN</td>
      <td>CONTINENT_DERIVED_FROM_COORDINATES</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 50 columns</p>
</div>



查看字段和对应的一行数据。


```python
ebird_df.loc[0]
```




    gbifID                                                         1169395868
    datasetKey                           4fa7b334-ce0d-4e88-aaae-2e0c138d049e
    occurrenceID                           URN:catalog:CLO:EBIRD:OBS251600139
    kingdom                                                          Animalia
    phylum                                                           Chordata
    class                                                                Aves
    order                                                       Passeriformes
    family                                                          Icteridae
    genus                                                            Agelaius
    species                                               Agelaius phoeniceus
    infraspecificEpithet                                                  NaN
    taxonRank                                                         SPECIES
    scientificName                       Agelaius phoeniceus (Linnaeus, 1766)
    verbatimScientificName                                Agelaius phoeniceus
    verbatimScientificNameAuthorship                                      NaN
    countryCode                                                            US
    locality                            Montrose Point, Lincoln Park, Chicago
    stateProvince                                                    Illinois
    occurrenceStatus                                                  PRESENT
    individualCount                                                      10.0
    publishingOrgKey                     e2e717bf-551a-4917-bdc9-4fa0f342c530
    decimalLatitude                                                 41.963383
    decimalLongitude                                                -87.63442
    coordinateUncertaintyInMeters                                         NaN
    coordinatePrecision                                                   NaN
    elevation                                                             NaN
    elevationAccuracy                                                     NaN
    depth                                                                 NaN
    depthAccuracy                                                         NaN
    eventDate                                             2014-05-07T00:00:00
    day                                                                     7
    month                                                                   5
    year                                                                 2014
    taxonKey                                                          9409198
    speciesKey                                                        9409198
    basisOfRecord                                           HUMAN_OBSERVATION
    institutionCode                                                       CLO
    collectionCode                                                      EBIRD
    catalogNumber                                                OBS251600139
    recordNumber                                                          NaN
    identifiedBy                                                          NaN
    dateIdentified                                                        NaN
    license                                                         CC_BY_4_0
    rightsHolder                                                          NaN
    recordedBy                                                     obsr223391
    typeStatus                                                            NaN
    establishmentMeans                                                    NaN
    lastInterpreted                                  2023-06-07T15:02:30.183Z
    mediaType                                                             NaN
    issue                                  CONTINENT_DERIVED_FROM_COORDINATES
    Name: 0, dtype: object



将 DataFrame 格式数据转化为 GeoDataFrame 格式数据。


```python
ebird_gdf=gpd.GeoDataFrame(ebird_df, geometry=gpd.points_from_xy(ebird_df.decimalLongitude, ebird_df.decimalLatitude), crs="EPSG:4326")
usda_utils.duration(s_t)
```

    end time: 2023-08-13 11:39:51.408119
    Total time spend:2.93 minutes
    

可以将 eBird 数据写入为`geopandas`库支持的 SHP、GPKG、GeoJSON等数据格式，及 SQLite 数据库。本次实验存储为 GPKG 格式后，用 QGIS 打开观测数据。

> 通过用`geopandas`库存储下载区域 eBird 数据至本地磁盘，可知目前`geopandas`库对大型数据的读写速度支持并不友好，因此在后续试验时，仍旧先用`pandas`库读取为 DataFrame 格式数据后再转化为 GeoDataFrame 格式数据使用，从上述读取和转化时长来看，3 分钟左右在可接受的区间内。


```python
print(ebird_gdf.shape)
```

    (8718718, 51)
    


```python
ebird_gpkg_fn=r'D:\datasets\ebird\ebird_cook.gpkg'
ebird_gdf.to_file(ebird_gpkg_fn,driver='GPKG',layer='ebird')
```

在 QGIS 中打开查看 eBird 数据。

<img src="./imgs/3_8/3_8_04.jpg" height='auto' width='auto' title="caDesign">

### 3.8.1.3 人口分布数据——[WorldPop](https://hub.worldpop.org/)<sup>⑩</sup>

WorldPop 为一个跨学科应用研究小组，主要侧重于支持改进空间人口证据库（spatial demographic evidence base），使用经同行评议的研究方法用于构建关于人口分布、人口统计和动态高分辨率开源的地理空间数据，并将其应用于健康和发展应用，包括实现可持续发展目标。其使命是使决策者能够通过高质量的应用研究利用空间人口数据的驱动，实现一个世界的愿景，在这个世界里，每个人无论在哪里，都能参与决策<sup>[3]</sup>。

WorldPop 提供有多种数据集，涉及人口数量（Population Count）、人口密度（Population Density）、人口加权密度（Population Weight Density）、年龄和性别结构（Age and sex structure）等。本次试验下载的数据为 2020年，高空分辨率为 100m 的美国区域人口数量分布栅格数据。该数据的估计是使用多源数据，主要包括含 13 个类别 30m 高空分辨率的土地覆盖（Land Cover）、数字高程及其衍生的坡度数据、由 MODIS 17A3 估计的净初级生产力（ net primary productivity，NPP）、夜间灯光、年平均降水量和平均气温、及与人口存在相关的地理空间数据，如道路和水道网络、较大的水体、聚落或人口密集的地点、各种城市基础服务设施（如诊所、医院和学校）等作为解释变量，以精细的人口普查数据对数转换的人口密度（log transformed population density）为结果变量，用随机森林（Random Forest）构建预测模型，预测全球人口分布<sup>[4]</sup>。

读取和打印查看库克郡区域的人口数量。


```python
us_pop_fn=r'D:\datasets\worldpop\usa_ppp_2020_UNadj_constrained.tif'
cook_border=r'D:\datasets\scale\Cook_County_Border.geojson'

cookcounty_border=gpd.read_file(cook_border)
cookcounty_border_envelope=cookcounty_border.envelope
cookcounty_border_envelope
left_bottom,right_top=usda_geodataProcess.find_boundingRectangle_coordis([cookcounty_border_envelope[0]])
LC_part,transform,ras_meta=usda_geodataProcess.rio_read_subset(us_pop_fn,[left_bottom,right_top]) 
LC_part[LC_part<0]=0
```


```python
plt.style.use('default')
fig, ax=plt.subplots(figsize=(10,10)) 
show(np.squeeze(LC_part),ax=ax,transform=transform,cmap='hot')
plt.show()
```


<img src="./imgs/3_8/output_71_0.png" height='auto' width='auto' title="caDesign">    



## 3.8.2 尺度的宽泛释义

景观生态学（landscape ecology）中的相关研究需要考虑尺度问题，这到20世纪80年代在生态学中得到广泛认识，例如样方大小对物种/区域关系的测量和识别的影响，并且发现，没有一个单一的时间和空间维度适用于所有生态问题的研究。尺度（scale）是指对象（object）或过程（process）的空间（spatial）或时间（temporal）维度。尺度通常以粒度（grain）和幅度（extent）为特征。粒度为给定的数据集可能的最佳空间分辨率级别；幅度为研究区域的大小或者时间跨度<sup>[5]</sup>。

## 3.8.2.1 栅格的粒度和幅度

栅格数据的粒度通常为栅格单元大小（cell size），例如前文下载的 NAIP 航拍影像数据及基于其预测的土地覆盖（landcover，LC）类型栅格数据的粒度约为 0.6m。下述试验则试图通过对 0.6m 粒度的 LC 数据粒度和幅度的变化进行比较观察。


```python
%load_ext autoreload 
%autoreload 2
import usda.geodata_process as usda_geodataProcess

import rioxarray as rxr
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from rasterio.plot import show
import shapely
import geopandas as gpd
import pandas as pd
```

    The autoreload extension is already loaded. To reload it, use:
      %reload_ext autoreload
    

读取 LC 栅格元数据，用其投影（CRS）信息变换样方左下角和右上角的经纬度坐标。


```python
seg_merged_fn=r'D:\datasets\cook_seg_merged.tif'   
LC=rxr.open_rasterio(seg_merged_fn)
grain_size=[round(i,3) for i in LC.rio.resolution()]
print(f'grain size (cell size)={grain_size}',)
LC
```

    grain size (cell size)=[0.6, -0.6]
    




<div><svg style="position: absolute; width: 0; height: 0; overflow: hidden">
<defs>
<symbol id="icon-database" viewBox="0 0 32 32">
<path d="M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z"></path>
<path d="M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
<path d="M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
</symbol>
<symbol id="icon-file-text2" viewBox="0 0 32 32">
<path d="M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z"></path>
<path d="M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
<path d="M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
<path d="M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
</symbol>
</defs>
</svg>
<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.
 *
 */

:root {
  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));
  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));
  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));
  --xr-border-color: var(--jp-border-color2, #e0e0e0);
  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);
  --xr-background-color: var(--jp-layout-color0, white);
  --xr-background-color-row-even: var(--jp-layout-color1, white);
  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);
}

html[theme=dark],
body[data-theme=dark],
body.vscode-dark {
  --xr-font-color0: rgba(255, 255, 255, 1);
  --xr-font-color2: rgba(255, 255, 255, 0.54);
  --xr-font-color3: rgba(255, 255, 255, 0.38);
  --xr-border-color: #1F1F1F;
  --xr-disabled-color: #515151;
  --xr-background-color: #111111;
  --xr-background-color-row-even: #111111;
  --xr-background-color-row-odd: #313131;
}

.xr-wrap {
  display: block !important;
  min-width: 300px;
  max-width: 700px;
}

.xr-text-repr-fallback {
  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */
  display: none;
}

.xr-header {
  padding-top: 6px;
  padding-bottom: 6px;
  margin-bottom: 4px;
  border-bottom: solid 1px var(--xr-border-color);
}

.xr-header > div,
.xr-header > ul {
  display: inline;
  margin-top: 0;
  margin-bottom: 0;
}

.xr-obj-type,
.xr-array-name {
  margin-left: 2px;
  margin-right: 10px;
}

.xr-obj-type {
  color: var(--xr-font-color2);
}

.xr-sections {
  padding-left: 0 !important;
  display: grid;
  grid-template-columns: 150px auto auto 1fr 20px 20px;
}

.xr-section-item {
  display: contents;
}

.xr-section-item input {
  display: none;
}

.xr-section-item input + label {
  color: var(--xr-disabled-color);
}

.xr-section-item input:enabled + label {
  cursor: pointer;
  color: var(--xr-font-color2);
}

.xr-section-item input:enabled + label:hover {
  color: var(--xr-font-color0);
}

.xr-section-summary {
  grid-column: 1;
  color: var(--xr-font-color2);
  font-weight: 500;
}

.xr-section-summary > span {
  display: inline-block;
  padding-left: 0.5em;
}

.xr-section-summary-in:disabled + label {
  color: var(--xr-font-color2);
}

.xr-section-summary-in + label:before {
  display: inline-block;
  content: '►';
  font-size: 11px;
  width: 15px;
  text-align: center;
}

.xr-section-summary-in:disabled + label:before {
  color: var(--xr-disabled-color);
}

.xr-section-summary-in:checked + label:before {
  content: '▼';
}

.xr-section-summary-in:checked + label > span {
  display: none;
}

.xr-section-summary,
.xr-section-inline-details {
  padding-top: 4px;
  padding-bottom: 4px;
}

.xr-section-inline-details {
  grid-column: 2 / -1;
}

.xr-section-details {
  display: none;
  grid-column: 1 / -1;
  margin-bottom: 5px;
}

.xr-section-summary-in:checked ~ .xr-section-details {
  display: contents;
}

.xr-array-wrap {
  grid-column: 1 / -1;
  display: grid;
  grid-template-columns: 20px auto;
}

.xr-array-wrap > label {
  grid-column: 1;
  vertical-align: top;
}

.xr-preview {
  color: var(--xr-font-color3);
}

.xr-array-preview,
.xr-array-data {
  padding: 0 5px !important;
  grid-column: 2;
}

.xr-array-data,
.xr-array-in:checked ~ .xr-array-preview {
  display: none;
}

.xr-array-in:checked ~ .xr-array-data,
.xr-array-preview {
  display: inline-block;
}

.xr-dim-list {
  display: inline-block !important;
  list-style: none;
  padding: 0 !important;
  margin: 0;
}

.xr-dim-list li {
  display: inline-block;
  padding: 0;
  margin: 0;
}

.xr-dim-list:before {
  content: '(';
}

.xr-dim-list:after {
  content: ')';
}

.xr-dim-list li:not(:last-child):after {
  content: ',';
  padding-right: 5px;
}

.xr-has-index {
  font-weight: bold;
}

.xr-var-list,
.xr-var-item {
  display: contents;
}

.xr-var-item > div,
.xr-var-item label,
.xr-var-item > .xr-var-name span {
  background-color: var(--xr-background-color-row-even);
  margin-bottom: 0;
}

.xr-var-item > .xr-var-name:hover span {
  padding-right: 5px;
}

.xr-var-list > li:nth-child(odd) > div,
.xr-var-list > li:nth-child(odd) > label,
.xr-var-list > li:nth-child(odd) > .xr-var-name span {
  background-color: var(--xr-background-color-row-odd);
}

.xr-var-name {
  grid-column: 1;
}

.xr-var-dims {
  grid-column: 2;
}

.xr-var-dtype {
  grid-column: 3;
  text-align: right;
  color: var(--xr-font-color2);
}

.xr-var-preview {
  grid-column: 4;
}

.xr-index-preview {
  grid-column: 2 / 5;
  color: var(--xr-font-color2);
}

.xr-var-name,
.xr-var-dims,
.xr-var-dtype,
.xr-preview,
.xr-attrs dt {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  padding-right: 10px;
}

.xr-var-name:hover,
.xr-var-dims:hover,
.xr-var-dtype:hover,
.xr-attrs dt:hover {
  overflow: visible;
  width: auto;
  z-index: 1;
}

.xr-var-attrs,
.xr-var-data,
.xr-index-data {
  display: none;
  background-color: var(--xr-background-color) !important;
  padding-bottom: 5px !important;
}

.xr-var-attrs-in:checked ~ .xr-var-attrs,
.xr-var-data-in:checked ~ .xr-var-data,
.xr-index-data-in:checked ~ .xr-index-data {
  display: block;
}

.xr-var-data > table {
  float: right;
}

.xr-var-name span,
.xr-var-data,
.xr-index-name div,
.xr-index-data,
.xr-attrs {
  padding-left: 25px !important;
}

.xr-attrs,
.xr-var-attrs,
.xr-var-data,
.xr-index-data {
  grid-column: 1 / -1;
}

dl.xr-attrs {
  padding: 0;
  margin: 0;
  display: grid;
  grid-template-columns: 125px auto;
}

.xr-attrs dt,
.xr-attrs dd {
  padding: 0;
  margin: 0;
  float: left;
  padding-right: 10px;
  width: auto;
}

.xr-attrs dt {
  font-weight: normal;
  grid-column: 1;
}

.xr-attrs dt:hover span {
  display: inline-block;
  background: var(--xr-background-color);
  padding-right: 10px;
}

.xr-attrs dd {
  grid-column: 2;
  white-space: pre-wrap;
  word-break: break-all;
}

.xr-icon-database,
.xr-icon-file-text2,
.xr-no-icon {
  display: inline-block;
  vertical-align: middle;
  width: 1em;
  height: 1.5em !important;
  stroke-width: 0;
  stroke: currentColor;
  fill: currentColor;
}
</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray (band: 1, y: 140841, x: 113056)&gt;
[15922920096 values with dtype=uint8]
Coordinates:
  * band         (band) int32 1
  * x            (x) float64 3.909e+05 3.909e+05 ... 4.588e+05 4.588e+05
  * y            (y) float64 4.672e+06 4.672e+06 ... 4.587e+06 4.587e+06
    spatial_ref  int32 0
Attributes:
    AREA_OR_POINT:             Area
    STATISTICS_APPROXIMATE:    YES
    STATISTICS_MAXIMUM:        6
    STATISTICS_MEAN:           2.5870012262799
    STATISTICS_MINIMUM:        0
    STATISTICS_STDDEV:         2.3042352364215
    STATISTICS_VALID_PERCENT:  100
    scale_factor:              1.0
    add_offset:                0.0</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span class='xr-has-index'>band</span>: 1</li><li><span class='xr-has-index'>y</span>: 140841</li><li><span class='xr-has-index'>x</span>: 113056</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-b8f0473b-7d30-4722-bc32-7445fdab934f' class='xr-array-in' type='checkbox' checked><label for='section-b8f0473b-7d30-4722-bc32-7445fdab934f' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>...</span></div><div class='xr-array-data'><pre>[15922920096 values with dtype=uint8]</pre></div></div></li><li class='xr-section-item'><input id='section-7fd26b9d-a83c-46b2-bd51-61daa4027eef' class='xr-section-summary-in' type='checkbox'  checked><label for='section-7fd26b9d-a83c-46b2-bd51-61daa4027eef' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>band</span></div><div class='xr-var-dims'>(band)</div><div class='xr-var-dtype'>int32</div><div class='xr-var-preview xr-preview'>1</div><input id='attrs-ce459a57-937c-497b-922a-b645d4b2e441' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-ce459a57-937c-497b-922a-b645d4b2e441' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f1de7ded-8de7-44b5-bb02-3563d1c9def9' class='xr-var-data-in' type='checkbox'><label for='data-f1de7ded-8de7-44b5-bb02-3563d1c9def9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([1])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>3.909e+05 3.909e+05 ... 4.588e+05</div><input id='attrs-23193a12-8d79-422f-b1ac-5b90297d165d' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-23193a12-8d79-422f-b1ac-5b90297d165d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-273cc4ac-1621-4e8f-ad9f-c039a14e7208' class='xr-var-data-in' type='checkbox'><label for='data-273cc4ac-1621-4e8f-ad9f-c039a14e7208' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([390937.5, 390938.1, 390938.7, ..., 458769.3, 458769.9, 458770.5])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>4.672e+06 4.672e+06 ... 4.587e+06</div><input id='attrs-a16a3c72-0353-4cd7-bfca-c92a0d8698c9' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a16a3c72-0353-4cd7-bfca-c92a0d8698c9' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-262bf574-b1e2-48a7-9bdd-f395fe7f7031' class='xr-var-data-in' type='checkbox'><label for='data-262bf574-b1e2-48a7-9bdd-f395fe7f7031' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([4671705.3, 4671704.7, 4671704.1, ..., 4587202.5, 4587201.9, 4587201.3])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int32</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-b3573261-c56d-40eb-bf76-29b322f0113c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b3573261-c56d-40eb-bf76-29b322f0113c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c5e5c6ab-4ae3-435b-b367-9be6acdbefe3' class='xr-var-data-in' type='checkbox'><label for='data-c5e5c6ab-4ae3-435b-b367-9be6acdbefe3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>PROJCS[&quot;NAD83 / UTM zone 16N&quot;,GEOGCS[&quot;NAD83&quot;,DATUM[&quot;North_American_Datum_1983&quot;,SPHEROID[&quot;GRS 1980&quot;,6378137,298.257222101,AUTHORITY[&quot;EPSG&quot;,&quot;7019&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6269&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4269&quot;]],PROJECTION[&quot;Transverse_Mercator&quot;],PARAMETER[&quot;latitude_of_origin&quot;,0],PARAMETER[&quot;central_meridian&quot;,-87],PARAMETER[&quot;scale_factor&quot;,0.9996],PARAMETER[&quot;false_easting&quot;,500000],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,EAST],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;26916&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314140356</dd><dt><span>inverse_flattening :</span></dt><dd>298.257222101</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>GRS 1980</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>NAD83</dd><dt><span>horizontal_datum_name :</span></dt><dd>North American Datum 1983</dd><dt><span>projected_crs_name :</span></dt><dd>NAD83 / UTM zone 16N</dd><dt><span>grid_mapping_name :</span></dt><dd>transverse_mercator</dd><dt><span>latitude_of_projection_origin :</span></dt><dd>0.0</dd><dt><span>longitude_of_central_meridian :</span></dt><dd>-87.0</dd><dt><span>false_easting :</span></dt><dd>500000.0</dd><dt><span>false_northing :</span></dt><dd>0.0</dd><dt><span>scale_factor_at_central_meridian :</span></dt><dd>0.9996</dd><dt><span>spatial_ref :</span></dt><dd>PROJCS[&quot;NAD83 / UTM zone 16N&quot;,GEOGCS[&quot;NAD83&quot;,DATUM[&quot;North_American_Datum_1983&quot;,SPHEROID[&quot;GRS 1980&quot;,6378137,298.257222101,AUTHORITY[&quot;EPSG&quot;,&quot;7019&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6269&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4269&quot;]],PROJECTION[&quot;Transverse_Mercator&quot;],PARAMETER[&quot;latitude_of_origin&quot;,0],PARAMETER[&quot;central_meridian&quot;,-87],PARAMETER[&quot;scale_factor&quot;,0.9996],PARAMETER[&quot;false_easting&quot;,500000],PARAMETER[&quot;false_northing&quot;,0],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,EAST],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;26916&quot;]]</dd><dt><span>GeoTransform :</span></dt><dd>390937.2 0.6000000000000012 0.0 4671705.6 0.0 -0.5999999999999852</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-60601cc9-70b1-4416-93e7-e59c4dd5f340' class='xr-section-summary-in' type='checkbox'  ><label for='section-60601cc9-70b1-4416-93e7-e59c4dd5f340' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>band</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-09a1b8d8-9aec-4ef8-bf55-33b868e80db0' class='xr-index-data-in' type='checkbox'/><label for='index-09a1b8d8-9aec-4ef8-bf55-33b868e80db0' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([1], dtype=&#x27;int32&#x27;, name=&#x27;band&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>x</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-442041c8-a6b2-4746-8795-8ca71309e24b' class='xr-index-data-in' type='checkbox'/><label for='index-442041c8-a6b2-4746-8795-8ca71309e24b' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([          390937.5,           390938.1,           390938.7,
                 390939.3,           390939.9,           390940.5,
                 390941.1,           390941.7,           390942.3,
                 390942.9,
       ...
       458765.10000000015,  458765.7000000001, 458766.30000000016,
       458766.90000000014,  458767.5000000001, 458768.10000000015,
        458768.7000000001, 458769.30000000016, 458769.90000000014,
        458770.5000000001],
      dtype=&#x27;float64&#x27;, name=&#x27;x&#x27;, length=113056))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>y</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-6ce75d47-dfd9-4e7f-bd05-54bba9527bbf' class='xr-index-data-in' type='checkbox'/><label for='index-6ce75d47-dfd9-4e7f-bd05-54bba9527bbf' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([         4671705.3,          4671704.7,          4671704.1,
                4671703.5,  4671702.899999999,          4671702.3,
                4671701.7,          4671701.1,          4671700.5,
        4671699.899999999,
       ...
        4587206.700000002, 4587206.1000000015,  4587205.500000002,
        4587204.900000002,  4587204.300000002,  4587203.700000002,
       4587203.1000000015,  4587202.500000002,  4587201.900000002,
        4587201.300000002],
      dtype=&#x27;float64&#x27;, name=&#x27;y&#x27;, length=140841))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-e7a37ccd-249d-45e2-8f98-63bed1de7fa1' class='xr-section-summary-in' type='checkbox'  checked><label for='section-e7a37ccd-249d-45e2-8f98-63bed1de7fa1' class='xr-section-summary' >Attributes: <span>(9)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>AREA_OR_POINT :</span></dt><dd>Area</dd><dt><span>STATISTICS_APPROXIMATE :</span></dt><dd>YES</dd><dt><span>STATISTICS_MAXIMUM :</span></dt><dd>6</dd><dt><span>STATISTICS_MEAN :</span></dt><dd>2.5870012262799</dd><dt><span>STATISTICS_MINIMUM :</span></dt><dd>0</dd><dt><span>STATISTICS_STDDEV :</span></dt><dd>2.3042352364215</dd><dt><span>STATISTICS_VALID_PERCENT :</span></dt><dd>100</dd><dt><span>scale_factor :</span></dt><dd>1.0</dd><dt><span>add_offset :</span></dt><dd>0.0</dd></dl></div></li></ul></div></div>



在幅度变化上，选择了100、500、1000和1500等 4 个连续大小变化的样方。所有幅度固定样方左下角位置坐标，仅通过变化右上角坐标调整样方大小。通过各个样方，用定义的`rio_read_subset()`方法读取对应样方的 LC 栅格数据，返回的值有 LC 栅格值数组`LC`，变换信息`transform`，元数据`ras_meta`等，其中变换信息用于后续地图打印、[xarray](https://docs.xarray.dev/en/stable/)<sup>⑪</sup>或[rioxarray](https://corteva.github.io/rioxarray/stable/)<sup>⑫</sup>库中[DataArray](https://docs.xarray.dev/en/stable/user-guide/data-structures.html)<sup>⑬</sup>格式数据重建。


```python
pt_leftBottom=[-87.840427, 41.876229]
pt_leftBottom_pj=usda_geodataProcess.pt_coordi_transform(4326,LC.rio.crs,pt_leftBottom)
ws=[100,500,1000,1500]
pt_rightTop_pj_lst=[]
for w in ws:    
    pt_rightTop_pj=[i+w for i in pt_leftBottom_pj] 
    pt_rightTop_pj_lst.append(pt_rightTop_pj)
```


```python
LC_extents=[]
for pt_rightTop_pj in tqdm(pt_rightTop_pj_lst):
    LC_part,transform,ras_meta=usda_geodataProcess.rio_read_subset(seg_merged_fn,[pt_leftBottom_pj,pt_rightTop_pj])  
    LC_part_info={'coordi':[pt_leftBottom_pj,pt_rightTop_pj],'LC':LC_part,'transform':transform,'ras_meta':ras_meta}  
    LC_extents.append(LC_part_info)
```

    100%|███████████████████████████████████████████████████| 4/4 [00:00<00:00, 10.83it/s]
    

为了打印 LC 地图时，在后一较大幅度地图中对应打印前一较小幅度边界，观察幅度对位关系，需要根据左下角和右上角坐标构建样方的几何矩形，并定义为`GeoDataFrame`格式数据。


```python
extend_shp_lst=[]
for pt_rightTop_pj in pt_rightTop_pj_lst:
    extend_shp_lst.append(shapely.box(*pt_leftBottom_pj,*pt_rightTop_pj))
extend_df=pd.DataFrame({'geometry':extend_shp_lst})
extend_gdf=gpd.GeoDataFrame(extend_df,geometry='geometry',crs=LC.rio.crs)
```

因为不同幅度的 LC 数据的粒度均为 0.6m，为了观察粒度的变化，定义`upNdownsampling()`方法实现重采样，包括上采样（upsampling）和下采样（downsampling）。配置下采样的比例有1/10、1/40和1/80等3个连续变化比例值，对应的栅格单元大小分别约为 6.0、25.0和50.0。对于 LC 数据的采样方法选择`mode`方式，选择所有采样点中出现次数最多的值。


定义的`upNdownsampling()`方法输入参数为`DataArray`数据格式，因此需要将各个幅度的 LC 数组转化为`DataArray`格式数据，定义`array2dataa下述打印结果中红色矩形为当前幅度的前一幅度样方边界。rray()`方法实现。


```python
fig, axs = plt.subplots(4, 4,figsize=(20,20))
LC_color_dict=usda_geodataProcess.Seg_config.LC_color_dict
scale_factors=[1/10,1/40,1/80]

for ncol,LC_info_dict in enumerate(LC_extents):
    LC_part=LC_info_dict['LC']
    transform=LC_info_dict['transform']
    cmap_LC,norm=usda_geodataProcess.cmap4LC(LC_color_dict,list(np.unique(LC_part)))
    ax=axs[ncol,0]
    show(np.squeeze(LC_part),ax=ax,cmap=cmap_LC,transform=transform)
    if ncol>0:
        extend_gdf.iloc[[ncol-1]].plot(color='none',edgecolor='red',linewidth=3,ax=ax,linestyle='-')
    ax.set_title(f'extent={ws[ncol]}$^2$;grain size={grain_size}')   
    ax.tick_params(axis='x',labelrotation=90)

    xda=usda_geodataProcess.array2dataarray(LC_part,transform,crs=LC.rio.crs) 
    for nrow,scale_factor in enumerate(scale_factors):
        downsampled=usda_geodataProcess.upNdownsampling(xda,scale_factor)   
        ax_row=axs[ncol,nrow+1]
        d_transform=downsampled.rio.transform()
        cmap_LC,norm=usda_geodataProcess.cmap4LC(LC_color_dict,list(np.unique(downsampled)))
        show(np.squeeze(downsampled.data),ax=ax_row,cmap=cmap_LC,transform=d_transform)
        new_grain_size=[round(i,3) for i in downsampled.rio.resolution()]
        ax_row.set_title(f'extent={ws[ncol]}$^2$;grain size={new_grain_size}')  
        if ncol>0:  
            extend_gdf.iloc[[ncol-1]].plot(color='none',edgecolor='red',linewidth=3,ax=ax_row,linestyle='-')     
        ax_row.tick_params(axis='x',labelrotation=90)
        
plt.tight_layout()        
plt.show()
```


<img src="./imgs/3_8/output_82_0.png" height='auto' width='auto' title="caDesign">    
    


打印查看提供的所有采样方法。通常根据不同的数据类型、不同的分析目的选择适合的采样方式，例如土地覆盖类型分类数据以类型出现频数最多的为结果值（`mode`）相对合理；表达空气污染浓度、地表温度等连续值，取均值（`average`）、最小（`min`）或最大值（`max`）、分位数（`q1`、`q3`）或求和（`sum`）等都相对合理，需要根据分析目的进一步确定。


```python
from rasterio.enums import Resampling
print(Resampling.__doc__)
```

    Available warp resampling algorithms.
        
        Attributes
        ----------
        nearest
            Nearest neighbor resampling (default, fastest algorithm, worst interpolation quality).
        bilinear
            Bilinear resampling.
        cubic
            Cubic resampling.
        cubic_spline
            Cubic spline resampling.
        lanczos
            Lanczos windowed sinc resampling.
        average
            Average resampling, computes the weighted average of all non-NODATA contributing pixels.
        mode
            Mode resampling, selects the value which appears most often of all the sampled points.
        gauss
            Gaussian resampling, Note: not available to the functions in rio.warp.
        max
            Maximum resampling, selects the maximum value from all non-NODATA contributing pixels. (GDAL >= 2.0)
        min
            Minimum resampling, selects the minimum value from all non-NODATA contributing pixels. (GDAL >= 2.0)
        med
            Median resampling, selects the median value of all non-NODATA contributing pixels. (GDAL >= 2.0)
        q1
            Q1, first quartile resampling, selects the first quartile value of all non-NODATA contributing pixels. (GDAL >= 2.0)
        q3
            Q3, third quartile resampling, selects the third quartile value of all non-NODATA contributing pixels. (GDAL >= 2.0)
        sum
            Sum, compute the weighted sum of all non-NODATA contributing pixels. (GDAL >= 3.1)
        rms
            RMS, root mean square / quadratic mean of all non-NODATA contributing pixels. (GDAL >= 3.3)
        
        Notes
        ----------
        The first 8, 'nearest', 'bilinear', 'cubic', 'cubic_spline',
        'lanczos', 'average', 'mode', and 'gauss', are available for making
        dataset overviews.
    
        'max', 'min', 'med', 'q1', 'q3' are only supported in GDAL >= 2.0.0.
    
        'nearest', 'bilinear', 'cubic', 'cubic_spline', 'lanczos',
        'average', 'mode' are always available (GDAL >= 1.10).
    
        'sum' is only supported in GDAL >= 3.1.
    
        'rms' is only supported in GDAL >= 3.3.
    
        Note: 'gauss' is not available to the functions in rio.warp.
        
    

## 3.8.2.2 尺度的其它类型

除了栅格单元大小反应空间维度外，各类模型中表征空间尺度的参数实际上都是尺度的一种表现，例如聚类簇数量的配置（如`KMeans`）、空间距离参数的配置（如`DBSCAN`）、分类中邻元的数量（如`KNeighborsClassifier`），及卷积核的大小等。下面以 `DBSCAN` 距离聚类为例，通过配置参数`eps`不同大小值，观察聚类结果。


```python
from sklearn.cluster import DBSCAN
from tqdm import tqdm
import geopandas as gpd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
```


```python
ebird_gdf.tail(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>gbifID</th>
      <th>datasetKey</th>
      <th>occurrenceID</th>
      <th>kingdom</th>
      <th>phylum</th>
      <th>class</th>
      <th>order</th>
      <th>family</th>
      <th>genus</th>
      <th>species</th>
      <th>...</th>
      <th>dateIdentified</th>
      <th>license</th>
      <th>rightsHolder</th>
      <th>recordedBy</th>
      <th>typeStatus</th>
      <th>establishmentMeans</th>
      <th>lastInterpreted</th>
      <th>mediaType</th>
      <th>issue</th>
      <th>geometry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8718716</th>
      <td>3603005263</td>
      <td>4fa7b334-ce0d-4e88-aaae-2e0c138d049e</td>
      <td>URN:catalog:CLO:EBIRD:OBS1249791849</td>
      <td>Animalia</td>
      <td>Chordata</td>
      <td>Aves</td>
      <td>Strigiformes</td>
      <td>Strigidae</td>
      <td>Bubo</td>
      <td>Bubo virginianus</td>
      <td>...</td>
      <td>NaN</td>
      <td>CC_BY_4_0</td>
      <td>NaN</td>
      <td>obsr326014</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2023-06-07T15:29:49.508Z</td>
      <td>NaN</td>
      <td>CONTINENT_DERIVED_FROM_COORDINATES</td>
      <td>POINT (-87.86573 41.60509)</td>
    </tr>
    <tr>
      <th>8718717</th>
      <td>3641927714</td>
      <td>4fa7b334-ce0d-4e88-aaae-2e0c138d049e</td>
      <td>URN:catalog:CLO:EBIRD:OBS1273643367</td>
      <td>Animalia</td>
      <td>Chordata</td>
      <td>Aves</td>
      <td>Coraciiformes</td>
      <td>Alcedinidae</td>
      <td>Megaceryle</td>
      <td>Megaceryle alcyon</td>
      <td>...</td>
      <td>NaN</td>
      <td>CC_BY_4_0</td>
      <td>NaN</td>
      <td>obsr607938</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2023-06-07T15:18:17.410Z</td>
      <td>NaN</td>
      <td>CONTINENT_DERIVED_FROM_COORDINATES</td>
      <td>POINT (-87.54932 41.75883)</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 51 columns</p>
</div>



构建$10000^2$大小的样方，提取该样方区域内观测鸟数据。


```python
pt_leftBottom=[-87.738802, 41.966790]
pt_leftBottom_pj=usda_geodataProcess.pt_coordi_transform(4326,LC.rio.crs,pt_leftBottom)
pt_rightTop_pj=[i+10000 for i in pt_leftBottom_pj]

quadrat_df=pd.DataFrame({'geometry':[shapely.box(*pt_leftBottom_pj,*pt_rightTop_pj)]})
quadrat_gdf=gpd.GeoDataFrame(quadrat_df,geometry='geometry',crs=LC.rio.crs)
```

提取的样方内数据约37万条。将提取的数据单独写入到本地磁盘，方便读取。


```python
ebird_gdf_prj=ebird_gdf.to_crs(LC.rio.crs)
quadrat=gpd.clip(ebird_gdf_prj,quadrat_gdf)
quadrat_fn=r'D:\datasets\ebird\ebird_quadrat.gpkg'
quadrat.to_file(quadrat_fn,driver='GPKG', layer='quadrat')
quadrat.shape
```




    (368556, 51)




```python
quadrat_fn=r'D:\datasets\ebird\ebird_quadrat.gpkg'
quadrat=gpd.read_file(quadrat_fn)
```

执行`DBSCAN`聚类，配置的`eps`有10、50和100m等3个两个样本之间的最大距离，如果两个样本之间的距离小于最大距离，则这两个样本归为一簇，一个样本为另一个的邻域。


```python
eps_lst=[10,50,100]
observed_pts=np.array([(x,y) for x,y in zip(quadrat['geometry'].x , quadrat['geometry'].y)])

clustering_labels={}
for eps in tqdm(eps_lst):
    clustering=DBSCAN(eps=eps, min_samples=3).fit(observed_pts)
    clustering_labels[eps]=clustering.labels_
```

    100%|███████████████████████████████████████████████████| 3/3 [04:23<00:00, 87.69s/it]
    

为了减小数据量，仅提取必要的列信息，并合并聚类结果为列。


```python
cols=['gbifID','kingdom','phylum','class','order','family','genus','species','scientificName','verbatimScientificName','locality','individualCount','eventDate','day','month','year','geometry']
quadrat_sel=quadrat[cols]
quadrat_cluster=pd.DataFrame.from_dict({f'cluster_{k}':v for k,v in clustering_labels.items()})
quadrat_sel_cluster=pd.concat([quadrat_sel,quadrat_cluster],axis=1)

quadrat_sel_cluster_fn=r'D:\datasets\ebird\quadrat_sel_cluster.gpkg'
quadrat_sel_cluster.to_file(quadrat_sel_cluster_fn,driver='GPKG', layer='cluster')
```

打印3个不同距离聚类结果，聚类簇的数量分别为2048，1289和669等不同的值，表明距离尺度对结果有影响。


```python
fig, axs = plt.subplots(1, 3,figsize=(20,6))
for idx,col in enumerate(['cluster_10','cluster_50','cluster_100']):
    unique_num=quadrat_sel_cluster[col].nunique()
    cmap = matplotlib.colors.ListedColormap (np.random.rand (unique_num,3))
    quadrat_sel_cluster.plot(column=col,ax=axs[idx],cmap=cmap,markersize=1)
    axs[idx].set_title(f'{col}:({unique_num})')

plt.show()
```


<img src="./imgs/3_8/output_98_0.png" height='auto' width='auto' title="caDesign">   



## 3.8.3 不同尺度作用结果的比较分析方法

* 分类数据

### 3.8.3.1 标记距离的尺度变化矩阵

同一幅度（分析范围同）不同尺度的栅格单元大小不同，长宽两个方向的栅格单元数不同，因此两个不同维度的矩阵比较时，使用*标记距离*中解释的类/簇大小直方图 （class/clump-size histogram）、共现关系（Co-occurrence）和层级分解（Hierarchical decomposition）等方法转化为同一维度矩阵后计算两两尺度间的距离。本次例举的方法为层级分解。

首先构建 5000m 的样方作为试验区域，提取 LC 数据。


```python
%load_ext autoreload 
%autoreload 2
from usda import pattern_signature as usda_signature
from usda import stats as usda_stats

import matplotlib.pyplot as plt
import seaborn as sns
from kneed import KneeLocator, DataGenerator
```

    The autoreload extension is already loaded. To reload it, use:
      %reload_ext autoreload
    


```python
pt_leftBottom=[-87.738802, 41.966790]
pt_leftBottom_pj=usda_geodataProcess.pt_coordi_transform(4326,LC.rio.crs,pt_leftBottom)
pt_rightTop_pj=[i+5000 for i in pt_leftBottom_pj]
LC_part,transform,ras_meta=usda_geodataProcess.rio_read_subset(seg_merged_fn,[pt_leftBottom_pj,pt_rightTop_pj])  
```

降采样时，按缩放比例构建了144个连续变化值。因为栅格单元数为整数值，因此部分比例值缩放后具有相同的尺度大小，例如 $5000*(1/101)=49.504$和$5000*(1/102)=49.019$，向下取整时均为49。这从下述打印的图表中就可观察到这一现象。


```python
scale_factors=[1/(10*i) for i in range(1,145,1)] 
downsampled_dict={}
xda=usda_geodataProcess.array2dataarray(LC_part,transform,crs=LC.rio.crs) 
for nrow,scale_factor in enumerate(scale_factors):
    downsampled=usda_geodataProcess.upNdownsampling(xda,scale_factor) 
    downsampled_dict[scale_factor]=downsampled
```


```python
fig, axs = plt.subplots(10, 10,figsize=(20,20))
LC_color_dict=usda_geodataProcess.Seg_config.LC_color_dict
axs_flat=axs.flat

i=0
for k,downsampled in downsampled_dict.items():
    ax=axs_flat[i]
    d_transform=downsampled.rio.transform()
    cmap_LC,norm=usda_geodataProcess.cmap4LC(LC_color_dict,list(np.unique(downsampled)))
    show(np.squeeze(downsampled.data),ax=ax,cmap=cmap_LC,transform=d_transform)
    new_grain_size=[round(i,3) for i in downsampled.rio.resolution()]
    ax.set_title(f'gs={new_grain_size[0]}')  
    i+=1
    if i==100:break

plt.tight_layout()  
plt.show()        
```


<img src="./imgs/3_8/output_104_0.png" height='auto' width='auto' title="caDesign">   


使用`signature2distance_integrating()`方法计算层级分解。


```python
downsampled_data_dict={v.rio.resolution()[0]:v.data[0] for k,v in downsampled_dict.items()}
pattern_distance_df=usda_signature.signature2distance_integrating(downsampled_data_dict,signatures_lst=['class_hierarchical_decomposition'])    
```

从下述结果可以发现固定5000m 幅度，在 116m 上下尺度时，有明显的距离跳变，例如 6.0尺度对109.0尺度的0.061，对116.0尺度0.437 的跳变。这一跳变表明，层级分解所表述的分类在各层级所占比例的频数在116m上下尺度时发生了结构上的变化。


```python
pattern_distance_resetIdx_df=pattern_distance_df.reset_index()
pattern_distance_resetIdx_df=pattern_distance_resetIdx_df.round({'level_0':0,'level_1':0,'class_hierarchical_decomposition':3})
pattern_distance_matrix=pattern_distance_resetIdx_df.pivot(index='level_0',columns='level_1',values='class_hierarchical_decomposition')
pattern_distance_matrix[[i for i in pattern_distance_matrix.columns if i>60 and i<160]].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>level_1</th>
      <th>67.0</th>
      <th>72.0</th>
      <th>78.0</th>
      <th>85.0</th>
      <th>91.0</th>
      <th>96.0</th>
      <th>102.0</th>
      <th>109.0</th>
      <th>116.0</th>
      <th>122.0</th>
      <th>128.0</th>
      <th>135.0</th>
      <th>139.0</th>
      <th>147.0</th>
      <th>152.0</th>
      <th>156.0</th>
    </tr>
    <tr>
      <th>level_0</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6.0</th>
      <td>0.049</td>
      <td>0.047</td>
      <td>0.048</td>
      <td>0.052</td>
      <td>0.049</td>
      <td>0.052</td>
      <td>0.049</td>
      <td>0.061</td>
      <td>0.437</td>
      <td>0.437</td>
      <td>0.063</td>
      <td>0.443</td>
      <td>0.438</td>
      <td>0.441</td>
      <td>0.051</td>
      <td>0.438</td>
    </tr>
    <tr>
      <th>12.0</th>
      <td>0.036</td>
      <td>0.036</td>
      <td>0.033</td>
      <td>0.038</td>
      <td>0.037</td>
      <td>0.043</td>
      <td>0.040</td>
      <td>0.053</td>
      <td>0.428</td>
      <td>0.429</td>
      <td>0.050</td>
      <td>0.430</td>
      <td>0.430</td>
      <td>0.433</td>
      <td>0.037</td>
      <td>0.431</td>
    </tr>
    <tr>
      <th>18.0</th>
      <td>0.025</td>
      <td>0.026</td>
      <td>0.023</td>
      <td>0.027</td>
      <td>0.028</td>
      <td>0.034</td>
      <td>0.032</td>
      <td>0.043</td>
      <td>0.413</td>
      <td>0.415</td>
      <td>0.039</td>
      <td>0.412</td>
      <td>0.416</td>
      <td>0.419</td>
      <td>0.028</td>
      <td>0.417</td>
    </tr>
    <tr>
      <th>24.0</th>
      <td>0.018</td>
      <td>0.020</td>
      <td>0.016</td>
      <td>0.020</td>
      <td>0.022</td>
      <td>0.027</td>
      <td>0.025</td>
      <td>0.036</td>
      <td>0.401</td>
      <td>0.402</td>
      <td>0.031</td>
      <td>0.398</td>
      <td>0.403</td>
      <td>0.407</td>
      <td>0.021</td>
      <td>0.404</td>
    </tr>
    <tr>
      <th>30.0</th>
      <td>0.011</td>
      <td>0.013</td>
      <td>0.011</td>
      <td>0.013</td>
      <td>0.016</td>
      <td>0.020</td>
      <td>0.019</td>
      <td>0.027</td>
      <td>0.388</td>
      <td>0.389</td>
      <td>0.022</td>
      <td>0.385</td>
      <td>0.391</td>
      <td>0.394</td>
      <td>0.016</td>
      <td>0.391</td>
    </tr>
  </tbody>
</table>
</div>




```python
f, ax = plt.subplots(figsize=(10, 9))
sns.heatmap(pattern_distance_matrix, annot=False, linewidths=.5, ax=ax,cmap='gist_stern')
ax.tick_params(axis='x', rotation=90)
plt.yticks(rotation=0) 
plt.show()
```


<img src="./imgs/3_8/output_109_0.png" height='auto' width='auto' title="caDesign">    



### 3.8.3.3 分类面积尺度变化曲线

计算所有尺度 LC 各类型的面积，查看面积随尺度变化的曲线，并计算曲线拐点，找到受尺度影响，类型面积的作用程度区间。从计算结果来看，水体和林地在约 72m 尺度下，面积变化开始趋缓；而灌木丛和不透水区域分别为最小和最大的60，及90m尺度面积变化趋于平缓。


```python
class_areas_dfs=[]
for grain,da in tqdm(downsampled_data_dict.items()):
    unique, counts = np.unique(da.data, return_counts=True)
    areas=np.array(counts)*grain
    df=pd.DataFrame(areas,index=unique,columns=[grain])
    class_areas_dfs.append(df)
    
class_areas_df=pd.concat(class_areas_dfs,axis=1).fillna(0)
class_areas_df.rename(index={1:'Water',3:'Tree Canopy',4:'Shrub',6:'Impervious'},inplace=True)
class_areas_df
```

    100%|███████████████████████████████████████████████| 52/52 [00:00<00:00, 1559.45it/s]
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>6.002161</th>
      <th>12.018750</th>
      <th>18.049819</th>
      <th>24.037500</th>
      <th>30.119277</th>
      <th>36.230435</th>
      <th>42.015126</th>
      <th>48.075000</th>
      <th>54.345652</th>
      <th>60.238554</th>
      <th>...</th>
      <th>357.128571</th>
      <th>384.600000</th>
      <th>416.650000</th>
      <th>454.527273</th>
      <th>499.980000</th>
      <th>555.533333</th>
      <th>624.975000</th>
      <th>714.257143</th>
      <th>833.300000</th>
      <th>999.960000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Water</th>
      <td>5.140251e+04</td>
      <td>2.096070e+04</td>
      <td>11172.838267</td>
      <td>7547.775</td>
      <td>5572.066265</td>
      <td>4528.804348</td>
      <td>3277.179832</td>
      <td>2836.425</td>
      <td>1684.715217</td>
      <td>1204.771084</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.00</td>
      <td>0.000000</td>
      <td>0.000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>Tree Canopy</th>
      <td>1.103935e+06</td>
      <td>5.583551e+05</td>
      <td>368432.915523</td>
      <td>268979.625</td>
      <td>208666.351807</td>
      <td>166841.152174</td>
      <td>139532.233613</td>
      <td>119995.200</td>
      <td>100322.073913</td>
      <td>89755.445783</td>
      <td>...</td>
      <td>9285.342857</td>
      <td>9615.0</td>
      <td>8333.0</td>
      <td>7726.963636</td>
      <td>7499.70</td>
      <td>4444.266667</td>
      <td>5624.775</td>
      <td>4285.542857</td>
      <td>3333.2</td>
      <td>3999.84</td>
    </tr>
    <tr>
      <th>Shrub</th>
      <td>7.905866e+05</td>
      <td>3.296142e+05</td>
      <td>189017.709747</td>
      <td>127591.050</td>
      <td>94574.530120</td>
      <td>74417.313043</td>
      <td>63022.689076</td>
      <td>51969.075</td>
      <td>48095.902174</td>
      <td>41383.886747</td>
      <td>...</td>
      <td>5356.928571</td>
      <td>5769.0</td>
      <td>4166.5</td>
      <td>5454.327273</td>
      <td>2999.88</td>
      <td>3333.200000</td>
      <td>1249.950</td>
      <td>1428.514286</td>
      <td>833.3</td>
      <td>999.96</td>
    </tr>
    <tr>
      <th>Impervious</th>
      <td>2.218909e+06</td>
      <td>1.170987e+06</td>
      <td>816321.136462</td>
      <td>635839.950</td>
      <td>521153.851807</td>
      <td>444185.130435</td>
      <td>389144.097479</td>
      <td>345178.500</td>
      <td>309878.908696</td>
      <td>282639.296386</td>
      <td>...</td>
      <td>55354.928571</td>
      <td>49613.4</td>
      <td>47498.1</td>
      <td>41816.509091</td>
      <td>39498.42</td>
      <td>37220.733333</td>
      <td>33123.675</td>
      <td>29284.542857</td>
      <td>25832.3</td>
      <td>19999.20</td>
    </tr>
  </tbody>
</table>
<p>4 rows × 52 columns</p>
</div>



使用[kneed](https://pypi.org/project/kneed/)<sup>⑭</sup>库实现曲线拐点的计算，其中参数`S`为可调节的灵敏度参数，测量曲线探测到拐点之前期望看到多少“平坦”点的度量，较小的值可以更快的检测到拐点；而较大的值则更保守。


```python
sns.set_theme(style="whitegrid")

colors=['red','blue','orange','green']
found_knee={}
for idx,row in class_areas_df.iterrows():
    kneedle=KneeLocator(class_areas_df.columns, row, S=10, curve='convex', direction='decreasing')
    found_knee[idx]=round(kneedle.knee, 3)
print(found_knee)

f, ax = plt.subplots(figsize=(20, 5))
sns.lineplot(data=class_areas_df.T, linewidth=2.5,ax=ax,palette=colors)
ax.set_xticks(class_areas_df.columns)
ax.tick_params(axis='x', rotation=90)

ax.vlines(x=list(found_knee.values()), ymin = 0.1, ymax =class_areas_df.to_numpy().max()*(2/3), color =colors,linewidth=3)

plt.tight_layout()
plt.show()
```

    {'Water': 72.461, 'Tree Canopy': 72.461, 'Shrub': 60.239, 'Impervious': 90.905}
    


<img src="./imgs/3_8/output_113_1.png" height='auto' width='auto' title="caDesign">   



* 数值数据

### 3.8.3.3 常规统计

观察常规统计量随尺度变化的数值变化，可以辅助判断所分析数据类型、内容受尺度影响变化趋势和关键转折点的信息。定义`comparisonOFdistribution()`函数计算返回主要的统计量，包括分析区域，不同粒度下栅格单元数（`n`）、最大（`max`）和最小（`min`）值、数值区间（`sampleRang`）、中位数（`median`）和平均数（`mean`），调和平均数（`harmonic_mean`）、标准差（`std`）和方差（`var`），不考虑空值下的标准差（`nanstd`）和方差（`nanvar`）、偏度（`skew`）和峰度（`kurtosis`）及信息熵（entropy=）等<sup>[Quattrochi, D., & Goodchild, M. (1997). Scale in Remote Sensing and GIS. Taylor & Francis.]82</sup>。

以库克郡区域的人口数量数据为分析对象，在降采样构建不同尺度时，使用`average`均值的方式。


```python
us_pop_fn=r'D:\datasets\worldpop\usa_ppp_2020_UNadj_constrained.tif'

left_bottom=[-87.840427, 41.966790]
left_bottom_prj=usda_geodataProcess.pt_coordi_transform(4326,LC.rio.crs,left_bottom)
right_top_prj=[i+10000 for i in left_bottom_prj]
right_top=usda_geodataProcess.pt_coordi_transform(LC.rio.crs,4326,right_top_prj)
pop_part,transform,ras_meta=usda_geodataProcess.rio_read_subset(us_pop_fn,[left_bottom,right_top]) 
pop_part[pop_part<0]=0
pop_cook_xda=usda_geodataProcess.array2dataarray(pop_part,transform,crs=4326) 
```

执行降采样，配置了40个连续变化的比例因子。


```python
scale_factors=[1/(1*i) for i in range(1,41,1)] 
downsampled_dict={}
for nrow,scale_factor in enumerate(scale_factors):
    downsampled=usda_geodataProcess.upNdownsampling(pop_cook_xda,scale_factor,resampling_method='average') 
    downsampled_dict[scale_factor]=downsampled
```

打印降采样后各个尺度下人口数量结果。


```python
plt.style.use('default')
fig, axs = plt.subplots(4, 10,figsize=(20,7))
axs_flat=axs.flat

i=0
for k,downsampled in downsampled_dict.items():
    ax=axs_flat[i]    
    d_transform=downsampled.rio.transform()
    show(np.squeeze(downsampled.data),ax=ax,cmap='hot',transform=d_transform)
    new_grain_size=[round(i,3) for i in downsampled.rio.resolution()]
    ax.set_title(f'gs={new_grain_size[0]}')  
    i+=1
    
plt.tight_layout()  
plt.show()   
```


<img src="./imgs/3_8/output_119_0.png" height='auto' width='auto' title="caDesign">    
    


计算各个尺度常规统计量


```python
stats4pop_df,exception_k=usda_stats.xdas_stats(downsampled_dict,exclulde=[0])
stats4pop_df.fillna(0,inplace=True)
stats4pop_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0.001</th>
      <th>0.002</th>
      <th>0.003</th>
      <th>0.004</th>
      <th>0.005</th>
      <th>0.006</th>
      <th>0.007</th>
      <th>0.009</th>
      <th>0.010</th>
      <th>0.011</th>
      <th>0.012</th>
      <th>0.013</th>
      <th>0.015</th>
      <th>0.017</th>
      <th>0.020</th>
      <th>0.024</th>
      <th>0.030</th>
      <th>0.040</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>n</th>
      <td>11420.000000</td>
      <td>1642.000000</td>
      <td>948.000000</td>
      <td>584.000000</td>
      <td>430.000000</td>
      <td>298.000000</td>
      <td>191.000000</td>
      <td>117.000000</td>
      <td>108.000000</td>
      <td>88.000000</td>
      <td>70.000000</td>
      <td>54.000000</td>
      <td>48.000000</td>
      <td>35.000000</td>
      <td>24.000000</td>
      <td>15.000000</td>
      <td>12.000000</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>242.971024</td>
      <td>106.291092</td>
      <td>97.578499</td>
      <td>83.294075</td>
      <td>78.565445</td>
      <td>77.462555</td>
      <td>67.056137</td>
      <td>49.099449</td>
      <td>46.989517</td>
      <td>41.403553</td>
      <td>35.273510</td>
      <td>31.256004</td>
      <td>29.722822</td>
      <td>26.796505</td>
      <td>27.915464</td>
      <td>20.643835</td>
      <td>21.022779</td>
      <td>18.719669</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.045336</td>
      <td>0.009883</td>
      <td>0.005559</td>
      <td>0.005961</td>
      <td>0.009110</td>
      <td>0.058633</td>
      <td>0.249622</td>
      <td>0.198972</td>
      <td>0.480419</td>
      <td>0.574475</td>
      <td>1.515746</td>
      <td>1.370304</td>
      <td>1.806680</td>
      <td>5.278272</td>
      <td>5.524227</td>
      <td>12.857833</td>
      <td>9.451230</td>
      <td>13.866064</td>
    </tr>
    <tr>
      <th>sampleRange</th>
      <td>242.925690</td>
      <td>106.281212</td>
      <td>97.572937</td>
      <td>83.288116</td>
      <td>78.556335</td>
      <td>77.403923</td>
      <td>66.806511</td>
      <td>48.900478</td>
      <td>46.509098</td>
      <td>40.829079</td>
      <td>33.757763</td>
      <td>29.885700</td>
      <td>27.916142</td>
      <td>21.518234</td>
      <td>22.391237</td>
      <td>7.786002</td>
      <td>11.571549</td>
      <td>4.853605</td>
    </tr>
    <tr>
      <th>median</th>
      <td>14.793374</td>
      <td>14.911302</td>
      <td>14.784887</td>
      <td>14.778898</td>
      <td>14.770516</td>
      <td>14.653145</td>
      <td>15.024829</td>
      <td>14.967964</td>
      <td>14.804311</td>
      <td>15.392523</td>
      <td>14.588770</td>
      <td>15.947055</td>
      <td>15.489057</td>
      <td>15.187284</td>
      <td>14.777534</td>
      <td>15.475512</td>
      <td>14.451845</td>
      <td>16.657173</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>18.463545</td>
      <td>16.220766</td>
      <td>15.803698</td>
      <td>15.519054</td>
      <td>15.485174</td>
      <td>15.516930</td>
      <td>15.494181</td>
      <td>15.413485</td>
      <td>15.413484</td>
      <td>15.413483</td>
      <td>15.413484</td>
      <td>15.413483</td>
      <td>15.413483</td>
      <td>15.413482</td>
      <td>15.413483</td>
      <td>16.124006</td>
      <td>15.413483</td>
      <td>16.657173</td>
    </tr>
    <tr>
      <th>harmonic_mean</th>
      <td>3.388648</td>
      <td>1.172556</td>
      <td>1.211745</td>
      <td>0.904078</td>
      <td>1.350724</td>
      <td>4.227163</td>
      <td>6.854211</td>
      <td>6.342031</td>
      <td>8.904241</td>
      <td>8.420422</td>
      <td>11.565533</td>
      <td>11.129253</td>
      <td>10.928900</td>
      <td>13.128486</td>
      <td>13.198285</td>
      <td>15.763003</td>
      <td>14.329232</td>
      <td>16.497907</td>
    </tr>
    <tr>
      <th>std</th>
      <td>17.592808</td>
      <td>12.151770</td>
      <td>11.250584</td>
      <td>10.538179</td>
      <td>10.243578</td>
      <td>9.487906</td>
      <td>8.938852</td>
      <td>7.869045</td>
      <td>7.829081</td>
      <td>7.517589</td>
      <td>6.736104</td>
      <td>6.232287</td>
      <td>6.126615</td>
      <td>5.673265</td>
      <td>5.421353</td>
      <td>2.467206</td>
      <td>3.955573</td>
      <td>1.587826</td>
    </tr>
    <tr>
      <th>var</th>
      <td>309.506897</td>
      <td>147.665512</td>
      <td>126.575638</td>
      <td>111.053230</td>
      <td>104.930885</td>
      <td>90.020348</td>
      <td>79.903084</td>
      <td>61.921867</td>
      <td>61.294506</td>
      <td>56.514137</td>
      <td>45.375095</td>
      <td>38.841400</td>
      <td>37.535412</td>
      <td>32.185940</td>
      <td>29.391069</td>
      <td>6.087108</td>
      <td>15.646558</td>
      <td>2.521192</td>
    </tr>
    <tr>
      <th>nanstd</th>
      <td>17.592808</td>
      <td>12.151770</td>
      <td>11.250584</td>
      <td>10.538179</td>
      <td>10.243578</td>
      <td>9.487906</td>
      <td>8.938852</td>
      <td>7.869045</td>
      <td>7.829081</td>
      <td>7.517589</td>
      <td>6.736104</td>
      <td>6.232287</td>
      <td>6.126615</td>
      <td>5.673265</td>
      <td>5.421353</td>
      <td>2.467206</td>
      <td>3.955573</td>
      <td>1.587826</td>
    </tr>
    <tr>
      <th>nanvar</th>
      <td>309.506897</td>
      <td>147.665512</td>
      <td>126.575638</td>
      <td>111.053230</td>
      <td>104.930885</td>
      <td>90.020348</td>
      <td>79.903084</td>
      <td>61.921867</td>
      <td>61.294506</td>
      <td>56.514137</td>
      <td>45.375095</td>
      <td>38.841400</td>
      <td>37.535412</td>
      <td>32.185940</td>
      <td>29.391069</td>
      <td>6.087108</td>
      <td>15.646558</td>
      <td>2.521192</td>
    </tr>
    <tr>
      <th>skew</th>
      <td>3.756055</td>
      <td>1.927418</td>
      <td>1.935872</td>
      <td>1.644230</td>
      <td>1.552715</td>
      <td>1.557257</td>
      <td>1.501385</td>
      <td>0.857266</td>
      <td>0.948056</td>
      <td>0.696420</td>
      <td>0.536776</td>
      <td>0.172671</td>
      <td>0.037189</td>
      <td>0.232792</td>
      <td>0.144560</td>
      <td>0.382140</td>
      <td>-0.079139</td>
      <td>-0.419828</td>
    </tr>
    <tr>
      <th>kurtosis</th>
      <td>24.453927</td>
      <td>7.416304</td>
      <td>8.234899</td>
      <td>5.966865</td>
      <td>5.689920</td>
      <td>6.421036</td>
      <td>5.835428</td>
      <td>2.016344</td>
      <td>1.989713</td>
      <td>1.160067</td>
      <td>0.083617</td>
      <td>0.017325</td>
      <td>0.012839</td>
      <td>-0.772670</td>
      <td>-0.458545</td>
      <td>-1.295912</td>
      <td>-1.383809</td>
      <td>-0.793735</td>
    </tr>
    <tr>
      <th>entropy</th>
      <td>9.012622</td>
      <td>7.138398</td>
      <td>6.612722</td>
      <td>6.143163</td>
      <td>5.844867</td>
      <td>5.512627</td>
      <td>5.089198</td>
      <td>4.627291</td>
      <td>4.552494</td>
      <td>4.352790</td>
      <td>4.151361</td>
      <td>3.900678</td>
      <td>3.783741</td>
      <td>3.486048</td>
      <td>3.113834</td>
      <td>2.696498</td>
      <td>2.451117</td>
      <td>1.787139</td>
    </tr>
  </tbody>
</table>
</div>



从打印各尺度常规统计量的结果中可以得知，各尺度下均值和中位数基本保持水平；调和平均数和最小值呈上升趋势；其它各值均快速下降后转平缓下降趋势。从图中也可以观察到在 0.002度，约 222m 的尺度上，存在一个较陡的下降。从数据平稳性来看，均值和方差不随尺度而变化是较理想的状态，但试验结果表明方差值持续下降，数据波动越来越小，数值间的差异减小。


```python
def f(s):
    return s/s.max()

sns.set_theme(style="whitegrid")
fig,ax=plt.subplots(figsize=(20,4))
sns.lineplot(data=stats4pop_df.T.apply(f,axis=0),ax=ax)
plt.legend(loc='upper right')
ax.set_xticks(stats4pop_df.columns)
ax.tick_params(axis='x', rotation=90)
plt.show()
```


<img src="./imgs/3_8/output_123_0.png" height='auto' width='auto' title="caDesign">    


单位度到米和米到度的转换。


```python
deg2meter=lambda degree:degree*(2 * math.pi * 6378137.0)/ 360
meter2deg=lambda meter:meter/(2 * math.pi * 6378137.0) * 360

deg2meter(0.002)
```




    222.63898158654717



### 3.8.3.4 半变异（方差）函数（semi-variograms）

大部分空间分布的数据，例如高程、地表温度、污染气体浓度、城市噪音分布、人口数量/密度分布、气象数据，NDVI、NDWI、NDBI等景观指数都在空间上具有关联，且近距离的对象的属性较之远距离的对象属性更相关，例如某一位置及其临近位置的高程值往往是连续的，但较远地点的高程值与观测位置往往相对独立。为了检验数据的空间自相关，在*空间自相关分析*部分解释了莫兰指数（Moran’s I），Geary’s C 和 Getis and Ord’s G 指数，而在1963年， Matheron Georges <sup>[6]</sup>提出的半变异函数用于分析区域化变量空间分布的差异和空间数据的插值，通过统计基于样本点两两距离大小划分的不同距离区段内（`lag`）两两样本点间属性差值平方的均值（`semivariance`），拟合出一条变化曲线，以`lag`（步长）为横坐标，以`semivariance`（用$\gamma(h)$表示，半变异函数值）为纵坐标，得到公式$\gamma(h)=\frac{1}{2 V} \iiint_V[f(M+h)-f(M)]^2 d V$，式中，$M$为几何场$V$内的一个（样本）点，三重积分表示为三维空间，$h$为感兴趣的分离距离（对应下述代码中`maxlag`参数），$f(M)$和$f(M+h)$则为样本点的属性值。

实现半变异函数计算的 Python 库有[SciKit GStat](https://scikit-gstat.readthedocs.io/en/latest/index.html)<sup>⑮</sup>和[Pyinterpolate](https://pyinterpolate.readthedocs.io/en/latest/index.html)<sup>⑯</sup>等，试验中使用`SciKit GStat`库计算。

* 基于简单样本数据的半变异函数解释

生成一个简单的随机样本数据，包括点的二维坐标值`x`和 `Y`，及一个属性值`v`。


```python
%load_ext autoreload 
%autoreload 2
from usda import network as usda_network

import skgstat as skg
import networkx as nx
import itertools
import math
import matplotlib.colors as mcolors
```

    The autoreload extension is already loaded. To reload it, use:
      %reload_ext autoreload
    


```python
data=skg.data.pancake(N=30, seed=42, as_dataframe=True).get('sample')
data.tail(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
      <th>v</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>27</th>
      <td>125</td>
      <td>327</td>
      <td>213</td>
    </tr>
    <tr>
      <th>28</th>
      <td>484</td>
      <td>42</td>
      <td>190</td>
    </tr>
    <tr>
      <th>29</th>
      <td>272</td>
      <td>380</td>
      <td>171</td>
    </tr>
  </tbody>
</table>
</div>



用空间样本点数据构建图$G$，打印的图反应了3部分内容，1是，根据坐标位置打印样本点的实际位置，可以观察样本点（顶点）的空间分布情况；2是，样本点的属性值`v`用颜色反应值的大小，可以观察到空间距离较近的样本通常具有相近的颜色值，如果距离较远，则样本点的值差异较大，基本相互独立；3是，建立样本点间两两连线的边，并按照距离将其划分为`[0,100,200,300,400,500,+inf]`为分隔点的6段区间，不同距离区间（`bins`）赋予不同的颜色标识，例如其中橘红色的边为`(0，100]`的区间，通过距离的区间划分可以清晰的观察到所要计算两两样本点间属性差值平方均值的不同区段。


```python
nodes=data.index.values.tolist()
edges=list(itertools.permutations(nodes,2))
nodes_v={node:{'v':data.iloc[node].v} for node in nodes}

G=nx.Graph()
G.add_edges_from(edges)
nx.set_node_attributes(G, nodes_v)
pos=data[['x','y']].values.tolist()

edge_distance_func=lambda edge:math.dist(pos[edge[0]],pos[edge[1]])
def edge_distance_classify_func(edge):
    x=edge_distance_func(edge)
    
    if x>0 and x<=100:return {'distance':x,'classification':0}
    elif x>100 and x<=200:return {'distance':x,'classification':1}
    elif x>200 and x<=300:return {'distance':x,'classification':2}
    elif x>300 and x<=400:return {'distance':x,'classification':3}
    elif x>400 and x<=500:return {'distance':x,'classification':4}
    else:
        return {'distance':x,'classification':5}

edges_distance={edge:edge_distance_classify_func(edge) for edge in edges}
nx.set_edge_attributes(G,edges_distance)

colors=['orangered','deepskyblue','lightseagreen','forestgreen','silver','dimgray']
edge_classification=nx.get_edge_attributes(G,'classification')
edgecolors=[colors[i] for i in edge_classification.values()]
edge_thickness=[6-i for i in list(edge_classification.values())]
usda_network.G_drawing(G,pos=pos,node_size=1500,linewidths=1,figsize=(15,15),node_color=data.v,edge_colors=edgecolors,routes=G.edges,edge_widths=edge_thickness,width=0)
```


<img src="./imgs/3_8/output_130_0.png" height='auto' width='auto' title="caDesign">    
    


以图 $G$ 表述的6个距离区段为横坐标（由`V._bins`方法赋值），表示为垂直的红虚线分隔；以各个距离区段按`matheron`方法计算半变异函数值为纵坐标，用`SciKit GStat`库计算结果如下。


```python
V=skg.Variogram(data[['x', 'y']].values, data.v.values, normalize=False, n_lags=6, use_nugget=True,estimator='matheron',model='spherical',dist_func='euclidean', maxlag=None)
V._bins=[100,200,300,400,500,600]
```

通过打印距离滞后（距离区段）和半变异函数值的散点图，可以粗略观察到样本点成对距离数量的区间分布情况，并可以从打印的直方图中更直观的观察到分布数量的关系。


```python
fig, ax=plt.subplots(figsize=(20, 3))
V.distance_difference_plot(ax=ax);
```


<img src="./imgs/3_8/output_134_0.png" height='auto' width='auto' title="caDesign">    

    


由`spherical`模型拟合的曲线如下，并可以观察到样本点间的距离越远，半变异函数值越大，表明样本点间属性的相关性小；而距离较近时，半变异函数值越小，相关性越大。当距离为0时，理论上半变异函数值为0，但由于测量误差的存在，这个值通常不为0，称为**块金效用（$C_0$）**。


```python
V.plot(show=False);
```


<img src="./imgs/3_8/output_136_0.png" height='auto' width='auto' title="caDesign">    



`maxlag`参数指定最大滞后距离。如果配置的值在(0,1)之间，则最大滞后距离为`maxlag * max(Variogram.distance)`，即距离的最大值乘以位于(0,1)区间的`maxlag`值；如果配置的值为字符窜`median`或者`mean`，则分别为所有样本点两两距离集的中位数和均值。下述分布配置了`maxlag`参数值为0.1，及`mean`和`median`，可以观察到最大滞后距离分别约为59、251和245。


```python
distances=np.array([v['distance'] for v in edges_distance.values()])
print(f'median={np.median(distances)};\nmean={np.mean(distances)};\n0.1={max(distances)*0.1}')

fig, axes_=plt.subplots(1, 3, figsize=(20, 5), sharey=False)
axes=axes_.flatten()

for ax, maxlag in zip(axes, (0.1,'mean','median')):
    V.maxlag=maxlag
    V.plot(axes=ax, hist=True, show=False)
    ax.set_title(maxlag)
plt.tight_layout()
plt.show()
```

    median=245.08161905781512;
    mean=251.37584689032064;
    0.1=59.383667788374275
    


<img src="./imgs/3_8/output_138_1.png" height='auto' width='auto' title="caDesign">    



拟合曲线前，需要计算每一滞后距离内的半变异函数值，除了`matheron`<sup>[7,8]</sup>方法（公式为$\gamma (h) = \frac{1}{2N(h)} * \sum_{i=1}^{N(h)}(x)^2$，式中，$x = Z(x_i) - Z(x_{i+h})$，为每一滞后距离内，整个数组的半方差值）外，还包括`cressie`<sup>[9]</sup>，公式为$ 2\gamma (h) = \frac{(\frac{1}{N(h)} \sum_{i=1}^{N(h)} |x|^{0.5})^4}{0.457 + \frac{0.494}{N(h)} + \frac{0.045}{N^2(h)}}$；`dowd`<sup>[10]</sup>，公式为$2\gamma (h) = 2.198 * {median(x)}^2$；`genton`<sup>[11]</sup>，公式为$Q_{N_h} = 2.2191\{|V_i(h) - V_j(h)|; i < j\}_{(k)}$，式中，$k = \binom{[N_h / 2] + 1}{2}$，$q = \binom{N_h}{2}$。其中，$k$是$q$个点对的第$k$个分位数。对于较大的$N$， $(k/q)$值接近0.25。如果$N >= 500$，$k/q$与0.25相差两位小数，则配置为0.5，并且不计算两个二项式系数$k$和$q$；及`minmax`，`percentile`和`entropy`（为 Shannon entropy）等，具体分别计算结果如下<sup>[12]</sup>。


```python
fig, axes_=plt.subplots(2, 3, figsize=(20, 5), sharey=False)
axes=axes_.flatten()

V.maxlag='median'
for ax, estimator in zip(axes, ('matheron', 'cressie', 'dowd','genton','minmax','entropy')):
    V.estimator=estimator
    V.plot(axes=ax, hist=True, show=False)
    ax.set_title(estimator)
plt.tight_layout()
plt.show()
```


<img src="./imgs/3_8/output_140_0.png" height='auto' width='auto' title="caDesign">   



半变异函数一般用变异曲线来表示，将半变异函数值拟合为曲线，其中关键的参数如下图<sup>[13]</sup>，

<img src="./imgs/3_8/3_8_06.png" height='auto' width=400 title="caDesign">

图中曲线表示为基于离散的半方差值（semi-variance），即经验半变异函数（empirical variogram）值拟合的半变异函数，即理论半变异函数（theoretical variogram）。因为距离近的样本点较之距离远的更具有相似性，因此半变异函数通常是随滞后距离（`Lag Distance（h）`）增加的递增曲线。半变异函数由3个主要的参数组成，由于测量误差影响的块金（效应）（nugget）表示为$C_1$；随着滞后距离的增加，递增曲线逐渐趋于平稳，当变异曲线首次呈现水平状态时的距离称为变程（`Range`），用$\alpha$表示；半变异函数在变程处取得的函数值为基台（`Sill`），为$C_0+C_1$，其中$C_0$为偏基台值（`Structural variance`）。


前文计算了各个滞后距离的半变异函数值，则可以依据半变异函数值拟合曲线，上述拟合的曲线均使用了`spherical`<sup>[14]</sup>方法，公式为$\gamma = b + C_0 * \left({1.5*\frac{h}{a} - 0.5*\frac{h}{a}^3}\right), h<r$，如果$h >= r$，则$ \gamma = b + C_0$，式中，$h$为滞后距离（`Lag Distance`），计入为因变量的分离距离；$r$为有效距离（`Range`），$a = r$；$C_0$为偏基台值；$b$为块金，即$C_1$；`exponential`<sup>[15,16,17]</sup>模型，公式为$\gamma = b + C_0 * \left({1 - e^{-\frac{h}{a}}}\right)$，式中，$a = \frac{r}{3}$；`gaussian`<sup>[16,17]</sup>模型，公式为$\gamma = b + c_0 * \left({1 - e^{-\frac{h^2}{a^2}}}\right)$，式中，$a = \frac{r}{2}$；`cubic`<sup>[18]</sup>模型，公式为$\gamma = b + C_0 *  \left[{7 * \left(\frac{h^2}{a^2}\right) - \frac{35}{4} * \left(\frac{h^3}{a^3}\right) + \frac{7}{2} * \left(\frac{h^5}{a^5}\right) - \frac{3}{4} * \left(\frac{h^7}{a^7}\right)}\right]$，式中，$a = r$；`stable`<sup>[18]</sup>模型，公式为$\gamma = b + C_0 * \left({1. - e^{- {\frac{h}{a}}^s}}\right)$，式中，$a = \frac{r}{3^{\frac{1}{s}}}$；`matern`<sup>[19]</sup>模型，公式为$\gamma (h) = b + C_0 \left( 1 - \frac{1}{2^{\upsilon - 1} \Gamma(\upsilon)}\left(\frac{h}{a}\right)^\upsilon K_\upsilon \left(\frac{h}{a}\right)\right)$，式中$a = \frac{r}{2}$<sup>[20]</sup>。

下述试验固定了半变异函数值的计算方法为`matheron`，应用不同拟合曲线的模型计算结果如下。


```python
fig, axes_=plt.subplots(2, 3, figsize=(20, 5), sharey=False)
axes=axes_.flatten()

V.estimator='matheron'
for ax, model in zip(axes, ('spherical', 'exponential', 'gaussian','cubic','stable','matern')):
    V.model=model 
    V.plot(axes=ax, hist=True, show=False)
    ax.set_title(model)
plt.tight_layout()
plt.show()
```


<img src="./imgs/3_8/output_142_0.png" height='auto' width='auto' title="caDesign">    


* 用半变异函数寻找尺度效应的幅度变化关系

因为半变异函数曲线反应了空间数据随滞后距离增加空间自相关变化，因此可以探测幅度变化下的尺度效应，估计曲线变化的拐点，用于确定分析样方大小等应用。

继续使用人口数量分布数据，范围大小为$10000 \times 10000m$。为了满足`SciKit GStat`库的数据输入要求，将栅格数据矢量化为点数据，并用 DataFrame 格式存储，字段`x`和`y`为栅格单元中心点经纬度坐标，字段`v`为人口数量数据。


```python
xv, yv = np.meshgrid(pop_cook_xda.x.data,pop_cook_xda.y.data)
lonlat=list(zip(*[xv.flatten(),yv.flatten(),pop_cook_xda.data.flatten()]))
xy_df=pd.DataFrame(lonlat,columns=['x','y','v'])
```


```python
xy_df.tail(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
      <th>v</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>15694</th>
      <td>-87.722916</td>
      <td>41.967917</td>
      <td>97.808548</td>
    </tr>
    <tr>
      <th>15695</th>
      <td>-87.722083</td>
      <td>41.967917</td>
      <td>100.478157</td>
    </tr>
  </tbody>
</table>
</div>



打印转化后的人口数量分布数据，确定转化正确。


```python
def plot_scatter(data, ax,dot_size=10):
    art = ax.scatter(data.x, data.y, s=dot_size, c=data.v, cmap='plasma')
    plt.colorbar(art, ax=ax)

fig, ax=plt.subplots(figsize=(18, 10))
plot_scatter(xy_df, ax,dot_size=20)
```


<img src="./imgs/3_8/output_147_0.png" height='auto' width='auto' title="caDesign">    


固定拟合曲线模型为`spherical`，打印`matheron`、`cressie`和`dowd`等3种半变异函数值的计算方法如下，可以发现`cressie`和`dowd`方法的变程均约为 0.025度（约为2782m）；`matheron`方法约在 0.045度（约为 5000m）。


```python
vario=skg.Variogram(xy_df[['x', 'y']].values, xy_df.v.values,use_nugget=True, normalize=False,n_lags=10,maxlag='median',model='spherical')

fig, axes_=plt.subplots(1, 3, figsize=(20, 3), sharey=False)
axes=axes_.flatten()

for ax, estimator_name in zip(axes, ('matheron', 'cressie', 'dowd')):
    vario.estimator=estimator_name
    vario.plot(axes=ax, hist=False, show=False)
    ax.set_title(estimator_name.capitalize())

plt.show()
```


<img src="./imgs/3_8/output_149_0.png" height='auto' width='auto' title="caDesign">    



```python
print(f'0.025 deg={deg2meter(0.025)}m; 0.045 deg={deg2meter(0.045)}m')
```

    0.025 deg=2782.9872698318395m; 0.045 deg=5009.377085697311m
    

同样试验分类数据，以土地覆盖数据为例。同时为了减小计算量，提取部分区域 LC 数据，并降采样至栅格单元大小约为 24m。


```python
LC_clipped= LC.rio.clip_box(
    minx=LC.x.values[50000],
    miny=LC.y.values[-50000],
    maxx=LC.x.values[50000]+5000,
    maxy=LC.y.values[-50000]+5000,
    crs=LC.rio.crs,
    )

LC_clipped_downsampled=usda_geodataProcess.upNdownsampling(LC_clipped,1/40)  
print(LC_clipped_downsampled.rio.resolution())
LC_clipped_downsampled.plot();
```

    (24.040384615383186, -24.040384615383186)
    


<img src="./imgs/3_8/output_152_1.png" height='auto' width='auto' title="caDesign">    



将栅格数据转化为 DataFrame格式的点数据。


```python
xv, yv = np.meshgrid(LC_clipped_downsampled.x.data,LC_clipped_downsampled.y.data)
lonlat=list(zip(*[xv.flatten(),yv.flatten(),LC_clipped_downsampled.data.flatten()]))
xy_df=pd.DataFrame(lonlat,columns=['x','y','v'])
```

半变异函数值计算采用`matheron`方法，曲线拟合模型为`spherical`，从计算结果可以得知对于所分析区域的变程约为 800m，即对于一个空间位置点，邻近 800m 范围的其它用地类型与分析位置点具有空间自相关性。除了`SciKit GStat`库提供的6类半变异函数值计算方法，可以根据分析目的自定义计算各滞后距离内样本属性值之间关系的函数。


```python
vario=skg.Variogram(xy_df[['x', 'y']].values, xy_df.v.values,use_nugget=True, normalize=False,n_lags=10,maxlag='median',model='spherical',estimator='matheron')
vario.plot(show=False);
```


<img src="./imgs/3_8/output_156_0.png" height='auto' width='auto' title="caDesign">    



## 3.8.4 空间相互作用模型（Spatial Interaction Modelling，SIM）

空间相互作用（Spatial Interaction）可以广义的定义为决策过程导致空间上的（物质）移动或（信息）交流。这一术语囊括了各类空间现象和行为，例如移民、购物、城市交通（通勤）、货物流动、航空客运流，及任何可以转化为供需关系的现象，如城市开发空间（绿地）供给服务和城市居民对绿地需求的关系等。移动或流动反应了空间位置距离信息，其空间相互作用的前提是空间行为发生和移动或流动的程度，即空间位置点或区域之间（一般表达为源/起点（Origin，O）和汇/终点（Destination，D））分析对象的相互作用与空间几何距离、时间或成本相关。时间距离成本可以理解为空间尺度的一种表现。

构建模型（数学建模）的目的通常为解释和预测，对于空间相互作用模型，解释为通过模型校准（model calibration，即模型参数估计）研究空间相互作用如何随空间变化。在这种模式下，存在三种不同类型的解释性空间相互作用模型，1. 通过提供相互作用的源和汇属性信息来洞察相互作用模式的模型，称为无约束模型（unconstrained model，UCM）。可以回答诸如为什么有些源汇之间的流量（flows）很大，而有些源汇之间流量会很小，是源或汇的什么属性信息导致了流量的变化等；2. 如果仅提供汇的属性信息称为生产约束模型（Production-constrained model，PCM）。可以回答诸如汇的哪些属性信息更吸引源的流入，将源的流出量分配到汇；3. 如果仅提供源的属性信息称为吸引力约束模型（Attraction-constrained model，ACM），可以回答诸如源的哪些属性信息造成源的大量流出，将汇的流入量分配给源。对模型目的中的预测，即不提供汇的属性信息，也不提供源的属性信息，仅是寻求分配已知源的流出和汇的流入数量到汇源的连接边上，并不提供关于什么属性信息使得源的需求增加（例如人口）或者汇的供给能力得以提升（例如开发空间）等信息，该类模型为 SIM 族（“family”）的第4个模型，称为双重约束（Doubly-constrained model，DCM）或生产-吸引力约束模型（Production-attraction-constrained model）。可以回答诸如已知源的流出和汇的流入，汇源之间的流动可能模式等。

> 为了避免空间相互作用模型族（SIM “family”）中各模型中文翻译多样化带来的混淆，表述中一律用英文缩写替代。

### 3.8.4.1 SIM 的基本要素<sup>[21]</sup>

如果源有$m$个，汇有$n$个，则源汇间的的流量数据为一个$m \times n$的矩阵，可表示为：

$\begin{aligned} & T=\left[\begin{array}{cccc} T_{11} & T_{12} & \ldots & T_{1 n} \\ T_{21} & T_{22} & \ldots & T_{2 n} \\ \vdots & \vdots & & \vdots \\ T_{m 1} & T_{m 2} & \ldots & T_{m n} \end{array}\right] \begin{array}{c} O_1, \\ O_2 \\ \vdots \\ O_m \end{array} \\ & \begin{array}{lllll} &&& \ D_1 & D_2 & \cdots & D_n & T \end{array} \\ &\ \end{aligned}$

式中，$O_i$为各个源的流出数；$D_j$为各个汇的流入数，其流出和流入总数可分别表示为和$\mathrm{T}=\sum_{\mathrm{i}} \mathrm{O}_{\mathrm{i}}=\sum_{\mathrm{i}} \sum_{\mathrm{j}} \mathrm{T}_{\mathrm{ij}}$和$\mathrm{T}=\sum_j \mathrm{D}_{\mathrm{j}}=\sum_j \sum_{\mathrm{i}} \mathrm{T}_{\mathrm{ij}}$。

源汇之间的（距离、时间）成本矩阵（$m \times n$），可表示为：

$C=\left[\begin{array}{cccc} c_{11} & c_{12} & \cdots & c_{1 n} \\ c_{21} & c_{22} & \cdots & c_{2 n} \\ \vdots & \vdots & & \vdots \\ c_{m 1} & c_{m 2} & \cdots & c_{m n} \end{array}\right] $

源的属性信息为一个$m \times p$ 的矩阵，可表示为：

$V=\left[\begin{array}{cccc} v_1{ }^1 & v_1{ }^2 & \ldots & v_1{ }^p \\ v_2{ }^1 & v_2{ }^2 & \ldots & v_2{ }^p \\ \vdots & \vdots & & \vdots \\ v_m{ }^1 & v_m{ }^2 & \ldots & v_m{ }^p \end{array}\right]$

汇的属性信息为一个$n \times q$ 的矩阵，可表示为：

$ W=\left[\begin{array}{cccc} w_1{ }^1 & w_2{ }^1 & \ldots & w_n{ }^1 \\ w_1{ }^2 & w_2{ }^2 & \ldots & w_n{ }^2 \\ \vdots & \vdots & & \\ w_1{ }^q & w_2{ }^q & \ldots & w_n{ }^q \end{array}\right]$

SIM 模型的本质是将源汇流量矩阵$T$的数量对应到源汇的属性信息$V，W$和成本矩阵$C$上，即以$T$为因变量，以$V，W$或者为源汇流出和流入量$O,D$，和$C$为自变量。在 PCM 中，源的属性信息$V$被$m \times 1$大小的$O$替代，即矩阵$T$各行的流出总数；类似的，在 ACM 中，汇的属性信息$W$被$1 \times N$大小的$D$替代，即矩阵$T$各列的流入总数；在 DCM 中，则$V、W$被$O,D$分别替代，在模型校准中，可获得的唯一解释性信息为源汇的距离时间成本在确定相互作用模式中的作用。

$V,W$源汇属性信息根据分析的对象发生改变，例如在开放空间供给的问题上，如果以开放空间为汇，则属性信息可以选择为开发空间的规模面积、设施的数量、景观质量等信息；以社区为源，则属性信息可以选择为人口数量/密度、人口结构（如老人、小孩的比例）等信息。

对于$C$，通常为源汇间的距离、或者时间成本，例如通勤时间。如果是动物的觅食、迁徙等活动，还会受到土地覆盖类型的影响，例如人为干扰严重的道路、建成区等。如果已知源汇坐标点，则距离公式可表示为$d_{i j}=k\left[\left|x_i-x_j\right|^p+\left|y_i-y_j\right|^p\right]^{1 / p}$，式中，$(x_i,y_i)$和$(x_j,y_j)$为源汇的坐标值，$k$为比例转化因子，通过$p$的校准可以使实际距离值与建模值间具有良好的对应关系，其取值范围在[1,2]之间，如果取值为1则为矩形距离，如果为2则为两点间的直线距离，大于1而小于2的值的距离结果则位于矩形距离和直线距离之间。

距离计算通常为两点间的距离，但是如何源汇数据为区域，例如行政区划或者不同流域，一种解决的方法是使用*空间权重*部分阐述的 Rook、Queen或Bishop等方式，通过比邻性建立距离关系；另一种是取区域几何质心，计算质心间的距离；对于取质心后计算距离的方法，如果假设源为区域，汇为点，如果点位于区域外，计算点到质心的距离是合理的，但是如果点位于区域内质心处，那么距离可能为0，因此，Fotheringham（1988）<sup>[22]</sup>提出了推导近似圆形区域内矩形距离的公式为，$\mathrm{d}=0.846(1.693)^{\mathrm{z} / \mathrm{r}} \cdot \mathrm{r}$。

式中，$\mathrm{z}$为点到质心的距离，$\mathrm{r}$为一个圆的半径，其圆的面积等于区域的面积。当点位于区域之内时，$\mathrm{z}=0$，有$d=0.846r$；如果点位于等价区域的圆周上时，$\mathrm{z}=\mathrm{r}$，则$d=1.432r$；如果点位于质心到圆周间，则$0.846r<d<1.432$。同样，Fotheringham 给出了近似矩形区域的公式为，$ \mathrm{d}=(1 / 4+\mathrm{z} / 4 \mathrm{q})(\mathrm{a}+\mathrm{b})$，式中，$\mathrm{q}=\left[(\mathrm{a} / 2)^2+(\mathrm{b} / 2)^2\right]$，$a$和$b$表示一个矩形的长和宽。

以芝加哥城开放空间提供的服务潜力和其临近居民需求的空间相互作用关系为例，首先根据 SIM 的数据要求，预处理数据并构建源汇的流量$T$，属性信息$V,W$，和成本$C$。芝加哥城开发空间（绿地公园）和人口分布数据均来自于[Chicago Data Portal，CDP](https://data.cityofchicago.org/)<sup>⑰</sup>。


```python
%load_ext autoreload 
%autoreload 2
import usda.sim as usda_sim
import usda.utils as usda_utils

import pandas as pd
import geopandas as gpd

from spint.gravity import  BaseGravity, Gravity, Production, Attraction, Doubly
from spint.dispersion import phi_disp
from spint.vec_SA import VecMoran
import pysal as ps
import libpysal.weights as LW
import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial import distance
import math

import statsmodels.api as sm
from patsy import dmatrices
from scipy.stats import poisson
import scipy.stats
```

    The autoreload extension is already loaded. To reload it, use:
      %reload_ext autoreload
    

读取查看人口分布数据。


```python
chicago_population_counts_fn=r'D:\datasets\Chicago_Population_Counts.csv' 
pop_counts_zip=pd.read_csv(chicago_population_counts_fn)
pop_counts_zip.loc[0]
```




    Geography Type                                     Citywide
    Year                                                   2018
    Geography                                           Chicago
    Population - Total                                  2705988
    Population - Age 0-17                                548999
    Population - Age 18-29                               552935
    Population - Age 30-39                               456321
    Population - Age 40-49                               336457
    Population - Age 50-59                               312965
    Population - Age 60-69                               262991
    Population - Age 70-79                               155334
    Population - Age 80+                                  79986
    Population - Age 0-4                                 168114
    Population - Age 5-11                              204152.0
    Population - Age 12-17                             175475.0
    Population - Age 5+                                 2537874
    Population - Age 18+                                2156989
    Population - Age 65+                                 349712
    Population - Female                                 1386113
    Population - Male                                   1319875
    Population - Latinx                                  776661
    Population - Asian Non-Latinx                      179841.0
    Population - Black Non-Latinx                      784266.0
    Population - White Non-Latinx                        899980
    Population - Other Race Non-Latinx                 119467.0
    Record ID                             Citywide-Chicago-2018
    Name: 0, dtype: object



将人口数据按照字段`"Geography`给出的邮政区号连接对应的空间区域几何对象，并配置投影。


```python
boundaries_ZIPcodes_fn=r'D:\datasets\Boundaries - ZIP Codes.geojson'
zip_boundaries=gpd.read_file(boundaries_ZIPcodes_fn)
zip_pop_gdf=pd.merge(zip_boundaries,pop_counts_zip,left_on="zip", right_on="Geography")

crs=26916
zip_pop_gdf.to_crs(crs,inplace=True)
zip_pop_gdf.plot(column='Population - Total',cmap='gray',legend=True,figsize=(8,8));
```


<img src="./imgs/3_8/output_163_0.png" height='auto' width='auto' title="caDesign">    



读取城市公园数据，并调整列的数据类型，配置设施数多列为整数型；面积、周长等为浮点型。同时，计算总的设施数，用于属性数据。


```python
park_boundaries_fn=r'D:\datasets\Parks - Chicago Park District Park Boundaries (current).geojson'
park_boundaries_gdf=gpd.read_file(park_boundaries_fn)
```


```python
columns_name4int=['pool_outdo', 'golf_cours', 'baseball_b', 'wheelchr_a','bocce_cour', 'sled_hill', 'artificial','tennis_cou','gymnasium', 'horseshoe_','objectid_1', 'boat_lau_1', 'harbor', 
                       'nature_bir', 'spray_feat','game_table', 'golf_drivi', 'fitness_co', 'cricket_fi', 'boat_slips','track', 'boxing_cen', 'golf_putti','handball_r','modeltrain', 'mountain_b',
                       'carousel', 'senior_cen', 'fitness_ce', 'lagoon', 'football_s', 'garden', 'baseball_j', 'wetland_ar', 'archery_ra', 'gallery','conservato', 'rowing_clu', 'gymnastic_', 'playground',
                       'handball_i', 'basketball', 'shuffleboa', 'band_shell', 'cultural_c', 'croquet','bowling_gr', 'playgrou_1', 'water_slid','nature_cen', 'modelyacht', 'pool_indoo', 'skate_park', 
                       'casting_pi','sport_roll', 'baseball_s', 'water_play', 'dog_friend', 'basketba_1','volleyba_1', 'iceskating','climbing_w', 'boat_launc', 'beach', 'volleyball', 'zoo', 'minigolf']
columns_name4float=['acres','shape_area','perimeter','shape_leng','ward','gisobjid','park_no']
park_boundaries_gdf[columns_name4int]=park_boundaries_gdf[columns_name4int].astype(int)
park_boundaries_gdf[columns_name4float]=park_boundaries_gdf[columns_name4float].astype(float)
park_boundaries_gdf.to_crs(crs,inplace=True)

park_boundaries_gdf['facilities_count']=park_boundaries_gdf.apply(lambda row:sum(row[columns_name4int]),axis=1)
```

查看一行数据。


```python
park_boundaries_gdf.loc[0].to_dict()
```




    {'pool_outdo': 0,
     'golf_cours': 0,
     'baseball_b': 0,
     'wheelchr_a': 0,
     'acres': 10.3,
     'bocce_cour': 3,
     'sled_hill': 0,
     'artificial': 0,
     'shape_area': 430341.671424,
     'location': '2901 S POPLAR AVE',
     'tennis_cou': 5,
     'perimeter': 2672.32662667,
     'zip': '60608',
     'gymnasium': 1,
     'horseshoe_': 3,
     'objectid_1': 1,
     'boat_lau_1': 0,
     'harbor': 0,
     'nature_bir': 0,
     'spray_feat': 1,
     'game_table': 0,
     'golf_drivi': 0,
     'fitness_co': 0,
     'cricket_fi': 0,
     'boat_slips': 0,
     'track': 0,
     'boxing_cen': 0,
     'golf_putti': 0,
     'label': 'McGuane',
     'handball_r': 0,
     'community_': '1',
     'modeltrain': 0,
     'mountain_b': 0,
     'carousel': 0,
     'senior_cen': 0,
     'fitness_ce': 1,
     'lagoon': 0,
     'football_s': 1,
     'garden': 0,
     'baseball_j': 3,
     'wetland_ar': 0,
     'archery_ra': 0,
     'gallery': 0,
     'shape_leng': 2672.32662666,
     'conservato': 0,
     'rowing_clu': 0,
     'gymnastic_': 0,
     'playground': 1,
     'ward': 11.0,
     'handball_i': 0,
     'basketball': 4,
     'shuffleboa': 0,
     'band_shell': 0,
     'cultural_c': 0,
     'croquet': 0,
     'bowling_gr': 0,
     'playgrou_1': 0,
     'water_slid': 0,
     'gisobjid': 416.0,
     'park': 'MCGUANE (JOHN)',
     'nature_cen': 0,
     'modelyacht': 0,
     'pool_indoo': 1,
     'skate_park': 0,
     'casting_pi': 0,
     'sport_roll': 0,
     'baseball_s': 1,
     'water_play': 0,
     'dog_friend': 0,
     'basketba_1': 1,
     'park_class': 'COMMUNITY PARK',
     'volleyba_1': 0,
     'iceskating': 0,
     'park_no': '2.0',
     'climbing_w': 0,
     'boat_launc': 0,
     'beach': 0,
     'volleyball': 0,
     'zoo': 0,
     'minigolf': 0,
     'geometry': <MULTIPOLYGON (((446184.954 4632184.749, 446182.772 4632183.72, 446169.854 4...>,
     'facilities_count': 27}



读取芝加哥城的边界数据，用于地图打印查看分析范围。


```python
chicago_boundary_fn=r'D:\datasets\scale\Boundaries - City.geojson'
chicago_boundary_gdf=gpd.read_file(chicago_boundary_fn)
chicago_boundary_gdf.to_crs(crs,inplace=True)
```

叠合打印人口分布、城市公园和芝加哥城边界，确定数据正确。


```python
fig, ax = plt.subplots(1, figsize=(8,8))

zip_pop_gdf.plot(column='Population - Total',cmap='gray',legend=True,ax=ax);
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=ax)
park_boundaries_gdf.plot(column='shape_area',cmap='hot',scheme='FisherJenks',k=10,ax=ax,legend=True,legend_kwds={'loc': 'lower left','frameon':False})
plt.show()
```


<img src="./imgs/3_8/output_172_0.png" height='auto' width='auto' title="caDesign">    
    


SIM 计算使用[SpInt（PySAL）](https://pysal.org/notebooks/model/spint/intro.html)<sup>⑱</sup>库，其输入参数中包含源汇的唯一标识，因此对于人口和公园增加了`pop_id`和`park_id`列。同时计算了人口和公园区域的几何中心点。在源汇距离计算上，除了计算质心点间距离，也计算了 Fotheringham 的距离计算方法用于比较，因此计算人口分布等同区域面积大小等价圆的半径`radius`。


```python
zip_pop_gdf['pop_id']=zip_pop_gdf.apply(lambda row:f'pop_{row.name}',axis=1)
zip_pop_controid_gdf=zip_pop_gdf.copy(deep=True)
zip_pop_controid_gdf['radius']=zip_pop_controid_gdf.apply(lambda row:math.sqrt(row.geometry.area/math.pi), axis=1)
zip_pop_controid_gdf['geometry']=zip_pop_controid_gdf['geometry'].centroid

park_boundaries_gdf['park_id']=park_boundaries_gdf.apply(lambda row:f'park_{row.name}',axis=1)
park_boundaries_centroid_gdf=park_boundaries_gdf.copy(deep=True)
park_boundaries_centroid_gdf['geometry']=park_boundaries_centroid_gdf['geometry'].centroid
```

打印查看质心数据。


```python
fig, ax = plt.subplots(1, figsize=(8,8))

zip_pop_gdf.boundary.plot(ax=ax,color='gray',linewidth=0.5)
zip_pop_controid_gdf.plot(column='Population - Total',cmap='gray',legend=True,ax=ax,marker='+',markersize=100)
park_boundaries_centroid_gdf.plot(column='shape_area',cmap='hot',scheme='FisherJenks',k=10,ax=ax,legend=True,legend_kwds={'loc': 'lower left','frameon':False},markersize=17)
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=ax)
plt.show()
```


<img src="./imgs/3_8/output_176_0.png" height='auto' width='auto' title="caDesign">    


计算与打印比较质心点间欧几里得距离和 Fotheringham 距离。从打印结果观察到，Fotheringham 距离程指数增长，距离较远的对象产生相对距离近对象更大的值，并仅打印了 Fotheringham 距离的前 400 个值，观察距离增长情况。


```python
pop_coordis=zip_pop_controid_gdf.get_coordinates().values
park_coordis=park_boundaries_centroid_gdf.get_coordinates().values
C_Z=distance.cdist(pop_coordis, park_coordis, 'euclidean')
R=zip_pop_controid_gdf.radius.values

fotheringham_d=lambda Z,R:list(map(lambda z,r:0.846*math.pow(1.693,z/r)*r,Z,R))
C_F=np.apply_along_axis(fotheringham_d,0,arr=C_Z,R=R)

fig,axes=plt.subplots(1,3,figsize=(12,4))
axes[0].plot(np.sort((C_Z)[0]))
axes[1].plot(np.sort(C_F[0]))
num=400
axes[2].plot(np.sort(C_F[0])[:num])

axes[0].set_title('Euclidean distance')
axes[1].set_title('Fotheringham distance')
axes[2].set_title(f'Fotheringham distance[{num}]')
plt.show()
```


<img src="./imgs/3_8/output_178_0.png" height='auto' width='auto' title="caDesign">    



将距离对应源汇唯一标识转换为 DataFrame 格式数据`sim_params_df`。并将源汇的属性信息增加到`sim_params_df`中，视作人口为源，属性信息字段配置有 'Population - Total','Population - Age 0-17'和'Population - Age 65+'；视作公园为汇，属性字段配置有'shape_area'和'facilities_count'。也把源汇唯一标识'pop_id'和'park_id'增加到`sim_params_df`中。


```python
sim_params_df=pd.DataFrame(C_F,columns=park_boundaries_centroid_gdf['park_id'],index=[zip_pop_controid_gdf['pop_id']])
sim_params_df=sim_params_df.melt(value_name='C',ignore_index=False).reset_index()

sim_params_df=pd.merge(sim_params_df,zip_pop_controid_gdf[['Population - Total','Population - Age 0-17','Population - Age 65+','pop_id']],on='pop_id')
sim_params_df=pd.merge(sim_params_df,park_boundaries_centroid_gdf[['shape_area','facilities_count','park_id']],on='park_id')
sim_params_df.tail(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pop_id</th>
      <th>park_id</th>
      <th>C</th>
      <th>Population - Total</th>
      <th>Population - Age 0-17</th>
      <th>Population - Age 65+</th>
      <th>shape_area</th>
      <th>facilities_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>147461</th>
      <td>pop_237</td>
      <td>park_616</td>
      <td>140926.319888</td>
      <td>61372</td>
      <td>12667</td>
      <td>11362</td>
      <td>17288.053354</td>
      <td>620</td>
    </tr>
    <tr>
      <th>147462</th>
      <td>pop_238</td>
      <td>park_616</td>
      <td>140926.319888</td>
      <td>63481</td>
      <td>13711</td>
      <td>11094</td>
      <td>17288.053354</td>
      <td>620</td>
    </tr>
  </tbody>
</table>
</div>



因为很难获取按人口分布区域到各个公园的人流量，因此假设人口区域（源）到各个公园（汇）的流出量为各人口区域人数按距离远近分配到各个公园中，较近的公园分配到更多的人数，而较远的公园分配到的人数趋于更少，其权重计算方式为$ \frac{max(D)-D}{ \sum D }$，式中，$D$为一个源到所有汇的距离向量。

图中打印了一个源汇距离权重（前400个）的变化趋势。


```python
C_F_weight=np.max(C_F,axis=1).reshape(-1,1)-C_F
C_F_weight_prob=C_F_weight/C_F_weight.sum(axis=1)[:,None]

fig,ax=plt.subplots(1,figsize=(3,3))
ax.plot(np.sort(C_F_weight_prob[0])[::-1][:400])
plt.show()
```


<img src="./imgs/3_8/output_182_0.png" height='auto' width='auto' title="caDesign">    



将流量并入到`sim_params_df`变量中。


```python
C_F_weight_prob=pd.DataFrame(C_F_weight_prob,columns=park_boundaries_centroid_gdf['park_id'],index=[zip_pop_controid_gdf['pop_id']])
C_F_weight_prob=C_F_weight_prob.melt(value_name='cf_w_prob',ignore_index=False).reset_index()
sim_params_df=pd.merge(sim_params_df,C_F_weight_prob, left_on=['pop_id','park_id'],right_on=['pop_id','park_id'])
sim_params_df['flow']=sim_params_df.apply(lambda row:int(row['cf_w_prob']*row['Population - Total']),axis=1)
sim_params_df.tail(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pop_id</th>
      <th>park_id</th>
      <th>C</th>
      <th>Population - Total</th>
      <th>Population - Age 0-17</th>
      <th>Population - Age 65+</th>
      <th>shape_area</th>
      <th>facilities_count</th>
      <th>cf_w_prob</th>
      <th>flow</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>147461</th>
      <td>pop_237</td>
      <td>park_616</td>
      <td>140926.319888</td>
      <td>61372</td>
      <td>12667</td>
      <td>11362</td>
      <td>17288.053354</td>
      <td>620</td>
      <td>0.001722</td>
      <td>105</td>
    </tr>
    <tr>
      <th>147462</th>
      <td>pop_238</td>
      <td>park_616</td>
      <td>140926.319888</td>
      <td>63481</td>
      <td>13711</td>
      <td>11094</td>
      <td>17288.053354</td>
      <td>620</td>
      <td>0.001722</td>
      <td>109</td>
    </tr>
  </tbody>
</table>
</div>



获得所有 SIM 的基本要素。


```python
sim_params_df.loc[sim_params_df['Population - Total'] == 0, ['Population - Total']] = 1
sim_params_df.loc[sim_params_df['Population - Age 0-17'] == 0, ['Population - Age 0-17']] = 1
sim_params_df.loc[sim_params_df['Population - Age 65+'] == 0, ['Population - Age 65+']] = 1

T=sim_params_df['flow'].values.reshape((-1,1))
C=sim_params_df['C'].values.reshape((-1,1))
V=np.hstack([
    sim_params_df['Population - Total'].values.reshape((-1,1)),
    sim_params_df['Population - Age 0-17'].values.reshape((-1,1)),
    sim_params_df['Population - Age 65+'].values.reshape((-1,1))
    ])
W=np.hstack([
    sim_params_df['shape_area'].values.reshape((-1,1)),
    sim_params_df['facilities_count'].values.reshape((-1,1)),
    ])
O=sim_params_df['pop_id'].values.reshape((-1,1))
D=sim_params_df['park_id'].values.reshape((-1,1))
```

### 3.8.4.2 SIM 族（family）<sup>[23,21]</sup>

#### 1）公式推导

Wilson, A. G. (1971)<sup>[24]</sup>提出的 SIM 族最具代表性，引入传统引力模型（gravity model），对应公式为$T_{i j}=k \frac{V_i^\mu W_j^\alpha}{d_{ij}^\beta} (1)$，式中，$T_{i j}$ 为$m$个源，$n$个汇，$m \times n$大小的源汇流量；$V$为$m \times p$ 大小的源属性信息矩阵；$W$为$n \times q$ 大小的汇属性信息矩阵；$d$为$m \times n$大小的源汇距离/时间/成本矩阵；$k$为待估计的比例因子，以确保观测到的和预测的总流量保持一致；$\mu$是一个$p \times 1$的参数向量，表示$p$个源属性信息对流量的影响；$\alpha$是一个$q \times 1$的参数向量，表示$q$个汇属性信息对流量的影响；$\beta$表示距离时间成本对流量的影响。

如果知道$T,V,W,d$等4个变量数据，就可以估计$k,\mu,\alpha,\beta$等模型参数，即模型校准（calibration），其反应了模型组份（$V,W, d$）的贡献度对已知流量系统（$T$）的作用。相反，当模型组份（$V,W$）发生变化，或者源汇位置、流向（$d$）发生改变，估计的参数可用于预测未知流量。

引力模型可以稍微更一般的写为$T_{i j}=kV_i^\mu W_j^\alpha f\left(d_{i j}\right) (2)$，式中$\frac{1}{d_{ij}^\beta} $被替换为$f\left(d_{i j}\right)$，$d_{i j}$的值减小时，$d_{i j}$函数的值增加。如果已知源各区域（点）$i$的总流出量$O_i$，则$\sum_j T_{i j}=O_i (3)$；类似的，如果已知汇各区域（点）$j$的总流入量，则$\sum_i T_{i j}=D_j (4)$。将 SIM 族的4类模型对应到公式（3）和（4）是否替换公式（2）对应的属性信息上可以表述为，如果如果不用（3）（4）替换任何属性值，保持公式（2）不变（但移除了比例因子$k$），有$ T_{i j}=V_i^\mu W_j^\alpha f\left(d_{i j}\right)$，即为无约束模型（unconstrained model，UCM）；如果替换$V_i^\mu$ ，有$T_{i j}  =A_i O_i W_j^\alpha f\left(d_{i j}\right)$，式中，$A_i =\sum_j W_j^\alpha f\left(d_{i j}\right)$，则为生产约束模型（Production-constrained model，PCM）；如果替换$W_j^\alpha$，有$T_{i j}  =B_j D_j V_i^\mu f\left(d_{i j}\right)$，式中，$B_j =\sum_i V_i^\mu f\left(d_{i j}\right)$，则为吸引力约束模型（Attraction-constrained model，ACM）；如果$V_i^\mu W_j^\alpha$均被替换，有$T_{i j}  =A_i B_j O_i D_j f\left(d_{i j}\right) $，式中，$A_i  =\sum_j W_j^\alpha B_j D_j f\left(d_{i j}\right)$，$B_j  =\sum_i V_i^\mu A_i O_i f\left(d_{i j}\right)$，则为双重约束（Doubly-constrained model，DCM）或生产-吸引力约束模型（Production-attraction-constrained model）。上述式中，$A_i$是源的$m \times 1$大小的平衡因子（balancing factors），确保总的流出量保持在预测的流量中；$B_i$是汇的$n \times 1$大小的平衡因子，确保总的流入量保持在预测的流量中。而距离函数$f\left(d_{i j}\right)$通常为幂函数或指数函数，分别对应公式为$f\left(d_{i j}\right)=d_{i j}^\beta$和$f\left(d_{i j}\right)=\exp \left(\beta d_{i j}\right)$，式中，$\beta$期望取负值。不同的距离衰减函数（distance-decay functions）对源汇间距离变化有不同的响应。需要注意，具有幂函数距离衰减的无约束模型（UCM）等效于公式（2）中的基本引力模型，但不包括比例因子$k$。事实上，在最大熵SIM族中的任何模型中都没有比例因子，因为在它们的推导和随后的模型校准中隐含着一个总流量（行程）约束。另外在 DCM 模型中，$A_i$和$B_j$相互依赖，可能需要根据校准技术迭代计算；且，通常还假设所有区域或位置都是原点和目的地

#### 2) 模型校准（参数估计）

估计 SIM 的参数通常用线性回归（linear regression），通过对 SIM 两边取自然对数实现，得到对数线性（log-linear）或对数正态（log-normal） 空间相互作用模型。例如对于 UCM，如果距离函数是幂函数，则为$\ln T_{i j}=k+\mu \ln V_i+\alpha \ln W_j-\beta \ln d_{i j}+\epsilon$；如果距离函数是指数函数，则为$\ln T_{i j}=k+\mu \ln V_i+\alpha \ln W_j-\beta d_{i j}+\epsilon $，式中，$\epsilon$是正态分布误差项，使得均值为0。对数正态引力模型存在几个问题，1. 流量为对象计数，例如人流量，应建模为离散的实体（discrete entities）；2. 流量往往并不是正态分布；3. 由于对流量的对数进行估计，而不是实际的流量，会导致预测的流量向下偏差； 4. 因为0的对数未被定义，因此流量为0时，需要用一个小的数量值（例如1）来替代。因此，Flowerdew, R.和Aitkin, M.（1982）<sup>[25]</sup>及Flowerdew, R.和Lovett, A. （1988）<sup>[26]</sup>等人提出了泊松对数线性回归（Poisson log-linear regression） SIM，其源汇$i,j$间的流量数值取自均值$ \lambda_{i j}=T_{i j}$的泊松分布（Poisson distribution ），假设$\lambda_{i j}$与变量的线性组合程对数关系，对于距离函数为幂函数时，有$\ln \lambda_{i j}=k+\mu \ln V_i+\alpha \ln W_j-\beta \ln d_{i j}$，并对方程两边取幂，得到 UCM 的泊松对数线性引力模型，有$T_{i j}=\exp \left(k+\mu \ln V_i+\alpha \ln W_j-\beta \ln d_{i j}\right)$。

> 对泊松分布的解释可以查看*概率论基础——泊松分布*部分。

类似，对于 PCM 有$T_{i j}=\exp \left(k+\mu_i+\alpha \ln W_j-\beta \ln d_{i j}\right)$；对于 ACM 有$T_{i j}=\exp \left(k+\mu \ln V_i+\alpha_j-\beta \ln d_{i j}\right)$ ；对于 DCM 有$T_{i j}=\exp \left(k+\mu_i+\alpha_j-\beta \ln d_{i j}\right)$，式中，$\mu_i$为源约束，$\alpha_j$为汇约束。$k$为平衡因子，也是估计的截距，必须被包含在上述泊松对数线性模型中，以确保流量的总数是守恒的。

泊松回归的参数估计可以使用广义线性模型（generalized linear modeling，GLM）的迭代加权最小二乘法（ iteratively weighted least squares，IWSL）（为了解决 GLM 中因变量/响应变量异方差问题而对普通最小二乘法进行的推广），收敛于最大似然估计。

#### 3）广义线性模型（GLM）

在统计学中，GLM 是对普通线性回归的推广，以一组自变量的线性函数来预测因变量的条件平均数或条件平均数的某种函数。且对于每个观测值或研究对象（因变量）的期望值或期望值的某种函数根据自变量确定。GLM 可以处理因变量的条件平均数为回归参数的非线性函数和因变量为非正态分布的数据<sup>[27]</sup>。在 GLM 中，假设因变量的每个观测值$Y$都是从指数族（exponential family）中某一个特定分布中产生，这里的指数族是一大类概率分布，包括正态分布（ normal）、二项分布（ binomial）、泊松分布（ Poisson ）和伽马分布（ gamma ）等。分布的平均值$\mu$取决于自变量$X$，有$\mathbf{E}(\mathbf{Y} \mid \mathbf{X})=\boldsymbol{\mu}=g^{-1}(\mathbf{X} \boldsymbol{\beta})$，式中，$\mathbf{E}(\mathbf{Y} \mid \mathbf{X})$为以$\mathbf{X}$为条件的$\mathbf{Y}$的期望值；$\mathbf{X} \boldsymbol{\beta}$是线性预测器（ linear predictor），为未知参数$\beta$的线性组合；$g$是连接函数。在整个结构中，方差$V$通常是均值的函数，有$\operatorname{Var}(\mathbf{Y} \mid \mathbf{X})=\mathrm{V}\left(g^{-1}(\mathbf{X} \boldsymbol{\beta})\right)$。方差服从指数分布族，也可能仅仅是预测值的一个函数。待估参数$\beta$通常用最大似然（ maximum likelihood）、拟最大似然（ quasi-likelihood）或贝叶斯（ Bayesian）估计。

GLM 包含3个组成部分，1. 一个来自于指数族概率分布中，用来对$Y$建模的的一个特定分布；2. 一个线性预测器，$\eta=X \beta$；3. 一个连接函数$g$，例如$\mathrm{E}(Y \mid X)=\mu=g^{-1}(\eta) $。下表为几个常用指数族分布表<sup>[28]</sup>，包括分布支持的数据类型和取值范围，典型的连接函数（canonical link function ）及其逆函数（有时称为均值函数）。

| Distribution（分布）  | Support of distribution（分布支持的数值和取值范围）	  | Typical uses（典型用途）  |  Link name（连接名） |  Link function（连接函数）, $$\mathbf{X} \boldsymbol{\beta}=g(\mu)$$| Mean function（均值函数）  |
|---|---|---|---|---|---|
| Normal（正态）   |  real（实数）：$(- \infty ,+ \infty )$ | Linear-response data（线性响应数据）  |  Identity | $\mathbf{X} \boldsymbol{\beta}=\mu$  | $\mu=\mathbf{X} \boldsymbol{\beta}$  |
| Exponential（指数）/ Gamma（伽马） | real：：$(0 ,+ \infty )$  | Exponential-response data, scale parameters （指数响应数据，尺度参数） |  Negative inverse |  $\mathbf{X} \boldsymbol{\beta}=-\mu^{-1}$ | $\mu=-(\mathbf{X} \boldsymbol{\beta})^{-1}$  |
| Inverse Gaussian（逆高斯）  |  real：$(0 ,+ \infty )$  |   |   Inverse squared| $\mathbf{X} \boldsymbol{\beta}=\mu^{-2}$  | $\mu=(\mathbf{X} \boldsymbol{\beta})^{-1 / 2}$  |
| Poisson （泊松） | integer（整数）：$0,1,2,  \cdots $  |  count of occurrences in fixed amount of time/space （在固定时间/空间内发生的次数）| Log  |  $\mathbf{X} \boldsymbol{\beta}=\ln (\mu)$ |  $\mu=\exp (\mathbf{X} \boldsymbol{\beta})$ |
| Bernoulli （伯努利） |  integer：$\{0,1\}$  | outcome of single yes/no occurrence（单个 是/否 事件发生的结果）  | Logit | $ \mathbf{X} \boldsymbol{\beta}=\ln \left(\frac{\mu}{1-\mu}\right)$  | $\mu=\frac{\exp (\mathbf{X} \boldsymbol{\beta})}{1+\exp (\mathbf{X} \boldsymbol{\beta})}=\frac{1}{1+\exp (-\mathbf{X} \boldsymbol{\beta})}$  |
| Binomial （二项）  |  integer：$0,1,\cdots, N $  |  count of "yes" occurrences out of N yes/no occurrences （N个 是/否 事件中出现“是”的次数）|  Logit | $\mathbf{X} \boldsymbol{\beta}=\ln \left(\frac{\mu}{n-\mu}\right)$  | 同上  |
| Categorical （分类） | integer：$[0,K)$ </br> K-vector of integer：$[0,1]$ where exactly one element in the vector has the value 1（向量中只有一个元素的值是1）|  outcome of single K-way occurrence （单个 K-类 事件发生的结果）| Logit  | $\mathbf{X} \boldsymbol{\beta}=\ln \left(\frac{\mu}{1-\mu}\right)$  |  同上 |
| Multinomial （多项式）  | K-vector of integer（整数K向量）： $[0,N]$ | count of occurrences of different types (1 .. K) out of N total K-way occurrences （N个总共K类事件中，不同类型发生的次数） |   Logit|  $\mathbf{X} \boldsymbol{\beta}=\ln \left(\frac{\mu}{1-\mu}\right)$ |  同上 |

指数分布和伽马分布情况下，典型连接函数的定义域和均值的允许范围不同，特别是线性预测器可能为正时，将会给出一个不可能的负均值。当最大化似然时，必须采取措施来避免这种情况，或使用非典型的连接函数。在伯努利分布、二项分布、分类分布和多项式分布情况下，分布支持的不是与被预测参数相同的数据类型。在所有这些情况下，预测参数是一个或多个概率，即在$[0,1]$范围内的实数。由此产生的模型被称为逻辑回归（ logistic regression）或多项逻辑回归（multinomial logistic regression）。对逻辑回归的解释可以查看* 逻辑回归二分类到 SoftMax 回归多分类*部分。在伯努利分布和二项分布中，参数为单个概率，表示单个事件发生的可能性。即使单个结果总是为0或1，伯努利仍然满足广义线性模型的基本条件，期望值仍然是一个实值概率，即出现“是”（或1）结果的概率。同样，在二项分布中，出现“是”结果的期望比例是被预测的概率。对于分类分布和多项式分布，要预测的参数是概率的K-向量，且所有概率的总和必须是1。每个概率表示K个可能值中的一个出现的可能性。对于多项式分布和向量形式的分类分布，向量元素的期望值与预测概率的关系类似于二项分布和伯努利分布。

GLM 拟合（fitting）通常使用最大似然（Maximum likelihood）或者贝叶斯方法（Bayesian methods）估计，对于最大似然估计可以使用迭代加权最小二乘法（ iteratively reweighted least squares）或者牛顿方法（ Newton's method ），其形式更新为$\boldsymbol{\beta}^{(t+1)}=\boldsymbol{\beta}^{(t)}+\mathcal{J}^{-1}\left(\boldsymbol{\beta}^{(t)}\right) u\left(\boldsymbol{\beta}^{(t)}\right)$，式中，$\mathcal{J}^{-1}\left(\boldsymbol{\beta}^{(t)}\right)$为观测到的信息矩阵（ observed information matrix）(黑森矩阵（Hessian matrix）的负数)；$u\left(\boldsymbol{\beta}^{(t)}\right)$是计分函数（ score function）。或使用费雪的计分方法（Fisher's scoring method），为$\boldsymbol{\beta}^{(t+1)}=\boldsymbol{\beta}^{(t)}+\mathcal{I}^{-1}\left(\boldsymbol{\beta}^{(t)}\right) u\left(\boldsymbol{\beta}^{(t)}\right)$，式中，$\mathcal{I}^{-1}\left(\boldsymbol{\beta}^{(t)}\right) $是费雪信息（Fisher information）矩阵。如果使用典型连接函数，则它们是相同的。

#### 4）GLM：泊松回归模型<sup>[29]</sup>

SIM 族模型校准时使用了泊松对数线性回归，其为 GLM 常用指数族的一种分布。为了更清晰的理解 GLM 的泊松分布，以[布鲁克林大桥骑车人计数数据集（骑行计数）](https://gist.github.com/sachinsdate/c17931a3f000492c1c42cf78bf4ce9fe)<sup>⑲</sup>为例来逐步解析，该数据集观测时间从2017年4月1日到2017年10月31日，有214行样本数据。从上表中可以得知泊松分布的因变量为计数数据，即大于等于0的整数，分析的内容是固定时间或空间内事件发生的次数，例如城市居民每日（或周、月、年及其它任何区段时间）使用城市绿地公园的次数；一个观测路口，每小时通过车辆的数量；特定地点每日骑车人数的统计等，且这些基于计数的数据服从泊松分布，其分布具有一定的偏斜（Skewed Distribution），数据可能只包含几个值的大量数据点，从而使频率分布发生偏斜，例如下图骑行计数的频数分布。


```python
nyc_bb_bicyclist_counts_fn=r'D:\datasets\nyc_bikes\nyc_bb_bicyclist_counts.csv'
nyc_bb_bicyclist_counts=pd.read_csv(nyc_bb_bicyclist_counts_fn, header=0, infer_datetime_format=True, parse_dates=[0], index_col=[0])
ds=nyc_bb_bicyclist_counts.index.to_series()
nyc_bb_bicyclist_counts['MONTH'] = ds.dt.month
nyc_bb_bicyclist_counts['DAY_OF_WEEK'] = ds.dt.dayofweek
nyc_bb_bicyclist_counts['DAY'] = ds.dt.day
nyc_bb_bicyclist_counts
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>HIGH_T</th>
      <th>LOW_T</th>
      <th>PRECIP</th>
      <th>BB_COUNT</th>
      <th>MONTH</th>
      <th>DAY_OF_WEEK</th>
      <th>DAY</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2017-04-01</th>
      <td>46.0</td>
      <td>37.0</td>
      <td>0.00</td>
      <td>606</td>
      <td>4</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2017-04-02</th>
      <td>62.1</td>
      <td>41.0</td>
      <td>0.00</td>
      <td>2021</td>
      <td>4</td>
      <td>6</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2017-04-03</th>
      <td>63.0</td>
      <td>50.0</td>
      <td>0.03</td>
      <td>2470</td>
      <td>4</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2017-04-04</th>
      <td>51.1</td>
      <td>46.0</td>
      <td>1.18</td>
      <td>723</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2017-04-05</th>
      <td>63.0</td>
      <td>46.0</td>
      <td>0.00</td>
      <td>2807</td>
      <td>4</td>
      <td>2</td>
      <td>5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2017-10-27</th>
      <td>62.1</td>
      <td>48.0</td>
      <td>0.00</td>
      <td>3150</td>
      <td>10</td>
      <td>4</td>
      <td>27</td>
    </tr>
    <tr>
      <th>2017-10-28</th>
      <td>68.0</td>
      <td>55.9</td>
      <td>0.00</td>
      <td>2245</td>
      <td>10</td>
      <td>5</td>
      <td>28</td>
    </tr>
    <tr>
      <th>2017-10-29</th>
      <td>64.9</td>
      <td>61.0</td>
      <td>3.03</td>
      <td>183</td>
      <td>10</td>
      <td>6</td>
      <td>29</td>
    </tr>
    <tr>
      <th>2017-10-30</th>
      <td>55.0</td>
      <td>46.0</td>
      <td>0.25</td>
      <td>1428</td>
      <td>10</td>
      <td>0</td>
      <td>30</td>
    </tr>
    <tr>
      <th>2017-10-31</th>
      <td>54.0</td>
      <td>44.0</td>
      <td>0.00</td>
      <td>2727</td>
      <td>10</td>
      <td>1</td>
      <td>31</td>
    </tr>
  </tbody>
</table>
<p>214 rows × 7 columns</p>
</div>




```python
ax=nyc_bb_bicyclist_counts.hist(column=['BB_COUNT'],figsize=(5,5))
```


<img src="./imgs/3_8/output_191_0.png" height='auto' width='auto' title="caDesign">    


泊松分布的概率质量函数（Probability mass function）定义为，如果离散随机变量$X$具有如下的概率质量函数，则称其具有参数$\lambda > 0$的泊松分布，$f(k ; \lambda)=\operatorname{Pr}(X=k)=\frac{\lambda^k e^{-\lambda}}{k !}$，式中，$k$是出现的次数（$k=0,1,2, \ldots $）；$e$是欧拉数（ Euler's number ）（$e=2.71828 \ldots$）;$！$是阶乘（ factorial function）。正实数$\lambda$等于$X$的的期望值，也等于它的方差，$ \lambda=\mathrm{E}(X)=\operatorname{Var}(X)$。泊松分布可以应用于具有大量可能事件的系统，这些事件中的每一个都是罕见的。在一定的时间间隔内发生这类事件的数目，在适当的情况下，是一个具有泊松分布的随机数。如果给出的是事件发生的平均速率$r$，而不是平均事件数$\lambda$，则可以对方程进行调整为，使$\lambda=rt$，有$ P(k \text { events in interval } t)=\frac{(r t)^k e^{-r t}}{k !}$。如果速率$r$或$\lambda$恒定，则可以简单的使用一个修正的平均模型来预测未来的事件计数。在这种情况下，可以将计数的所有预测值设置为整个常数$r$或$\lambda$。如下图，用`np.random.poisson`方法从设置固定$\lambda$为20的泊松分布中采样20个数值的结果，同时打印了时间间隔上的变化值和直方图（计数的频率分布图）及其分布曲线，可以观察到$\lambda$即为计数的期望值。


```python
#np.random.seed(983154356)
X=list(range(0,100,5))
print(len(X))
lam=20
data_fixed_r=np.random.poisson(lam=lam,size=len(X))
fig,axes=plt.subplots(1,2,figsize=(20,3),gridspec_kw={'width_ratios': [3, 1]})
axes[0].plot(X,data_fixed_r)
axes[0].xaxis.set_ticks(X)
axes[0].grid()
axes[0].hlines(y = lam, color = 'r', linestyle = '--',xmin=0,xmax=100)
n, bins, patches=axes[1].hist(data_fixed_r,density=True)
axes[1].vlines(x = lam, color = 'r', linestyle = '--',ymin=0,ymax=0.1)
axes[1].plot(X,poisson.pmf(X,lam))
plt.show()
```

    20
    

<img src="./imgs/3_8/output_193_1.png" height='auto' width='auto' title="caDesign">
    

当不固定$\lambda$，$\lambda$可以从一个观测值变化到下一个观测值，下面假设了$\lambda$值取自指数分布，其公式为$f(x)=exp(-x),x \geq 0$。然后，由各个$\lambda$建立各自对应的泊松分布，并从各自对应的泊松分布中随机采样一个值。


```python
from scipy.stats import expon

mean, var, skew, kurt=expon.stats(moments='mvsk')
X=np.linspace(expon.ppf(0.01),expon.ppf(0.99), 50)
y=expon.pdf(X)*100

y_poisson=np.random.poisson(y)

fig,ax=plt.subplots(figsize=(20,3))
ax.plot(X,y_poisson)
ax.plot(X,y,'o')
ax.plot(X,y,'--')
ax.xaxis.set_ticks(X.flatten())
ax.tick_params(axis='x', rotation=90)
ax.grid()
plt.show()
```


<img src="./imgs/3_8/output_195_0.png" height='auto' width='auto' title="caDesign">   



如果上述假设的$\lambda$值受到属性信息（自变量）的影响，为$X \beta$线性预测器的一个函数，即连接函数$\boldsymbol{f}(\cdot)$，且选择一个指数函数为连接函数，为$\lambda_i=e^{x_i \beta}$，一般的形式表示为$\lambda=e^{X \boldsymbol{\beta}}$。即使当自变量$X$或回归系数$\beta$为负值，该连接函数也保持$\lambda$为非负值，满足泊松分布基于计数数据的要求；根据可用于计数数据的泊松分布，可以建立$\lambda$于$y$之间的关系，其中$y$即对应到泊松分布中的$k$，因此可写作$\operatorname{P}(y_i   \mid x_i)=\frac{e^{-\lambda_i}  \lambda_i^{y_i}}{y_{i} !}$，式中，$\lambda_i=e^{x_i \beta}$，自变量$x_i$对应数据集中第$i$个观测值$y_i$，观测到$y_i$计数的概率为泊松分布。并意味着观测到$y_i$的概率是事件向量$\lambda_i$的函数。下述关系图解了 GLM 泊松分布的结构。

$\left(\begin{array}{ccccc}\mathrm{x}_{11} & \mathrm{x}_{12} & \mathrm{x}_{13} & \ldots & \mathrm{x}_{1 \mathrm{~m}} \\ \mathrm{x}_{21} & \mathrm{x}_{22} & \mathrm{x}_{23} & \ldots & \mathrm{x}_{2 \mathrm{~m}} \\ \vdots & \vdots & \vdots & & \vdots \\ \vdots & \vdots & \vdots & & \vdots \\ \mathrm{x}_{\mathrm{n} 1} & \mathrm{x}_{\mathrm{n} 2} & \mathrm{x}_{\mathrm{n} 3} & \cdots & \mathrm{x}_{\mathrm{nm}}\end{array}\right) \boldsymbol{f}(\cdot)\left(\begin{array}{c}\beta_1 \\ \beta_2 \\ \vdots \\ \vdots \\ \beta_{\mathrm{m}}\end{array}\right) \Rightarrow\left(\begin{array}{c}\lambda_1 \\ \lambda_2 \\ \vdots \\ \vdots \\ \lambda_{\mathrm{n}}\end{array}\right) f(y ; \lambda)\left(\begin{array}{c}\mathrm{y}_1 \\ \mathrm{y}_2 \\ \vdots \\ \vdots \\ y_{\mathrm{n}} \end{array}\right)$

使用最大似然估计参数$\beta$，在数据上得到充分训练确定$\beta$后，就可以用该模型进行预测。要预测与观测到的自变量$x_p$输入行对应的事件计数$y_p$，可以使用公式$y_p=\lambda_p=e^{x_p \boldsymbol{\beta}}$计算。

训练泊松回归模型使用最大似然估计（Maximum Likelihood Estimation，MLE），其过程可以拆解为（使用骑行计数数据示例），首先根据数据集中已知的事件计数因变量$y_i$，即`BB_COUNT`列；并将列`HIGH_T`、`LOW_T`、`PRECIP`、`MONTH`、`DAY_OF_WEEK`和`DAY`作为自变量$x_i$，由泊松分布的概率质量函数（Probability mass function，PMF），有如下示例：

$
\begin{aligned}
& P\left(606 \mid x_1\right)=\frac{e^{-\lambda_1} * \lambda_1^{606}}{606 !} \\
& P\left(2021 \mid x_2\right)=\frac{e^{-\lambda_2} * \lambda_2^{2021}}{2021 !} \\
& P\left(2470 \mid x_3\right)=\frac{e^{-\lambda_3} * \lambda_3^{2470}}{2470 !} \\
& P\left(723 \mid x_4\right)=\frac{e^{-\lambda_4} * \lambda_4^{723}}{723 !}
\end{aligned}
$

上述式中的$\lambda_1,\lambda_2,\lambda_3, \cdots,\lambda_n$由连接函数示例为（回归系数为$\beta$），

$
\begin{aligned}
& \lambda_1=e^{x_1 \beta} \\
& \lambda_2=e^{x_2 \beta} \\
& \lambda_3=e^{x_3 \beta} \\
& \lambda_4=e^{x_4 \beta}
\end{aligned}
$

数据集中整个$n$个$y_1,y_2,\cdots,y_n$计数集合发生的概率是单个计数出现的联合概率。计数$y$服从泊松分布，$y_1,y_2,\cdots,y_n$是独立的随机变量，对应自变量$x_1,x_2,\cdots,x_n$。因此，$y_1,y_2,\cdots,y_n$出现的联合概率可以表示为单个概率的乘法，有，

$\begin{aligned} & P(\boldsymbol{y} \mid \boldsymbol{X})=P\left(606 \mid \boldsymbol{x}_{\mathbf{1}}\right) * P\left(2021 \mid \boldsymbol{x}_2\right) * P\left(2470 \mid \boldsymbol{x}_3\right) * \ldots * P\left(723 \mid \boldsymbol{x}_{\boldsymbol{n}}\right) \\ & \therefore L(\boldsymbol{\beta})=P(\boldsymbol{y} \mid \boldsymbol{X})=\frac{e^{-\lambda_1} * \lambda_1^{606}}{606 !} * \frac{e^{-\lambda_2} * \lambda_2^{2021}}{2021 !} * \frac{e^{-\lambda_3} * \lambda_3^{2470}}{2470 !} * \ldots * \frac{e^{-\lambda_n} * \lambda_n^{723}}{723 !}\end{aligned}$

估计参数$\beta$使得由自变量$X$最有可能得到观测计数集合因变量$y$，就是上式的联合概率达到最大值时的$\beta$，即$\beta$的联合概率变化率为0时$\beta$的值，通过微分联合概率方程并令微分方程为0得到该解。对联合概率方程的对数求导比对原方程求导容易，且对数方程的求解得到相同最有$\beta$值。这个对数方程被称为对数似然函数（ log-likelihood function），泊松回归的对数似然函数为$\ln L(\boldsymbol{\beta})=\sum_{i=1}^n\left(y_i \boldsymbol{x}_{\boldsymbol{i}} \boldsymbol{\beta}-e^{\boldsymbol{x}_{\boldsymbol{i}} \boldsymbol{\beta}}-\ln y_{i} !\right)$，式中已经将$\lambda$替换为连接函数。微分这个对数似然方程，并将其设为0，有$\sum_{i=1}^n\left(y_i-e^{\boldsymbol{x}_i \boldsymbol{\beta}}\right) \boldsymbol{x}_{\boldsymbol{i}}=\mathbf{0}$，求解该方程，估计回归参数$\beta$，得到$\beta$的最大似然估计（用迭代加权最小二乘法）。

将骑行计数数据集切分为训练和测试数据集。


```python
mask=np.random.rand(len(nyc_bb_bicyclist_counts)) < 0.8
df_train=nyc_bb_bicyclist_counts[mask]
df_test=nyc_bb_bicyclist_counts[~mask]
print('Training data set length='+str(len(df_train)))
print('Testing data set length='+str(len(df_test)))
```

    Training data set length=164
    Testing data set length=50
    

指定因变量（结果变量）和自变量（解释变量）对应的列名，得到训练和测试数据集的因变量和自变量数据。


```python
expr="""BB_COUNT ~ DAY  + DAY_OF_WEEK + MONTH + HIGH_T + LOW_T + PRECIP"""
y_train, X_train=dmatrices(expr, df_train, return_type='dataframe')
y_test, X_test=dmatrices(expr, df_test, return_type='dataframe')
```

用[statsmodels](https://www.statsmodels.org/dev/glm.html)<sup>⑳</sup>库的 GLM 计算。 


```python
poisson_training_results=sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()
```

打印结果。报告中偏差值（Deviance）和皮尔逊卡方值（Pearson chi2）都非常大，因此获得一个好的拟合模型不太可能（泊松分布的期望值（均值）和方差都为$\lambda$，这相当严格的条件在大多数实际数据中都很难达到）。为了定量确定在某种置信水平上的拟合优度（goodness-of-fit），例如95%置信水平（p=0.05），用`scipy.stats.chi2.ppf`方法计算求得为值为193.791，这远远小于报告中的统计量22419和22000。


```python
print(poisson_training_results.summary())
print(scipy.stats.chi2.ppf(1-.05, df=163))
```

                     Generalized Linear Model Regression Results                  
    ==============================================================================
    Dep. Variable:               BB_COUNT   No. Observations:                  164
    Model:                            GLM   Df Residuals:                      157
    Model Family:                 Poisson   Df Model:                            6
    Link Function:                    Log   Scale:                          1.0000
    Method:                          IRLS   Log-Likelihood:                -11999.
    Date:                Sat, 26 Aug 2023   Deviance:                       22419.
    Time:                        10:09:32   Pearson chi2:                 2.20e+04
    No. Iterations:                     5   Pseudo R-squ. (CS):              1.000
    Covariance Type:            nonrobust                                         
    ===============================================================================
                      coef    std err          z      P>|z|      [0.025      0.975]
    -------------------------------------------------------------------------------
    Intercept       6.8603      0.013    526.255      0.000       6.835       6.886
    DAY             0.0009      0.000      4.940      0.000       0.001       0.001
    DAY_OF_WEEK    -0.0265      0.001    -35.320      0.000      -0.028      -0.025
    MONTH           0.0096      0.001     11.574      0.000       0.008       0.011
    HIGH_T          0.0267      0.000     76.718      0.000       0.026       0.027
    LOW_T          -0.0148      0.000    -39.297      0.000      -0.016      -0.014
    PRECIP         -0.7707      0.008   -102.173      0.000      -0.785      -0.756
    ===============================================================================
    193.7914446333242
    

除了从打印结果查看统计推断，同时用训练的模型预测测试数据集，检验模型的精度，结果中，`mean`为预测的$y$计数值；`mean_se`为标准误（Standard error）；`mean_ci_lower`和`mean_ci_upper`为95%置信区间（Confidence interval）。


```python
poisson_predictions=poisson_training_results.get_prediction(X_test)
poisson_predictions = poisson_training_results.get_prediction(X_test)
#summary_frame() returns a pandas DataFrame
predictions_summary_frame=poisson_predictions.summary_frame()
print(predictions_summary_frame[:10])
```

                       mean    mean_se  mean_ci_lower  mean_ci_upper
    Date                                                            
    2017-04-13  2484.023894   8.762912    2466.908141    2501.258399
    2017-04-15  2309.102089   8.168081    2293.148313    2325.166858
    2017-04-19  1955.970363   9.262967    1937.899278    1974.209963
    2017-04-21  1475.335071   7.102270    1461.480342    1489.321142
    2017-04-24  2388.521467  11.094577    2366.875179    2410.365722
    2017-04-25   988.518737   7.541341     973.847937    1003.410550
    2017-04-28  3554.405901  19.974595    3515.471227    3593.771785
    2017-05-05   197.742351   4.356498     189.385496     206.467962
    2017-05-07  1750.721683   9.140672    1732.897648    1768.729049
    2017-05-08  2375.721875  10.561282    2355.112061    2396.512048
    

打印测试数据集的实际观测计数和用训练模型预测的计数，可以看到预测结果虽然预测值和观测值有偏差，有的甚至相差甚远，但是或多或少的跟随了观测值的变化趋势，这从预测值和观测值的散点图中也可以观察到。


```python
predicted_counts=predictions_summary_frame['mean']
actual_counts=y_test['BB_COUNT']

fig,axes=plt.subplots(1,2,figsize=(20,3),gridspec_kw={'width_ratios': [3, 1]})

axes[0].set_title('Predicted versus actual bicyclist counts on the Brooklyn bridge')
predicted,=axes[0].plot(X_test.index, predicted_counts, 'go-', label='Predicted counts')
actual,=axes[0].plot(X_test.index, actual_counts, 'ro-', label='Actual counts')
axes[0].legend(handles=[predicted, actual])

axes[1].set_title('Scatter plot of Actual versus Predicted counts')
axes[1].scatter(x=predicted_counts, y=actual_counts, marker='.')
axes[1].set_xlabel('Predicted counts')
axes[1].set_ylabel('Actual counts')
plt.show()
```


<img src="./imgs/3_8/output_209_0.png" height='auto' width='auto' title="caDesign">   



#### 5）模型拟合统计（Model ﬁt statistics）

`SpInt`库在构建 SIM 时，自定义了 GLM 实现库[SpGLM](https://github.com/pysal/spglm)<sup>㉑</sup>，目前支持使用迭代加权最小二乘法（ iteratively weighted least squares estimation , IWLS）对高斯、泊松和逻辑回归进行估计。`SpGLM`与`Statsmodels`库最大的不同是，`SpGLM`自定义的 IWLS 是完全稀疏兼容的，这有益于 SIM（constrained） 中出现的非常稀疏的设计矩阵，以减小计算开销；另外，`SpGLM`支持准泊松模型（QuasiPoisson models）的估计。

为了评估 SIM 拟合，一般可以使用各种统计量辅助检验。对于对数正态回归（log-normal regression）通常使用决定系数（coeﬃcient of determination，$R^2$），虽然`SpInt`库的 GLM $R^2$不可用，但该库提供了一个基于似然函数的伪（pseudo）$R^2$，公式为$R_{\text {pseudo }}^2=1-\frac{\ln \hat{L}\left(M_{\text {full }}\right)}{\ln \hat{L}\left(M_{\text {Intercept }}\right)}$，式中，$\hat{L}$是估计模型的似然；$M_{\text {full }}$是包含所有感兴趣解释变量的模型；$M_{\text {Intercept }}$是只有一个截距（即没有协变量）的模型。同$R^2$，$R_{\text {pseudo }}^2$在值为1时达到最大值，值越高表示模型拟合的越好。考虑到模型的复杂性，该统计量的一个调整版本（调整$R^2$）为$R_{\text {adj-pseudo }}^2=1-\frac{\ln \hat{L}\left(M_{\text {full }}\right)-K}{\ln \hat{L}\left(M_{\text {Intercept }}\right)}$，式中，$k$是回归量的个数。如果模型拟合没有得到充分的改善，那么随着变量的增加，这个度量可能会减小，表明额外的变量对更好的模型拟合没有贡献。

`SpInt`库提供了赤池信息准则（Akaike information criterion，AIC），公式为$A I C=-2 \ln \hat{L}\left(M_{\text {full }}\right)+2 K$，AIC 值越低表明模型拟合越好。该统计数据基于信息论，即 AIC 是使用完整模型来表示给定理论过程时丢失信息的渐近估计。

$R^2$和AIC是为模型选择而设计，这意味着这个两个统计量不应该用于不同空间系统之间的比较。这个问题的一个解决方案是使用标准化均方根误差（standardized root mean square error，SRMSE），公式为，$\text { SRMSE }=\frac{\sqrt{\sum_i \sum_j\left(T_{i j}-\hat{T}_{i j}\right)^2}}{\frac{\sum_i \sum_j T_{i j}}{n m}}$，式中，分子为观察流量$T_{i j}$与模型预测流量$\hat{T}_{i j}$的均方根误差；分母为观察流量的平均值，负责统计量的标准化。$nm$是构成流系统源汇对的数量。SRMSE 值为0表示模型拟合完美，值越高表示模型拟合的越差，然而统计量的上限并不一定值为1，而是取决于观测值的分布。

最后一个拟合统计量是调整的 Sorensen 相似度指数（modiﬁed Sorensen similarity inde，SSI）。SSI 在处理一些非参数模型（ non-parametric models）的空间相互作用文献中越来越受欢迎，使用与 SRMSE 相同的符号定义，$S S I=\frac{1}{(n m)} \sum_i \sum_j \frac{2 \min \left(T_{i j}, \hat{T}_{i j}\right)}{T_{i j}+\hat{T}_{i j}}$，其值位于[0,1]区间，接近1的值表示更好的模型拟合。

### 3.8.4.3 绿地公园供给与人口需求关系的 SIM 参数估计

#### 1）参数估计与拟合统计

使用`SpInt` 分别计算 SIM 族的4个模型，UCM、PCM、ACM和DCM。估计的参数可以通过实例化 SIM 的`params`属性访问，其中第1个参数始终为总体截距。指数距离（`exp`）衰减使得模型不取$d_{ij}$（$C$）的对数，因此可以使用具有指数衰减结构的 UCM （无约束引力模型）说明泊松回归系数的典型解释。-9.41798514e-34 数值是距离变量$C$的系数，在泊松回归中，系数通常被解释为预测响应的比例变化，这里为$T_{ij}$，如果将解释变量增加一个单位，则$\tilde{T}_{i j}=T_{i j} * \exp (\beta)$，式中，$\tilde{T}_{i j}$是$T_{i j}$的一个更新值；$\beta$是变量距离$C$的系数。这意味着距离增加1个单位，保持所有其它因素不变，$T_{ij}$将减少至$\tilde{T}_{i j}$。因为本次试验假设的（人）流量是以距离$C$为权重分配人口数据获得，因此可以发现距离变量$C$的系数非常小，则$exp(\beta)$的值接近于1，对流量的更新的影响趋近于0。由上述公式也可以得出，距离增加一个单位带来预期变化百分比为$\Delta_{\%}=(1-\exp (\beta)) * 100.0$。

参数属性中除了首尾项，其它各项对应到源汇属性信息（解释变量）的系数，依次为源的`Population - Total`（-4.66792387e-02）、`Population - Age 0-17`（3.15354732e-01 ）和`Population - Age 65+`（6.41389065e-02），及汇的`shape_area`（ -5.81482478e-03）和`facilities_count`（2.10285439e-03）。如果将相关的属性信息增加1%，系数的解释就变成了预测响应（$T_{ij}$）的百分比变化。比较源汇属性信息的系数，可发现`Population - Age 0-17`属性对应的系数相对最大，即该属性对流量的影响相对较大。

* 计算 UCM


```python
grav=Gravity(flows=T, o_vars=V, d_vars=W, cost=C, cost_func='exp')
print(grav.params)
print('Adjusted psuedo R2: ', grav.adj_pseudoR2)
print('Adjusted D2: ', grav.adj_D2)
print('SRMSE: ', grav.SRMSE)
print('Sorensen similarity index: ', grav.SSI)
```

    [ 1.55407020e+00 -4.66792387e-02  3.15354732e-01  6.41389065e-02
     -5.81482478e-03  2.10285439e-03 -9.41798514e-34]
    Adjusted psuedo R2:  0.573436458549502
    Adjusted D2:  0.880857842507946
    SRMSE:  0.33455359456419226
    Sorensen similarity index:  0.802674531230268
    

* 计算 PCM


```python
prod=Production(flows=T, origins=O, d_vars=W, cost=C, cost_func='exp')
print(prod.params[-10:]) 
print('Adjusted psuedo R2: ', prod.adj_pseudoR2)
print('Adjusted D2: ', prod.adj_D2)
print('SRMSE: ', prod.SRMSE)
print('Sorensen similarity index: ', prod.SSI)
```

    [-1.02279538e+00 -1.08659008e+00 -1.08638777e+00 -1.07315069e+00
     -6.86714736e-01 -6.73693564e-01 -6.61992369e-01 -5.37841158e-03
      2.46504896e-03 -7.26787973e-34]
    Adjusted psuedo R2:  0.727269563162199
    Adjusted D2:  1.0983445508107443
    SRMSE:  0.17668822312452181
    Sorensen similarity index:  0.8931709974634726
    

* 计算 ACM


```python
att=Attraction(flows=T, destinations=D, o_vars=V, cost=C, cost_func='exp')
print (att.params[-10:]) 
print('Adjusted psuedo R2: ', att.adj_pseudoR2)
print('Adjusted D2: ', att.adj_D2)
print('SRMSE: ', att.SRMSE)
print('Sorensen similarity index: ', att.SSI)
```

    [-1.95448402e-01 -1.28132269e-02 -2.25242954e-01 -1.73857612e-01
     -1.82371404e-01 -2.70588242e-02 -4.55830541e-02  3.13965004e-01
      6.36717634e-02 -5.63400200e-34]
    Adjusted psuedo R2:  0.5890179668150771
    Adjusted D2:  0.8968799270754723
    SRMSE:  0.32096483077055943
    Sorensen similarity index:  0.8047194666026806
    

* 计算 DCM


```python
doub=Doubly(flows=T, origins=O, destinations=D,cost=C, cost_func='exp')
print(doub.params[-10:]) 
print('Adjusted psuedo R2: ', doub.adj_pseudoR2)
print('Adjusted D2: ', doub.adj_D2)
print('SRMSE: ', doub.SRMSE)
print('Sorensen similarity index: ', doub.SSI)
```

    [-5.75921223e-02 -5.72823652e-02 -6.18055231e-02 -1.91514557e-01
     -1.16349116e-02 -2.21155673e-01 -1.70075201e-01 -1.78055592e-01
     -2.46477482e-02 -3.43299795e-34]
    Adjusted psuedo R2:  0.743205114160063
    Adjusted D2:  1.1143006951889258
    SRMSE:  0.14480371517338012
    Sorensen similarity index:  0.8905701288173588
    

比较 SIM 4类不同模型的拟合统计，将$R^2$、$R^2_{adj}$、AIC、SRMSE 和 SSI 汇总在一个 DataFrame 格式数据下。从结果表中可以发现，PCM 和 DCM 相对拟合的较好，而 UCM 相对最差，DCM 相对最好。同时可以看到，$R^2$和$R^2_{adj}$值接近，这是因为模型具有非常相似的解释变量数量，从而导致很少或者没有对模型复杂性的惩罚。


```python
R2, adjR2, SSI, SRMSE, AIC=[], [], [], [], []
model_name=['grav', 'prod', 'att', 'doub']
col_names=['R2', 'adjR2', 'AIC', 'SRMSE', 'SSI']
models=[grav, prod, att, doub]

for model in models:
    R2.append(model.pseudoR2)
    adjR2.append(model.adj_pseudoR2)
    SSI.append(model.SSI)
    SRMSE.append(model.SRMSE)
    AIC.append(model.AIC)

cols = {'model_name': model_name,
        'R2': R2,
        'adjR2': adjR2,
        'SSI': SSI,
        'SRMSE': SRMSE,
        'AIC': AIC }
data=pd.DataFrame(cols).set_index('model_name')
data[col_names]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>R2</th>
      <th>adjR2</th>
      <th>AIC</th>
      <th>SRMSE</th>
      <th>SSI</th>
    </tr>
    <tr>
      <th>model_name</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>grav</th>
      <td>0.573439</td>
      <td>0.573436</td>
      <td>2.183933e+06</td>
      <td>0.334554</td>
      <td>0.802675</td>
    </tr>
    <tr>
      <th>prod</th>
      <td>0.727364</td>
      <td>0.727270</td>
      <td>1.396333e+06</td>
      <td>0.176688</td>
      <td>0.893171</td>
    </tr>
    <tr>
      <th>att</th>
      <td>0.589261</td>
      <td>0.589018</td>
      <td>2.104158e+06</td>
      <td>0.320965</td>
      <td>0.804719</td>
    </tr>
    <tr>
      <th>doub</th>
      <td>0.743540</td>
      <td>0.743205</td>
      <td>1.314746e+06</td>
      <td>0.144804</td>
      <td>0.890570</td>
    </tr>
  </tbody>
</table>
</div>



也可以比较指数和幂距离衰减对结果的影响，从结果来看，`pow`方法产生了相对更好的模型拟合。


```python
print('SRMSE for exp distance-decay: ', doub.SRMSE)
pow_doubly=Doubly(T, O, D, C, 'pow')
print('SRMSE for pow distance-decay: ', pow_doubly.SRMSE)
```

    SRMSE for exp distance-decay:  0.14480371517338012
    SRMSE for pow distance-decay:  0.12262615981596203
    

#### 2）局部模型（Local models）

`SpInt`库可指定源汇数据的子集建立“局部”模型校准，以便研究空间相互作用过程如果随空间变化。对 UCM 模型提取局部参数时需要指定`loc_index`和`locs`参数，所获得的结果如下。


```python
local_gravity=grav.local(loc_index=O, locs=np.unique(O))
local_gravity.keys()
```




    dict_keys(['AIC', 'deviance', 'pseudoR2', 'adj_pseudoR2', 'D2', 'adj_D2', 'SSI', 'SRMSE', 'param0', 'stde0', 'pvalue0', 'tvalue0', 'param1', 'stde1', 'pvalue1', 'tvalue1', 'param2', 'stde2', 'pvalue2', 'tvalue2', 'param3', 'stde3', 'pvalue3', 'tvalue3', 'param4', 'stde4', 'pvalue4', 'tvalue4', 'param5', 'stde5', 'pvalue5', 'tvalue5'])




```python
grav.local(loc_index=D, locs=np.unique(D)).keys()
```




    dict_keys(['AIC', 'deviance', 'pseudoR2', 'adj_pseudoR2', 'D2', 'adj_D2', 'SSI', 'SRMSE', 'param0', 'stde0', 'pvalue0', 'tvalue0', 'param1', 'stde1', 'pvalue1', 'tvalue1', 'param2', 'stde2', 'pvalue2', 'tvalue2', 'param3', 'stde3', 'pvalue3', 'tvalue3', 'param4', 'stde4', 'pvalue4', 'tvalue4', 'param5', 'stde5', 'pvalue5', 'tvalue5'])



通过 PCM 和 ACM 的局部模型参数可以推断对应到各自的源汇点或子区域，源汇属性信息对流量的贡献（度）大小。


```python
local_prod = prod.local()
local_prod.keys()
```




    dict_keys(['AIC', 'deviance', 'pseudoR2', 'adj_pseudoR2', 'D2', 'adj_D2', 'SSI', 'SRMSE', 'param0', 'stde0', 'pvalue0', 'tvalue0', 'param1', 'stde1', 'pvalue1', 'tvalue1', 'param2', 'stde2', 'pvalue2', 'tvalue2'])




```python
local_att = att.local()
local_att.keys()
```




    dict_keys(['AIC', 'deviance', 'pseudoR2', 'adj_pseudoR2', 'D2', 'adj_D2', 'SSI', 'SRMSE', 'param0', 'stde0', 'pvalue0', 'tvalue0', 'param1', 'stde1', 'pvalue1', 'tvalue1', 'param2', 'stde2', 'pvalue2', 'tvalue2', 'param3', 'stde3', 'pvalue3', 'tvalue3'])



* 查看汇的两个属性公园面积`prod_area`和设施数量`facilities_count`对应到源子区域上的系数

从打印的结果地图中可以观察到，对于公园面积，城南的系数较小，公园面积的变化对流量不敏感；而城北系数较高，公园面积的变化对流量敏感。从公园面积分布来看，城南的公园面积相对高于城北。对于公园的设施数量，城西系数最小，对流量影响最低；而城东南，系数相对较高，对流量变化敏感。


```python
local_vals_prod=pd.DataFrame({'betas': local_prod['param2'],'prod_area': local_prod['param0'],'prod_facilitycount': local_prod['param1'], 'pop_id':np.unique(O)})
local_vals_prod=pd.merge(zip_pop_gdf,local_vals_prod,on='pop_id')
local_vals_prod.tail(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>objectid</th>
      <th>shape_area</th>
      <th>shape_len</th>
      <th>zip</th>
      <th>geometry</th>
      <th>Geography Type</th>
      <th>Year</th>
      <th>Geography</th>
      <th>Population - Total</th>
      <th>Population - Age 0-17</th>
      <th>...</th>
      <th>Population - Latinx</th>
      <th>Population - Asian Non-Latinx</th>
      <th>Population - Black Non-Latinx</th>
      <th>Population - White Non-Latinx</th>
      <th>Population - Other Race Non-Latinx</th>
      <th>Record ID</th>
      <th>pop_id</th>
      <th>betas</th>
      <th>prod_area</th>
      <th>prod_facilitycount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>237</th>
      <td>61</td>
      <td>167872012.644</td>
      <td>53040.9070778</td>
      <td>60619</td>
      <td>MULTIPOLYGON (((451287.216 4622352.460, 451287...</td>
      <td>Zip Code</td>
      <td>2020</td>
      <td>60619</td>
      <td>61372</td>
      <td>12667</td>
      <td>...</td>
      <td>1050</td>
      <td>173.0</td>
      <td>58359.0</td>
      <td>613</td>
      <td>1177.0</td>
      <td>ZIP_CODE-60619-2020</td>
      <td>pop_237</td>
      <td>-2.282708e-07</td>
      <td>-0.000217</td>
      <td>0.001159</td>
    </tr>
    <tr>
      <th>238</th>
      <td>61</td>
      <td>167872012.644</td>
      <td>53040.9070778</td>
      <td>60619</td>
      <td>MULTIPOLYGON (((451287.216 4622352.460, 451287...</td>
      <td>Zip Code</td>
      <td>2021</td>
      <td>60619</td>
      <td>63481</td>
      <td>13711</td>
      <td>...</td>
      <td>1043</td>
      <td>168.0</td>
      <td>60173.0</td>
      <td>607</td>
      <td>1490.0</td>
      <td>ZIP_CODE-60619-2021</td>
      <td>pop_238</td>
      <td>-2.270953e-07</td>
      <td>-0.000316</td>
      <td>0.001257</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 35 columns</p>
</div>




```python
fig, axes = plt.subplots(1,3, figsize=(20,5))

local_vals_prod.plot(column='betas',cmap='gray',legend=True,ax=axes[0])
park_boundaries_centroid_gdf.plot(ax=axes[0],markersize=15,color='red',marker='+')
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=axes[0])
axes[0].set_title('cost')

local_vals_prod.plot(column='prod_area',cmap='gray',legend=True,ax=axes[1])
park_boundaries_centroid_gdf.plot(column='shape_area',cmap='hot',scheme='FisherJenks',k=10,ax=axes[1],legend=True,legend_kwds={'loc': 'lower left','frameon':False},markersize=8)
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=axes[1])
axes[1].set_title('prod_area')

local_vals_prod.plot(column='prod_facilitycount',cmap='gray',legend=True,ax=axes[2])
park_boundaries_centroid_gdf.plot(column='facilities_count',cmap='hot',scheme='FisherJenks',k=10,ax=axes[2],legend=True,legend_kwds={'loc': 'lower left','frameon':False},markersize=8)
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=axes[2])
axes[2].set_title('prod_facility count')

plt.show()
```


<img src="./imgs/3_8/output_231_0.png" height='auto' width='auto' title="caDesign">    



* 查看源的3个属性`Population - Total`、`Population - Age 0-17`和`Population - Age 65+`对应到汇子区域上的系数

从打印的结果地图中可以观察到，对于总人口，由城南到城北系数逐渐增高，对流量的影响程上升趋势，即城北人口数量的变化对于流量影响高于城南；对于年龄在0~17之间的人口数和高于65岁的人口数，在局部公园的区域上有相对较高的系数值，即该区域的公园服务潜力（流入量）受到人口结构的影响相对较大。


```python
local_vals_att=pd.DataFrame({'betas': local_att['param3'],'Population - Total': local_att['param0'],'Population - Age 0-17': local_att['param1'],'Population - Age 65+': local_att['param2'], 'park_id':np.unique(D)})
local_vals_att=pd.merge(park_boundaries_gdf,local_vals_att,on='park_id')
local_vals_att.tail(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pool_outdo</th>
      <th>golf_cours</th>
      <th>baseball_b</th>
      <th>wheelchr_a</th>
      <th>acres</th>
      <th>bocce_cour</th>
      <th>sled_hill</th>
      <th>artificial</th>
      <th>shape_area</th>
      <th>location</th>
      <th>...</th>
      <th>volleyball</th>
      <th>zoo</th>
      <th>minigolf</th>
      <th>geometry</th>
      <th>facilities_count</th>
      <th>park_id</th>
      <th>betas</th>
      <th>Population - Total</th>
      <th>Population - Age 0-17</th>
      <th>Population - Age 65+</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>615</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>792825.043404</td>
      <td>7000 W IRVING PARK RD</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>MULTIPOLYGON (((433295.937 4645008.936, 433397...</td>
      <td>619</td>
      <td>park_615</td>
      <td>-1.159793e-30</td>
      <td>0.018155</td>
      <td>0.292388</td>
      <td>0.018995</td>
    </tr>
    <tr>
      <th>616</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>17288.053354</td>
      <td>4008 W MADISON ST</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>MULTIPOLYGON (((439763.453 4636859.455, 439764...</td>
      <td>620</td>
      <td>park_616</td>
      <td>-5.850666e-23</td>
      <td>-0.068158</td>
      <td>0.332276</td>
      <td>0.070245</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 87 columns</p>
</div>




```python
fig, axes = plt.subplots(1,4, figsize=(20,5))

local_vals_att.plot(column='betas',cmap='hot',legend=True,ax=axes[0])
zip_pop_controid_gdf.plot(ax=axes[0],markersize=100,color='k',marker='+')
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=axes[0])
axes[0].set_title('cost')

local_vals_att.plot(column='Population - Total',cmap='hot',legend=True,ax=axes[1])
zip_pop_controid_gdf.plot(column='Population - Total',scheme='FisherJenks',k=5,cmap='gray',legend=True,ax=axes[1],marker='+',legend_kwds={'loc': 'lower left','frameon':False},markersize=100)
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=axes[1])
axes[1].set_title('Population - Total')

local_vals_att.plot(column='Population - Age 0-17',cmap='hot',legend=True,ax=axes[2])
zip_pop_controid_gdf.plot(column='Population - Age 0-17',scheme='FisherJenks',k=5,cmap='gray',legend=True,ax=axes[2],marker='+',legend_kwds={'loc': 'lower left','frameon':False},markersize=100)
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=axes[2])
axes[2].set_title('Population - Age 0-17')

local_vals_att.plot(column='Population - Age 65+',cmap='hot',legend=True,ax=axes[3])
zip_pop_controid_gdf.plot(column='Population - Age 65+',scheme='FisherJenks',k=5,cmap='gray',legend=True,ax=axes[3],marker='+',legend_kwds={'loc': 'lower left','frameon':False},markersize=100)
chicago_boundary_gdf.boundary.plot(color='k',linewidth=1,ax=axes[3])
axes[3].set_title('Population - Age 65+')

plt.show()
```


<img src="./imgs/3_8/output_234_0.png" height='auto' width='auto' title="caDesign">    


---

注释（Notes）：

①  GDAL vrt，（<https://gdal.org/drivers/raster/vrt.html>）。

②  Google Colaboratory，CoLab，（<https://colab.research.google.com/?utm_source=scs-index>）。

③  Google Drive，（<https://www.google.com/drive/>）。

④  Planetary Computer，PC，（<https://planetarycomputer.microsoft.com/>）。

⑤  GDAL库首页，（<https://gdal.org/index.html>）。

⑥  Archived: Unofficial Windows Binaries for Python Extension Packages，（<https://www.lfd.uci.edu/~gohlke/pythonlibs/>）。

⑦  TorchGeo，（<https://torchgeo.readthedocs.io/en/stable/api/samplers.html>）。

⑧  eBird，（<https://ebird.org/home>）。

⑨  EBD，（<https://www.gbif.org/occurrence/download?dataset_key=4fa7b334-ce0d-4e88-aaae-2e0c138d049e>）。

⑩  WorldPop，（<https://hub.worldpop.org/>）。

⑪  xarray，（<https://docs.xarray.dev/en/stable/>）。

⑫  rioxarray，（<https://corteva.github.io/rioxarray/stable/>）。

⑬  DataArray，（<https://docs.xarray.dev/en/stable/user-guide/data-structures.html>）。

⑭  kneed，（<https://pypi.org/project/kneed/>）。

⑮  SciKit GStat，（<https://scikit-gstat.readthedocs.io/en/latest/index.html>）。

⑯  Pyinterpolate，（<https://pyinterpolate.readthedocs.io/en/latest/index.html>）。

⑰  Chicago Data Portal，CDP，（<https://data.cityofchicago.org/>）。

⑱  SpInt（PySAL），（<https://pysal.org/notebooks/model/spint/intro.html>）。

⑲  布鲁克林大桥骑车人计数数据集（骑行计数），（<https://gist.github.com/sachinsdate/c17931a3f000492c1c42cf78bf4ce9fe>）。

⑳  statsmodels，（<https://www.statsmodels.org/dev/glm.html>）。

㉑  SpGLM，（<ttps://github.com/pysal/spglm>）。

参考文献（References）:

[1] VRT -- GDAL Virtual Format, <https://gdal.org/drivers/raster/vrt.html#vrt-gdal-virtual-format>.

[2] About eBird, <https://ebird.org/about>.

[3] What is WorldPop?, <https://www.worldpop.org/>.

[4] Stevens FR, Gaughan AE, Linard C, Tatem AJ (2015) Disaggregating Census Data for Population Mapping Using Random Forests with Remotely-Sensed and Ancillary Data. PLoS ONE 10(2): e0107042. https://doi.org/10.1371/journal.pone.0107042

[5] Allen, T. F. H., & Starr, T. B. (1982). Hierarchy: Perspectives for Ecological Complexity. Retrieved from https://api.semanticscholar.org/CorpusID:55743947

[6] Matheron, Georges (1963). "Principles of geostatistics". Economic Geology. 58 (8): 1246–1266. doi:10.2113/gsecongeo.58.8.1246. ISSN 1554-0774

[7] Matheron, G. (1962): Traité de Géostatistique Appliqué, Tonne 1. Memoires de Bureau de Recherches Géologiques et Miniéres, Paris.

[8] Matheron, G. (1965): Les variables regionalisées et leur estimation.Editions Masson et Cie, 212 S., Paris.]

[9] Cressie, N., and D. Hawkins (1980): Robust estimation of the variogram. Math. Geol., 12, 115-125.

[10] Dowd, P. A., (1984): The variogram and kriging: Robust and resistant estimators, in Geostatistics for Natural Resources Characterization. Edited by G. Verly et al., pp. 91 - 106, D. Reidel, Dordrecht.

[11] Genton, M. G., (1998): Highly robust variogram estimation, Math. Geol., 30, 213 - 221.

[12] scikit-gstat-skgstat-estimators.py, <https://github.com/mmaelicke/scikit-gstat/blob/main/skgstat/estimators.py>.

[13] Chen, L., Gao, Y., Zhu, D., Yuan, Y., & Liu, Y. (2019). Quantifying the scale effect in geospatial big data using semi-variograms. PLOS ONE, 14(11), e0225139. doi:10.1371/journal.pone.0225139

[14] Burgess, T. M., & Webster, R. (1980). Optimal interpolation and isarithmic mapping of soil properties. I.The semi-variogram and punctual kriging. Journal of Soil and Science, 31(2), 315–331, http://doi.org/10.1111/j.1365-2389.1980.tb02084.x

[15] Cressie, N. (1993): Statistics for spatial data. Wiley Interscience.

[16] Chiles, J.P., Delfiner, P. (1999). Geostatistics. Modeling Spatial Uncertainty. Wiley Interscience.

[17] Journel, A G, and Huijbregts, C J. Mining geostatistics. United Kingdom: N. p., 1976.

[18] Montero, J.-M., Mateu, J., & others. (2015). Spatial and spatio-temporal geostatistical modeling and kriging (Vol. 998). John Wiley & Sons.

[19] Zimmermann, B., Zehe, E., Hartmann, N. K., & Elsenbeer, H. (2008). Analyzing spatial data: An assessment of assumptions, new methods, and uncertainty using soil hydraulic data. Water Resources Research,44(10), 1–18. https://doi.org/10.1029/2007WR006604

[20] scikit-gstat-skgstat-models.py, <https://github.com/mmaelicke/scikit-gstat/blob/main/skgstat/models.py>.

[21] Fotheringham, A. Stewart. (1989). Spatial interaction models : formulations and applications. Dordrecht ; Boston :Kluwer Academic Publishers

[22] Fotheringham, A.S. and R. Flowerdew (1988) Comments on the spatial choices of British migrants, manuscript

[23] Taylor M Oshan, 2016. "A primer for working with the Spatial Interaction modeling (SpInt) module in the python spatial analysis library (PySAL)," REGION, European Regional Science Association, vol. 3, pages 11-23.

[24] Wilson, A. G. (1971). A Family of Spatial Interaction Models, and Associated Developments. Environment and Planning A: Economy and Space, 3(1), 1–32. doi:10.1068/a030001 

[25] Flowerdew, R., & Aitkin, M. (1982). A METHOD OF FITTING THE GRAVITY MODEL BASED ON THE POISSON DISTRIBUTION*. Journal of Regional Science, 22(2), 191–202. doi:10.1111/j.1467-9787.1982.tb00744.x 

[26] Flowerdew, R., & Lovett, A. (1988). Fitting Constrained Poisson Regression Models to Interurban Migration Flows. Geographical Analysis, 20(4), 297–307. doi:10.1111/j.1538-4632.1988.tb00184.x

[27] Dunteman, G. H. and Ho., M. R., 2006. An Introduction to Generalized Linear Models. Quantitative Applications in the Social Sciences. Thousand Oaks, CA: SAGE Publications, Inc.

[28] Generalized linear model, Wikipedia, <https://en.wikipedia.org/wiki/Generalized_linear_model>.

[29] The Poisson Regression Model, <https://timeseriesreasoning.com/contents/poisson-regression-model/>
