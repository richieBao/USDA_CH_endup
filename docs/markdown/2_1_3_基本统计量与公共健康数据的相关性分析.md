> Last updated on Sat Oct 22 2022 @author: Richie Bao

## 2.1.3 基本统计量与公共健康数据的相关性分析

### 2.1.3.1  基本统计量：标准误、中心极限定理、t分布、统计显著性、效应量和置信区间 <sup>[1][2]</sup>

#### 1）标准误

定义：标准误是某一统计量（例如均值、两个均值只差、相关系数等）抽样分布（sampling distribution）的标准差（即样本均值的标准差，而不是样本的标准差），度量了从同一总体中抽取相同容量样本的预期随机差异。在下述代码中，从服从平均值为30，标准差为5的正态分布中，随机提取2000个样本（sample），各个样本容量为1000，并计算每一样本的均值，查看这2000个样本的均值分布，即均值抽样分布（sampling distribution of the mean）。普通分布具有均值和标准差，在均值的抽样分布中，均值则称为均值的期望值（expected value of the mean），这是因为样本均值的最佳猜测与总体均值一致。下述代码中计算了均值抽样分布的均值几乎与总体（population）一样，证明了这一点。均值抽样分布的标准差则称为标准误。标准差是分布中的单个取值与分布均值之间的平均差异或平均离差，均值的标准误提供了同样的信息，只是单个样本均值与其期望值之间的平均差异，可以理解为对样本均值代表实际总体均值的确信程度，有助于确定样本统计值（例如样本均值）与总体参数（例如总体均值）之间的差异是否有意义。同时计算了均值抽样分布标准计分的K-S test正态性检验，每次代码运行产生的数据是符合上述条件下的随机值，其p-value是变化的，但通常高于0.5，即>0.05，因此可以判断均值抽样分布符合正态分布。

均值标准误的计算公式：

 $\sigma _{ \overline{x} }= \frac{ \sigma }{ \sqrt{n} } $或 $S _{ \overline{x} }= \frac{s }{ \sqrt{n} }$，式中$\sigma$为总体标准差，$s$为标准差的样本估计，$n$为样本容量。因为通常$\sigma$总体的标准差未知，因此用标准差的样本估计，计算标准误。


```python
import numpy as np
from scipy import stats
import seaborn as sns
import math

sns.set()
mu,sigma=30,5
sample_size=1000
sample_mus=np.array([np.random.normal(mu, sigma, sample_size).mean() for i in range(2000)]) # 从服从平均值为30，标准差为5的正态分布中，随机提取2000个样本，每个样本容量为1000的样本，并计算每一样本的均值
bins=30
sns.histplot(sample_mus,bins=bins,kde=True,stat="density", linewidth=0) # 查看2000个样本均值分布
print("sample_mus mu:%.2f,sigma:%.2f"%(sample_mus.mean(),sample_mus.std()))
kstest_test=stats.kstest(stats.zscore(sample_mus),cdf='norm')
print("sampling distribution of the mean - K-S test statistic:%.2f,p-value:%.2f"%kstest_test)
print("standard error of mean:",sample_mus.std()/math.sqrt(sample_size) ) # 计算标准误
print("standard error of mean_scipy.stats.sem():",stats.sem(sample_mus,ddof=0)) # 使用scipy.stats库直接计算标准误
```

    sample_mus mu:30.00,sigma:0.16
    sampling distribution of the mean - K-S test statistic:0.02,p-value:0.41
    standard error of mean: 0.0049183131322453714
    standard error of mean_scipy.stats.sem(): 0.0034777725678095507
    


<img src="./imgs/2_1_3/output_2_1.png" height='auto' width='auto' title="caDesign">   
    


单个样本的容量越大，就越接近于总体，均值抽样分布的均值越趋近于总体均值，而其标准差即标准误越小，表明样本均值的统计量（或计算其它的统计量）能够代表该总体的统计量，对总体的估计也就越准确。利用下述代码分析上述观点，计算多个不用样本容量均值抽样分布的标准误，随着样本容量的增加，标准误迅速的减小后逐步趋于缓和，说明单个样本容量越大，越接近总体，对总体的估计越准确。


```python
import matplotlib.pyplot as plt 

sample_mu_list=[]
sample_sigma_list=[]
sample_size_list=list(range(10,2000,20))
for sample_size in sample_size_list:
    sample_size_mu=np.array([np.random.normal(mu, sigma, sample_size).mean() for i in range(2000)])
    sample_mu_list.append(sample_size_mu.mean())
    sample_sigma_list.append(sample_size_mu.std())
ax=sns.lineplot(x=sample_size_list,y=sample_sigma_list)  
ax.set(xlabel='sample_size',ylabel='sample_sigma')
plt.show()
```


<img src="./imgs/2_1_3/output_4_0.png" height='auto' width='auto' title="caDesign">   


#### 2）中心极限定理

中心极限定义（central limit theorem）表明，只要样本容量足够大，即使样本取值的总体分布不是正态分布，均值的抽样分布也服从正态分布。或理解为不管样本总体服从什么分布，当样本数量足够大时，样本的均值以正态分布的形式围绕总体均值波动。为验证中心极限，使用百度POI数据，提取美食的价格数据。在正态分布与概率密度函数一节，已经通过正态性检验确定了该美食价格数据集为非正态分布，下述代码重现了K-S test正态性检验，p-value=5.153632014592587e-26，小于0.05，拒绝原假设，即不符合正态分布。从该数据集中随机抽取单个样本容量为350，总共2000个样本，计算均值抽样分布后的K-S test正态性检验，其值p-value通常大于0.5，表明均值抽样分布为正态分布，满足中心极限定义。

很多统计量都依赖于从正态分布中得到概率，而中心极限定理确定了足够多的样本均值服从正态分布（即使总体分布不是正态的），即均值抽样分布为正态分布，因此这一定理使得众多统计量的概率计算成为可能。


```python
import util_misc
import pandas as pd

poi_gpd=pd.read_pickle('./data/poisInAll/poisInAll_gdf.pkl') # 读取已经存储为.pkl格式的POI数据，其中包括geometry字段，为GeoDataFrame地理信息数据，可以通过poi_gpd.plot()迅速查看数据。
delicacy_price=poi_gpd.xs('poi_0_delicacy',level=0).detail_info_price  # 提取美食价格数据
delicacy_price_df=delicacy_price.to_frame(name='price').astype(float) 
delicacy_price_df_clean=delicacy_price_df.dropna()
_,delicacyPrice_outliersDrop=util_misc.is_outlier(delicacy_price_df_clean.price,threshold=3.5) # 将异常值处理函数放置于util_misc.py文件中，直接调用。见异常值处理一节

delicacy_price_array_dropna=delicacyPrice_outliersDrop.to_numpy().reshape(-1)
print("original data mean:%.2f"%delicacy_price_array_dropna.mean())

print("original data K-S test:",stats.kstest(stats.zscore(delicacy_price_array_dropna),cdf='norm')) # 计算标准化后的K-S test正态性检验

delicacy_price_sample_mus=np.array([np.random.choice(delicacy_price_array_dropna,350).mean() for i in range(2000)])
sns.histplot(delicacy_price_sample_mus,bins=bins,kde=True,stat="density", linewidth=0)
print("sample_mus mu:%.2f,sigma:%.2f"%(delicacy_price_sample_mus.mean(),delicacy_price_sample_mus.std()))
kstest_test=stats.kstest(stats.zscore(delicacy_price_sample_mus),cdf='norm')
print("sample_mus K-S test statistic:%.2f,p-value:%.2f"%kstest_test)
```

    original data mean:47.70
    original data K-S test: KstestResult(statistic=0.11464960111854455, pvalue=5.153632014592587e-26)
    sample_mus mu:47.76,sigma:1.69
    sample_mus K-S test statistic:0.01,p-value:0.76
    


<img src="./imgs/2_1_3/output_6_1.png" height='auto' width='auto' title="caDesign">    
   


#### 3）t分布 (Student's t-distribution)

如果要使用正态分布，通过z-score（标准计分）获得精确概率，至少需要满足两个条件，一个是，$\sigma$总体标准差已知；另一个是，大样本（即样本容量足够大）。如果上述条件均不满足，则需要考虑样本容量，这就用到t分布。t分布的形状受样本容量影响，大样本条件下，t分布与正态分布的形状几何完全一样，而随样本容量减少，t分布的形状变得中间更平坦，两端更粗厚，即均值周围的取值变得更少，而远离均值，位于分布尾部的取值变得更多。以`SciPy`库[scipy.stats.t](https://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.stats.t.html)<sup>①</sup>给出的案例来观察t分布，其代码方法与正态分布基本相同，可以互相比较查看之间的差异。

t分布中，增加了一个新的参数自由度（degree of freedom，df），通常用符号$\nu $来表示，当以样本的统计量来估计总体的参数时，对N个随机样本而言，其自由度为N-1。

数据集为正态分布，给定一个取值，计算该取值的z-score后，就可以获得该取值的概率。而在t分布下，要获取对应取值的概率，则需要计算t值（t统计量），其公式定义为：$t= \frac{ \overline{X} - \mu }{S _{ \overline{x} }} $，式中$\mu$为总体均值，$\overline{X}$为样本均值，${S _{ \overline{x} }}$为标准误的样本估计。知道t值，及自由度（样本容量N-1），可以查t值表得到该取值的概率。当然，目前直接使用`SciPy`等库计算，无需再计算t值再对应查表。


```python
from scipy.stats import norm
from scipy.stats import t
import matplotlib.pyplot as plt
import numpy as np

fig, ax=plt.subplots(1, 1)
df=2.75 # 配置自由度
mean, var, skew, kurt=t.stats(df, moments='mvsk') # 查看服从t分布的相关统计量
print('mean, var, skew, kurt=',(mean, var, skew, kurt))

x=np.linspace(t.ppf(0.01, df),t.ppf(0.99, df), 100) # 获取服从自由度df，概率位于1%到99%的100个取值
ax.plot(x, t.pdf(x, df),'r-', lw=5, alpha=0.6, label='t_%.2f pdf'%df)

rv=t(df) # 指定固定自由度的随机变量（random variable）序列
ax.plot(x, rv.pdf(x), 'k-', lw=2, label='t_%.2f frozen pdf'%df)

rv_df_02=t(0.2) # 指定固定自由度为0.2的随机变量序列
ax.plot(x, rv_df_02.pdf(x), 'g-', lw=2, label='t_0.2 pdf')

vals=t.ppf([0.001, 0.5, 0.999], df) # 返回概率为0.1%、50%和99.9%的取值
print("验证累积分布函数CDF返回值与其PPF返回值是否相等或近似：",np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df)))

rv_1000=t.rvs(df, size=1000) # 获取服从自由度df的1000个随机取值
ax.hist(rv_1000, density=True, histtype='stepfilled', alpha=0.2)

# 比较标准正态分布
x_nd=np.linspace(norm.ppf(0.01),norm.ppf(0.99), 100)
ax.plot(x_nd, norm.pdf(x_nd), 'b--', lw=2, alpha=0.8, label='norm pdf')
ax.legend(loc='best', frameon=False)
plt.show()
```

    mean, var, skew, kurt= (array(0.), array(3.66666667), array(nan), array(inf))
    验证累积分布函数CDF返回值与其PPF返回值是否相等或近似： True
    


<img src="./imgs/2_1_3/output_8_1.png" height='auto' width='auto' title="caDesign">    


```python
# 如果需要计算服从t分布的概率则指定df，及loc（均值，默认为0）和scale(标准差，默认为1)
print("用.cdf计算值小于或等于113的概率为：",t.cdf(113,df=999,loc=100,scale=12)) 
print("用.sf计算值大于或等于113待概率为：",t.sf(113,df=999,loc=100,scale=12)) 
print("可以观察到.cdf（<=113）概率结果+.sf(>=113)概率结果为：",t.cdf(113,df=999,loc=100,scale=12)+t.sf(113,df=999,loc=100,scale=12))
print("用.ppf找到给定概率值为98%的数值为：",t.ppf(0.81766,df=999,loc=100,scale=12))
```

    用.cdf计算值小于或等于113的概率为： 0.8605390547558804
    用.sf计算值大于或等于113待概率为： 0.13946094524411956
    可以观察到.cdf（<=113）概率结果+.sf(>=113)概率结果为： 1.0
    用.ppf找到给定概率值为98%的数值为： 110.88276310459135
    

#### 4）统计显著性
使用`SciPy`库建立服从均值为100，标准差为12的正态分布。绘制样本容量为1000的单个样本的随机取值分布；同时绘制样本容量为1000，样本数为2000的均值抽样分布。计算2000个样本均值抽样分布的标准差，将其作为绘制t分布的标准差，均值保持不变为100，自由度为1999（即样本容量为2000，等于均值抽样分布的样本数量）。观察均值抽样分布与对应的t分布（同均值和标准差，及自由度为均值抽样分布的样本数），曲线形状基本保持一致，再一次验证中心极限定理。在这样本容量为1000的2000个样本中，可以依据t分布估计样本均值概率小于0.05（5%）的取值为 99.387，大于0.95（95%）的取值为100.641。就此我们能够估计取值小于99.387或者大于100.641的样本均值出现的概率（机率）小于5%（0.05）。

如果我们不是根据概率获取对应的取值（即样本均值），而是估计取值的概率，例如估计样本均值小于或者等于99.0的概率为多少，计算结果为0.00439。这个概率值是否能确定样本统计量（例如样本均值）与总体参数（例如总体均值）之间某种差异（例如样本均值为99.0，而总体均值为100，100-99.0=1.0的差异），是否仅仅由随机抽样误差或偶然因素导致？惯例是取概率P值p-value=0.05作为一个水平的界限。这一水平就为，当原假设为真时所得到的样本观察结果或更极端结果出现的概率，其P值很小，说明原假设发生的概率很小，根据小概率原理，0.00439<0.05，有理由拒绝原假设。P值越小，拒绝原假设的理由越充分，就是说样本均值为99.0与总体均值之间的差异不是偶然的，得出结论认为99.0的样本均值实际上异于总计均值，从该样本中观察到的相关情况不能代表总体中的实际现象。

对于上述过程，可转换为假设检验的推断过程，提出一种假设并确立一种准则，用于决定保留或拒绝假设。假设样本均值不异于总体均值为原假设即零假设（null hypothesis，$H_{0} $），零假设一般指总体中的效应不存在（总体均值与样本均值不会不同），符号表示为：$H_{0} : \mu = \overline{X} $，其中$ \mu$为总体均值，$\overline{X}$为样本均值。如果假设样本均值异于总体均值，则为对立假设（一种替代假设），符号表示为：$H_{A} : \mu  \neq  \overline{X} $。样本均值趋向于等同总体均值，但是因为毕竟不是总体，之间总会存在差异，引起这个差异的原因就是随机抽样误差或偶然因素，这正是均值抽样分布呈正态性的内在原因。其p-value<=0.05的概率对应着显著性水平（α，alpha level），例如样本均值99.0的概率为0.00439，小于显著性水平α，拒绝零假设，样本均值异于总体均值，其结果是统计显著的（指在原假设为真的条件下，用于检验的样本统计量的值落在了拒绝域内，做出了拒绝原假设的决定）。显著性水平常用值的除了0.05，还有0.01。

在上述计算过程中，计算有概率小于0.025，和大于0.975的两种情况，如果同时包括两种情况，则为双尾检测，仅有一种情况，但是概率需要小于0.05或者大于0.95的情况下为单尾检测。


```python
import matplotlib.pyplot as plt
df,loc,scale,sample_size=1999,100,12,1000

# 绘制样本容量为1000的单个样本的随机取值分布
x=np.linspace(norm.ppf(0.001, loc=loc,scale=scale),norm.ppf(0.999, loc=loc,scale=scale), sample_size)
fig, axs=plt.subplots(1,3,figsize=(27,5))
axs[0].plot(x, norm.pdf(x,loc=loc,scale=scale),'r-', lw=2, alpha=0.8, label='nd-the distribution \n of individual sample values')
axs[0].legend(loc='upper left', frameon=False)

# 绘制样本容量为1000，样本数为2000的均值抽样分布
samples=np.array([norm.rvs(loc=loc,scale=scale,size=sample_size).mean() for i in range(df+1)])
bins=30
sns.histplot(samples,bins=bins,ax=axs[1],label='sampling distribution of the mean',kde=True,stat="density",linewidth=0)
axs[1].legend(loc='upper left', frameon=False)

# 计算2000个样本均值抽样分布的标准差，作为绘制t分布的标准差，均值保持保持保持不变为100，自由度为1999（即样本容量为2000，等于均值抽样分布的样本数量）
samples_std=samples.std()
t_x=np.linspace(t.ppf(0.001, df=df,loc=loc,scale=samples_std),t.ppf(0.999, df=df,loc=loc,scale=samples_std), df+1)  # 获取服从自由度df，概率位于1%到99%的100个取值
t_rv=t(df=df,loc=loc,scale=samples_std) # 指定固定自由度
axs[2].plot(t_x, t_rv.pdf(t_x), 'g-', lw=2, label='t-$mu$=%d,sigma=%d'%(loc,scale))

# 显著性水平为0.05时，曲线区间绘制
pValue_5percent=t.ppf(0.025,df=df,loc=loc,scale=samples_std)
pValue_95percent=t.ppf(0.975,df=df,loc=loc,scale=samples_std)
axs[2].axvline(pValue_5percent, 0,0.3,c='gray',label='alpha level')
axs[2].axvline(pValue_95percent, 0,0.3,c='gray')
axs[2].annotate('reject null hypothesis,a=%.3f'%0.025,xy=(pValue_5percent,0.3),xytext=(pValue_5percent-0.1, 0.5),arrowprops=dict(facecolor='black', shrink=0.01),horizontalalignment='right',)
axs[2].annotate('reject null hypothesis,a=%.3f'%0.025,xy=(pValue_95percent,0.3),xytext=(pValue_95percent+0.07, 0.55),arrowprops=dict(facecolor='black', shrink=0.01),horizontalalignment='left',)
axs[2].legend(loc='upper left', frameon=False)

plt.show()

print("用.ppf找到给定概率值为5%的数值为：",pValue_5percent)
print("用.ppf找到给定概率值为95%的数值为：",pValue_95percent)
print("用.cdf计算值小于或等于99的概率为：",t.cdf(99.0,df=df,loc=loc,scale=samples_std))
```


<img src="./imgs/2_1_3/output_11_0.png" height='auto' width='auto' title="caDesign">   
    


    用.ppf找到给定概率值为5%的数值为： 99.24538217329646
    用.ppf找到给定概率值为95%的数值为： 100.75461782670354
    用.cdf计算值小于或等于99的概率为： 0.004710985065683475
    

#### 5）效应量
效应量（effect size）是量化现象强度的数值。其绝对值越大表示效应越强，也就是现象越明显。对于均值抽样分布，效应量的计算公式为：$d=\frac{ \overline{X} - \mu }{s}$，式中$d$为效应量，$ \overline{X}$为样本均值，$\mu$为总体均值，$s$为标准差的样本估计。效应量所代表的就是以标准差为单位所度量的差异，这个与标准计分（z-score）非常相似，标准计分是以标准差为单位，从感兴趣的点到均值之间有多少个标准差。关于效应量是否有意义时，所建议的效应量值的区间也所有差异，关键是检验什么以及所持观点，一般而言效应量小于0.20算小，在0.25~0.75之间算中等，超过0.80算大。对于样本均值99的效应量计算结果为-2.583，其绝对值大于0.8，表明样本均值99到总体均值100之间所差的100-99=1的差异，按照标准差为单位，效应量显著，即样本均值99远离总体均值100。


```python
print("样本均值99的效应量：",(99-100)/samples_std)
```

    样本均值99的效应量： -2.5988670698883993
    

#### 6）置信区间
一个概率样本的置信区间（Confidence interval，CI），是对产生这个样本的总体的参数分布（Parametric Distribution）中的某一个位置参数值，以区间的形式给出估计。对于现实中均值抽样分布，实际上总体的均值（总体参数的实际值）并不知道，我们所拥有的只是样本数据，而通过样本数据是可以估计总体均值，给出总体均值分布的区间，这个区间即为置信区间。计算公式为：$C I_{95}= \overline{X}   \pm ( t_{95} )( s_{ \overline{X} })$ 和$C I_{99}= \overline{X}   \pm ( t_{99} )(s_{ \overline{X} } )$，式中$C I_{95}$为95%置信区间； $C I_{99}$为99%置信区间； $\overline{X}$为样本均值； $ s_{ \overline{X} }$；为标准误； $t_{95} $为给定自由度条件下，$a$水平为0.05双尾检验所对应的t值； $t_{99} $为给定自由度条件下，$a$水平为0.01双尾检验所对应的t值。

对于置信区间的计算可以直接使用`SciPy`库。


```python
print("0.05_置信区间：",stats.t.interval(0.95, len(samples)-1, loc=np.mean(samples), scale=stats.sem(samples)))
print("0.01_置信区间：",stats.t.interval(0.99, len(samples)-1, loc=np.mean(samples), scale=stats.sem(samples)))
```

    0.05_置信区间： (99.98972207782829, 100.02347805302809)
    0.01_置信区间： (99.98441087421692, 100.02878925663946)
    

### 2.1.3.2 相关性

如果要判断两个变量之间是否相互联系，则需要计算相关系数。在变量之间相关性分析时，涉及到三种情况，包括数值数据与数值数据、数值数据与分类数据，及分类数据与分类数据。因此不同的变量类型之间的相关性计算指标有所差异：

| 数据类型  | 指标  | 值的范围  | 计算公式  | 说明  |
|---|---|---|---|---|
| 数值数据和数值数据  | 相关系数  |  -1~1 |$ r=\frac{ S_{xy} }{ \sqrt{ S_{xx}\cdot S_{yy} } } = \frac{ \sum_{i=1}^n ( x_{i}-  \overline{x}   )( y_{i}- \overline{y}  ) }{  \sqrt{ \sum_{i=1}^n ( x_{i} -\overline{x})^{2} } \cdot \sum_{i=1}^n ( y_{i} -\overline{y})^{2} }  \in [-1,1] $ |  式中$S_{xx}$叫做$x$的方差（Variance），$S_{yy}$ 叫做$y$的方差，$S_{xy}$叫做$x$和$y$的协方差（Covariance）|
| 数值数据和分类数据  | 相关比  | 0~1  |  $WSS= \sum_{k=1}^K S_{ x^{k}  x^{i} } = \sum_{k=1}^K  \sum_{i=1}^{ n_{i} }  ( x_i^k - \overline{ x^{k} }  ) ^{2}  $ $BSS= \sum_{k=1}^K n_{k}  ( \overline{ x^{k} } - \overline{x} )^{2}  $ $cr=\frac{BSS}{BSS+WSS} $ | 设共有n个数值数据，它们被分成了K个类别，$x_1^1,x_2^1, \cdots, x_{ n_{1} } ^1;x_1^2,x_2^2, \cdots, x_{ n_{2} } ^2;\cdots;x_1^K,x_2^K, \cdots, x_{ n_{K} } ^K$,式中$n_{K} $代表第$K$个类别的数值的个数，记$ \overline{ x^{k}}$为第K个类别数值的均值，$\overline{x}$为所有数据的均值。WSS（within sum of squares）为组内变异 ，BSS（between sum of squares）为组间变异 |
| 分类数据和分类数据  | 克莱姆相关系数  | 0~1  | $ \chi ^{2} = \sum_{i,j}^{}  \frac{ ( n_{ij}- \frac{ n_{i}  n_{j} }{n}  )^{2} }{ \frac{ n_{i}  n_{j} }{n} } $ $V= \sqrt{ \frac{  \chi ^{2}/n }{min(k-1,r-1)} } $ | 设有两个类别变量A和B，观测样本总数为$n$，对于$i=1,\cdots, r;j=1,\cdots,k $， $n_{ij}$ 为$( A_{i}, B_{j}  )$的观测次数（频数），$k$为观察次数表列，$r$为其行。$ \frac{ n_{i}  n_{j} }{n}$为计算期望次数|

对于数值数据和数值数据之间的相关性分析，使用相关系数计算（通常为Pearson's r，即皮尔森相关系数 r，为线性相关）。先建立简单数据集，数据来源于《漫画统计学》询问10名20多岁女性的化妆品费和置装费<sup>[2]</sup>。待分析的两个变量为x和y值，打印散点图观察点的分布是否存在规律，可以比较明显的观察到点的分布似乎沿一条隐藏的倾斜直线分布，可以初步判断化妆品费和置装费之间存在关联。

> 皮尔森相关系数，又称积差相关系数、积矩相关系数（Pearson product-moment correlation coefficient，PPMCC或PCCs）


```python
import pandas as pd

dressUp_cost={'name':["miss_A","miss_B","miss_C","miss_D","miss_E","miss_F","miss_G","miss_H","miss_I","miss_J"],
              "cosmetics_fee":[3000,5000,12000,2000,7000,15000,5000,6000,8000,10000],
             "clothes_fee":[7000,8000,25000,5000,12000,30000,10000,15000,20000,18000]
             }
dressUp_cost_df=pd.DataFrame.from_dict(dressUp_cost)
dressUp_cost_df.plot.scatter(x='cosmetics_fee',y='clothes_fee',c='DarkBlue')
plt.show()
```


<img src="./imgs/2_1_3/output_18_0.png" height='auto' width='auto' title="caDesign">    
   


使用上述公式计算相关系数。

> 人工计算的过程有助于理解概念及公式。


```python
import math

dressUp_cost_df["xSx_mean"]=dressUp_cost_df.cosmetics_fee.apply(lambda row: row-dressUp_cost_df.cosmetics_fee.mean()) 
dressUp_cost_df["ySy_mean"]=dressUp_cost_df.clothes_fee.apply(lambda row: row-dressUp_cost_df.clothes_fee.mean())
dressUp_cost_df["square_xSx_mean"]=dressUp_cost_df.xSx_mean.apply(lambda row: math.pow(row,2))
dressUp_cost_df["square_ySy_mean"]=dressUp_cost_df.ySy_mean.apply(lambda row: math.pow(row,2))
dressUp_cost_df["xSx_meanMySy_mean"]=dressUp_cost_df.xSx_mean*dressUp_cost_df.ySy_mean
r=dressUp_cost_df.xSx_meanMySy_mean.sum()/math.sqrt(dressUp_cost_df.square_xSx_mean.sum()*dressUp_cost_df.square_ySy_mean.sum())
print("Pearson's r:",r)

dressUp_cost_df
```

    Pearson's r: 0.968019612860768
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>cosmetics_fee</th>
      <th>clothes_fee</th>
      <th>xSx_mean</th>
      <th>ySy_mean</th>
      <th>square_xSx_mean</th>
      <th>square_ySy_mean</th>
      <th>xSx_meanMySy_mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>miss_A</td>
      <td>3000</td>
      <td>7000</td>
      <td>-4300.0</td>
      <td>-8000.0</td>
      <td>18490000.0</td>
      <td>64000000.0</td>
      <td>34400000.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>miss_B</td>
      <td>5000</td>
      <td>8000</td>
      <td>-2300.0</td>
      <td>-7000.0</td>
      <td>5290000.0</td>
      <td>49000000.0</td>
      <td>16100000.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>miss_C</td>
      <td>12000</td>
      <td>25000</td>
      <td>4700.0</td>
      <td>10000.0</td>
      <td>22090000.0</td>
      <td>100000000.0</td>
      <td>47000000.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>miss_D</td>
      <td>2000</td>
      <td>5000</td>
      <td>-5300.0</td>
      <td>-10000.0</td>
      <td>28090000.0</td>
      <td>100000000.0</td>
      <td>53000000.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>miss_E</td>
      <td>7000</td>
      <td>12000</td>
      <td>-300.0</td>
      <td>-3000.0</td>
      <td>90000.0</td>
      <td>9000000.0</td>
      <td>900000.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>miss_F</td>
      <td>15000</td>
      <td>30000</td>
      <td>7700.0</td>
      <td>15000.0</td>
      <td>59290000.0</td>
      <td>225000000.0</td>
      <td>115500000.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>miss_G</td>
      <td>5000</td>
      <td>10000</td>
      <td>-2300.0</td>
      <td>-5000.0</td>
      <td>5290000.0</td>
      <td>25000000.0</td>
      <td>11500000.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>miss_H</td>
      <td>6000</td>
      <td>15000</td>
      <td>-1300.0</td>
      <td>0.0</td>
      <td>1690000.0</td>
      <td>0.0</td>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>miss_I</td>
      <td>8000</td>
      <td>20000</td>
      <td>700.0</td>
      <td>5000.0</td>
      <td>490000.0</td>
      <td>25000000.0</td>
      <td>3500000.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>miss_J</td>
      <td>10000</td>
      <td>18000</td>
      <td>2700.0</td>
      <td>3000.0</td>
      <td>7290000.0</td>
      <td>9000000.0</td>
      <td>8100000.0</td>
    </tr>
  </tbody>
</table>
</div>



使用`SciPy`库直接计算，其结果保持一致。


```python
from scipy import stats

r_=stats.pearsonr(dressUp_cost_df.cosmetics_fee,dressUp_cost_df.clothes_fee)
print(
    "pearson's r:",r_[0],"\n",
    "p_value:",r_[1]
     )
```

    pearson's r: 0.968019612860768 
     p_value: 4.402991448166131e-06
    

相关系数的强弱并没有严格的规定，通常需要根据具体的应用背景和目的确定，例如复杂多变因素影响的变量，0.9的值则是相当高的。对于相关系数值的意义不同的文献给出的参考意义也有所差别，这里给出《漫画统计学》里的划分：

| 相关系数的绝对值  | 若细分  | 若大体上划分  |  
|---|---|---|
| 1.0~0.9  |  非常强 |  相关 |   
| 0.9~0.7  | 有点强  |  相关 |   
|0.7~0.5   | 有点弱  |  相关 |   
| 未满0.5  |  非常弱 | 不相关  |   


及Wikipedia给出的参考：

| 相关性  | 负  | 正  |  
|---|---|---|
| 无  | −0.09 to 0.0 | 0.0 to 0.09  |   
| 弱  | −0.3 to −0.1  | 0.1 to 0.3 |   
|中   |−0.5 to −0.3  | 0.3 to 0.5 |   
| 强  | −1.0 to −0.5 |0.5 to 1.0   | 

若相关系数接近$ \pm1 $，则相关性越强；如果接近0，则相关性越弱。如果为正，则为正相关；如果为负，则为负相关。

通过计算相关系数，获得结果r=0.968，说明抽样的10名20多岁女性在化妆品费和置装费这两个变量间存在强相关性，那么该值是否代表其抽样的总体（所有女性）中两个变量（化妆品费和置装费）间也存在非常强的相关关系？为了确认这个现象，需要检验相关系数是否统计显著。提出零假设，代表总体下两个变量（化妆品费和置装费）之间完全无关，即总体的相关系数为0。通常用t分布来检验相关系数是否统计显著，进行t检验。其t值的计算公式为：$t=r \sqrt{ \frac{N-2}{1- r^{2} } } $，式中自由度为$N-2$，即样本对象数减去2，即N-2=10-2=8。使用t分布的累计分布函数`t.cdf()`或者`t.sf()`计算P值，因为双尾检查，因此再乘以2，其计算结果与`stats.pearsonr()`计算相关系数时给出的P值保持一致。因为p_value=4.402991448104743e-06<0.05，因此拒绝原假设，即拒绝总体下两个变量（化妆品费和置装费）之间完全无关，就是说总体下两个变量是相关的。因此所计算的相关系数值为0.968说明了总体所有女性在化妆品费和置装费的消费上是强关联的，如果一个女性花费较多的钱在化妆品费上，那么她花费在置装费的钱也相对较多，反之亦然。


```python
t_value=r*math.sqrt((10-2)/(1-math.pow(r,2)))
print("p-value_cdf:",(1-stats.t.cdf(t_value,10-2))*2)
print("p-value_sf:",stats.t.sf(t_value,10-2)*2)
```

    p-value_cdf: 4.402991448104743e-06
    p-value_sf: 4.402991448166121e-06
    

### 2.1.3.3 卡方分布与独立性检验

#### 1） 卡方分布（Chi-Square Distribution，χ²-distribution）<sup>[1][2][3]</sup>
若k个随机变量$Z_{1} , \ldots  \ldots ,Z_{k}$，是相互独立，符合标准正态分布的随机变量（数学期望为0，方差为1），则随机变量$Z$的平方和$X= \sum_{i=1}^k   x_{i} ^{2}$被称为服从自由度为$k$的卡方分布，记作$X \sim  x^{2} (k)$或$X \sim   x_{k} ^{2} $。卡方分布的概率密度函数（即计算卡方分布曲线的公式）为：$f_{k} (x)= \frac{  \frac{1}{2} ^{ \frac{k}{2} } }{ \Gamma ( \frac{k}{2} )}  x^{ \frac{k}{2}-1 } e^{ \frac{-x}{2} }  $，式中$x \geq 0$，当$x  \leq 0$时$f_{k} (x)=0$。$\Gamma $代表Gamma函数。在Python中绘制卡方分布，仍旧与正态分布、t分布一样，直接由`SciPy`库完成。

因为卡方分布表述的是多个事件（随机变量）的机率，每个事件符合标准正态分布，而标准正态分布表为记录对应横轴刻度的机率表，卡方分布表则是记录对应几率的横轴刻度表。

对于卡方分布的理解，可以结合比较均值抽样分布，二者具有类似的逻辑。可以表述为从同一总体中抽取相同容量样本平方和的分布，假设从服从平均值为30，标准差为5的正态分布中，随机提取2000个样本（事件，或随机变量，即$Z_{k}$），各个样本的容量为1000，计算每一样本的平方和，观察这2000个样本的卡方分布情况。从下述实验打印结果来看，$x^{2}$趋近服从自由度为$(2000-1)$的$x^{2}$分布。即可以通过一个检验统计量（平方和）来比较期望结果和实际结果之间的差别，然后得出观察频数极值的发生概率。因此以特定概率分布为某种情况建模时，事件长期结果较为稳定，能够清晰进行把握。但是如果期望与事实存在差异时，则可以应用卡方分布判断偏差是正常的小幅度波动还是建模上的错误。一是，可以检验一组给定数据与指定分布的吻合程度；二是，可以检验两个变量的独立性，即变量之间是否存在某种关系。

* $\Gamma $函数

在数学中，$\Gamma $函数，也称为伽马函数（Gamma函数），是阶乘函数在实数与复数域上的扩展。如果$n$为正整数，则：$\Gamma (n)=(n-1)!$，即正整数的阶乘；对于实数部分为正的复数$z$，伽马函数定义为：$\Gamma (z)= \int_0^ \infty   t^{z-1} e^{-t} dt$。发现$\Gamma $函数的起因是数列插值问题，即找到一个光滑的曲线连接那些由$y=(x-1)!$所给定的点$(x,y)$，并要求$x$为正整数。但是如果$x$由正整数拓展到实数，即可以计算$2!,3!, \ldots ,$，那么是否可以计算$2.5！$，并绘制$(n,n!)$的平滑曲线？而$\Gamma $函数正是借由微积分的积分与极限表达阶乘。

伽马（Gamma）分布，假设$X_{1} ,X_{2}, \ldots ,X_{n}$为连续发生事件的等候时间，且这$n$次等侯时间为独立的，那么这$n$此等候时间之和$Y(Y=X_{1} ,X_{2}, \ldots ,X_{n})$服从伽马分布，即$Y \sim Gamma( \alpha , \beta )$，式中$\alpha =n, \beta = \gamma $，$\alpha$是伽马分布中的母数，称为形状参数，$\beta$为尺度参数。$\gamma$是连续发生事件的平均发生频率。指数分布是伽马分布$\alpha=1$的特殊情况。 

令$X \sim  \Gamma ( \alpha , \beta )$，且$\lambda = \beta $（即$X \sim  \Gamma ( \alpha ,  \gamma  )$），则伽马分布的概率密度函数为：$f(x)= \frac{ x^{a-1}  \gamma ^{a}   e^{- \gamma x} }{ \Gamma (a)} ,x>0$，式中伽马函数的特征为：$\begin{cases}  \Gamma (a)=(a-1)! & if\: a\: is\:  \mathbb{Z}^{+} \\\Gamma (a)=(a-1) \Gamma (a-1) & if\: a\: is\:  \mathbb{R}^{+} \\\ \Gamma ( \frac{1}{2}= \sqrt{ \pi }  ) \end{cases} $



```python
import numpy as np
from scipy import stats
import seaborn as sns
import math

sns.set()
mu,sigma=30,5
sample_size=1000
sample_square=np.array([sum(np.square(np.random.normal(mu, sigma, sample_size))) for i in range(2000)]) # 从服从平均值为30，标准差为5的正态分布中，随机提取2000个样本，每个样本容量为1000的样本，并计算每一样本的均值
bins=30
sns.histplot(sample_square,bins=bins,kde=True,legend=False) # 查看2000个样本平方和的分布
```




    <AxesSubplot:ylabel='Count'>




<img src="./imgs/2_1_3/output_26_1.png" height='auto' width='auto' title="caDesign">    
   


使用`SciPy`库计算打印卡方分布及伽马分布。


```python
import matplotlib.pyplot as plt
from scipy.stats import chi2
import numpy as np

fig, axs=plt.subplots(1,2,figsize=(18/1.5,8/1.5)) 

# A-卡方分布
df=55
mean,var,skew,kurt=chi2.stats(df, moments='mvsk')
# 打印卡方分布的概率密度函数 Display the probability density function (pdf)
x=np.linspace(chi2.ppf(0.01, df),chi2.ppf(0.99, df), 100)
axs[0].plot(x, chi2.pdf(x, df),'r-', lw=5, alpha=0.6, label='chi2 pdf_55')

df_lst=[20,30,40,80,100]
fmts=['b-','g-','y-','m-','c-']
for i in range(len(df_lst)):
    axs[0].plot(x, chi2.pdf(x, df_lst[i]),fmts[i], lw=3, alpha=0.6, label='chi2 pdf_%d'%df_lst[i])
    
# 固定分布 Alternatively, freeze the distribution and display the frozen pdf
rv=chi2(df)
axs[0].plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf_55')

vals=chi2.ppf([0.001, 0.5, 0.999], df)
print("Chi_2_Check accuracy of cdf and ppf:",np.allclose([0.001, 0.5, 0.999], chi2.cdf(vals, df)))

r=chi2.rvs(df, size=1000)
axs[0].hist(r, density=True, histtype='stepfilled', alpha=0.2)
axs[0].legend(loc='best', frameon=False)
               
# B-Gamma分布    
from scipy.stats import gamma
a=1.99323054838
mean_,var_,skew_,kurt_=gamma.stats(a, moments='mvsk')
# 打印Gamma分布的概率密度函数 Display the probability density function (pdf):
x_= np.linspace(gamma.ppf(0.01, a),gamma.ppf(0.99, a), 100)
axs[1].plot(x_, gamma.pdf(x_, a),'r-', lw=5, alpha=0.6, label='gamma pdf')
# Alternatively, freeze the distribution and display the frozen pdf:
rv_= gamma(a)
axs[1].plot(x_, rv_.pdf(x_), 'k-', lw=2, label='frozen pdf')

vals = gamma.ppf([0.001, 0.5, 0.999], a)
print("Gamma_Check accuracy of cdf and ppf:",np.allclose([0.001, 0.5, 0.999], gamma.cdf(vals, a)))

r_=gamma.rvs(a, size=1000)
axs[1].hist(r_, density=True, histtype='stepfilled', alpha=0.2)
axs[1].legend(loc='best', frameon=False)

axs[0].set_title(r'Chi-Square Distribution', fontsize=15)
axs[1].set_title(r'Gamma Distribution', fontsize=15)
plt.show()               
```

    Chi_2_Check accuracy of cdf and ppf: True
    Gamma_Check accuracy of cdf and ppf: True
    


<img src="./imgs/2_1_3/output_28_1.png" height='auto' width='auto' title="caDesign">    
   


#### 2） （卡方）独立性检验
卡方检验（Chi-Squared Test，或 $x^{2}$ Test），是假设检验的一种，一种非参数假设检验，主要用于类别/分类变量（类别变量就是取值为离散值的变量，例如性别即为一个类别变量，有男女两类，又或者国籍、学科、植物等的类别等），在没有其它的限制条件或说明时，卡方检验一般指代的是皮尔森卡方（Pearson）检验。1900年，Pearson发表了著名的$x^{2}$检验的论文，假设实验中从总体随机取样得到的$n$个观察值被划分为$k$个互斥的分类中，这样每个分类都有一个对应实际的观察次数（或观测频数，observed frequencies）$ x_{i} (i=1,2, \ldots ,k)$。对实验中各个观察值落入第$i$个分类的概率$p_{i}$的分布提出零假设，获得对应所有第$i$分类的理论期望次数（或预期频数，expected frequencies）及限制条件，$\sum_{i=1}^k  p_{i}  =1,and \: \sum_{i=1}^k  m_{i} =\sum_{i=1}^k  x_{i} =n$。在上述零假设成立以及$n$趋向$\infty $时，以下统计量的极限分布趋向$x^{2}$分布，$X^{2} = \sum_{i=1}^k  \frac{ ( x_{i} - m_{i} )^{2} }{m_{i}}  =\sum_{i=1}^k  \frac{ x_{i} ^{2} }{m_{i}} -n$。$X^{2}$值的计算公式通常表示为：$X^{2} =  \sum ( \frac{ (O-E)^{2} }{E} )$，其中，$O$为各个单元格（对列联表而言）的观测值（观测频数），$E$为各个单元格的预期值（预期频数）。零假设中所有分类的理论期望次数$m_{i}$均为足够大且已知的情况，同时假设各分类的实际观察次数$x_{i}$均服从正态分布，得出样本容量$n$足够大时，$x^{2}$趋近服从自由度为$(k-1)$的$x^{2}$分布。

通常将用于卡方检验的数据以表格的形式给出并依据表格进行计算，这个表格即为列联表（contingency tabel）。以《白话统计学》性别与专业的修订数据为例，


| 性别/专业      | 心理学 | 英语     |生物学 |行合计     |
| :---        |    :----:   |          ---: | ---: |---: |
| 男生      | 35       | 50  |15|100|
| 女生      | 30        | 25      |45|100|
| 列合计      | 65        | 75      |60|200|

利用表格每一单元格中的观测频数，及行、列和整个样本的合计频数，计算每个单元格的预期频数。男女两行中每一单元格的预期值都相等，是因为样本中的男女生人数相等。并根据上述的$X^{2}$值的计算公式，计算$X^{2}$，其和为：0.19+0.19+4.17+4.17+7.5+7.5=23.72。

| 性别/专业      | 心理学 | 英语     |生物学 |
| :---        |    :----  |          :---| :--- |
| 男生      |    观测频数：35 </br>预期频数：$\frac{100 \times 65}{200} =32.5$ </br>$x^{2}$值：$ \frac{ (35-32.5)^{2} }{32.5} =0.19$   |观测频数：50 </br> 预期频数:$\frac{100 \times 75}{200} =37.5$</br> $x^{2}$值：$ \frac{ (50-37.5)^{2} }{37.5} =4.17$ |观测频数：15 </br> 预期频数: $\frac{100 \times 60}{200} =30$</br> $x^{2}$值：$ \frac{ (15-30)^{2} }{30} =7.5$ |
| 女生      | 观测频数：30 </br>预期频数 $\frac{100 \times 65}{200} =32.5$ </br>$x^{2}$值：$ \frac{ (30-32.5)^{2} }{32.5} =0.19$    |观测频数：25 </br> 预期频数:  $\frac{100 \times 75}{200} =37.5$ </br>$x^{2}$值：$ \frac{ (25-37.5)^{2} }{37.5} =4.17$ | 观测频数：45 </br> 预期频数: $\frac{100 \times 60}{200} =30$</br>$x^{2}$值：$ \frac{ (45-30)^{2} }{30} =7.5$ |

注意到$x^{2}$值较大是因为男女生在选择英语或生物专业时存在相对较大的差异。而心理学专业的观测值和预期值之差相对较小，对整体$x^{2}$值的贡献不大。获得观测的$x^{2}$值，则需要查表（或程序）查找临界$x^{2}$值，其自由度$df=(R-1)(C-1)=(2-1)\times(3-1)=2$，使用`SciPy`的`chi2.ppf(q=1-0.05,df=2)`计算可得0.05的$\alpha $水平，自由度为2的条件下临界$x^{2}$值为5.99，而观测的$x^{2}$值为23.72，所以可以得出结论，男女生在专业选择上存在统计显著的差异。而因为观测的$x^{2}$值足够大，在0.001的显著性水平上`chi2.ppf(q=1-0.001,df=2)`（临界值为13.815510557964274），也是统计显著的（即$p<0.001$）。


```python
print("𝑝<0.05,df=2,Chi-Squared=%.3f"%chi2.ppf(q=1-0.05,df=2))
print("𝑝<0.001,df=2,Chi-Squared=%.3f"%chi2.ppf(q=1-0.001,df=2))
```

    𝑝<0.05,df=2,Chi-Squared=5.991
    𝑝<0.001,df=2,Chi-Squared=13.816
    

使用`SciPy`的`chi2_contingency`方法计算列联表，其计算结果与手工计算结果保持一致。


```python
from scipy.stats import chi2_contingency
import numpy as np

schoolboy=(35,50,15)
schoolgirl=(30,25,45)
statistical_data=np.array([schoolboy,schoolgirl])
chi2_results=chi2_contingency(statistical_data)
print("卡方值：%.3f \n P值：%.10f \n 自由度:%d \n 对应预期频数（期望值）：\n %s"%chi2_results)
```

    卡方值：23.718 
     P值：0.0000070748 
     自由度:2 
     对应预期频数（期望值）：
     [[32.5 37.5 30. ]
     [32.5 37.5 30. ]]
    

#### 3） 协方差估计（Covariance Estimators）
统计学上常用的统计量包括平均值、方差、标准差等。平均值描述了样本集合的中间点；方差描述了一组数据与其平均值的偏离程度，方差越小，数据越集中，方差越大，数据越离散；标准差描述了样本集中各个样本点到均值的距离的平均值，同方差，描述数据集的集聚离散程度。这些统计量是针对一维数组，到处理高维时，用到协方差，度量多个随机变量关系的统计量，结果均为正则正相关，都为负则负相关，均趋近于0，则不相关。协方差是计算不同特征之间的统计量，不是不同样本之间的统计量。同时，协方差的大小，除了和变量之间的相关程度有关，也与变量本身的方差大小有关，因此引入相关系数，移除变量本身的影响。在协方差计算时可以使用协方差（矩阵）计算公式（查看方差和协方差部分），而有时并不使用全部的样本数据计算协方差矩阵，而是利用部分样本数据计算，这是就需要考虑样本计算得到的协方差矩阵是否和总体的协方差矩阵相同和近似。大多数情况下，估计总体的协方差矩阵必须在样本的性质（大小size，结构structure，同质性homogeneity）对估计质量有很大影响下进行，在`sklearn.covariance`模块中则提供了多个健壮的协方差估计算法，列表如下：

| 协方差估计方法                                        | 解释                                                                     |
|------------------------------------------------|------------------------------------------------------------------------|
| covariance.EmpiricalCovariance(*[, …])         | 最大似然协方差估计 Maximum likelihood covariance estimator                                |
| covariance.EllipticEnvelope(*[, …])            | 用于检测高斯分布数据集中异常值的对象 An object for detecting outliers in a Gaussian distributed dataset.    |
| covariance.GraphicalLasso([alpha, mode, …])    | 带L1惩罚估计量的稀疏逆协方差估计 Sparse inverse covariance estimation with an l1-penalized estimator.   |
| covariance.GraphicalLassoCV(*[, alphas, …])    | 稀疏逆协方差w/交叉验证l1惩罚的选择 Sparse inverse covariance w/ cross-validated choice of the l1 penalty. |
| covariance.LedoitWolf(*[, store_precision, …]) |LedoitWolf估计量  LedoitWolf Estimator                                                   |
| covariance.MinCovDet(*[, store_precision, …])  |最小协方差行列式(MCD):协方差的稳健估计  Minimum Covariance Determinant (MCD): robust estimator of covariance.  |
| covariance.OAS(*[, store_precision, …])        |Oracle逼近收缩估计  Oracle Approximating Shrinkage Estimator                               |
| covariance.ShrunkCovariance(*[, …])            |协方差缩水（shrinkage）估计 Covariance estimator with shrinkage                                    |
| covariance.empirical_covariance(X, *[, …])     |计算最大似然协方差估计量 Computes the Maximum likelihood covariance estimator                   |
| covariance.graphical_lasso(emp_cov, alpha, *)  | l1-惩罚项协方差估计量 l1-penalized covariance estimator                                      |
| covariance.ledoit_wolf(X, *[, …])              | 估计缩水的Ledoit-Wolf协方差矩阵 Estimates the shrunk Ledoit-Wolf covariance matrix.                    |
| covariance.oas(X, *[, assume_centered])        |用Oracle近似缩水算法估计协方差 Estimate covariance with the Oracle Approximating Shrinkage algorithm. |
| covariance.shrunk_covariance(emp_cov[, …])     |计算在对角线上缩水的协方差矩阵 Calculates a covariance matrix shrunk on the diagonal                  |

下述假设了一个协方差矩阵，并根据该协方差矩阵生产一组数据集，分布使用了`sklearn.covariance `提供的`GraphicalLassoCV`，`EmpiricalCovariance`，`MinCovDet`，及`NumPy`库提供的`np.cov()`方法进行计算比较观察，其结果相近，向真实假设的协方差矩阵值靠近。


```python
import numpy as np
from sklearn.covariance import GraphicalLassoCV,EmpiricalCovariance,MinCovDet

# 假设的协方差矩阵，包含4个特征量
true_cov=np.array([[0.8, 0.0, 0.2, 0.0],
                  [0.0, 0.4, 0.0, 0.0],
                  [0.2, 0.0, 0.3, 0.1],
                  [0.0, 0.0, 0.1, 0.7]])
np.random.seed(0)
# 生成满足假设协方差矩阵的特征值
X=np.random.multivariate_normal(mean=[0, 0, 0, 0], cov=true_cov,size=200)
# A-使用GraphicalLassoCV方法
cov=GraphicalLassoCV().fit(X)
print("A-GraphicalLassoCV algorithm:\n{},estimated location(the estimated mean):{}".format(np.around(GraphicalLassoCV().fit(X).covariance_, decimals=3),np.around(cov.location_, decimals=3)))

# B-EmpiricalCovariance
print("A-EmpiricalCovariance algorithm:\n{}".format(np.around(EmpiricalCovariance().fit(X).covariance_, decimals=3)))

# C-MinCovDet
print("A-MinCovDet:\n{}".format(np.around(MinCovDet().fit(X).covariance_, decimals=3)))

# D-np.cov
print("A-np.cov:\n{}".format(np.around(np.cov(X.T), decimals=3)))
```

    A-GraphicalLassoCV algorithm:
    [[0.816 0.051 0.22  0.017]
     [0.051 0.364 0.018 0.036]
     [0.22  0.018 0.322 0.094]
     [0.017 0.036 0.094 0.69 ]],estimated location(the estimated mean):[0.073 0.04  0.038 0.143]
    A-EmpiricalCovariance algorithm:
    [[0.816 0.059 0.228 0.009]
     [0.059 0.364 0.025 0.044]
     [0.228 0.025 0.322 0.103]
     [0.009 0.044 0.103 0.69 ]]
    A-MinCovDet:
    [[ 0.741 -0.005  0.162  0.089]
     [-0.005  0.305  0.024  0.061]
     [ 0.162  0.024  0.237  0.117]
     [ 0.089  0.061  0.117  0.55 ]]
    A-np.cov:
    [[0.82  0.059 0.229 0.009]
     [0.059 0.366 0.025 0.044]
     [0.229 0.025 0.324 0.103]
     [0.009 0.044 0.103 0.694]]
    

### 2.1.3.4 公共健康数据的地理空间分布与相关性分析

#### 1）公共健康数据的地理空间分布

公共健康数据为芝加哥社区选定的公共健康（卫生）指标（public health indicator）。该数据集包括27项重要的公共健康指标，这些指标为比率、百分比、或者出生率、死亡率、传染病、铅中毒及经济状况相关的指标。该数据集是按照社区进行统计，在字段中给出了社区名，因此可以将数据按照社区名配对到各个社区范围的地理空间数据上。芝加哥社区边界及公共健康数据均来源于[Chicago Data Portal，CDP](https://data.cityofchicago.org/)<sup>②</sup>。关于数据的细节描述可以参考CDP提供的文件。

> 在JupyterLab中打印数据的方式，包括`print()`和直接将要显示数据的变量放置于单个单元格（cell）两种方式。但是前者显示为固定宽度，不能按照屏幕宽度自动缩放，后者虽为100%宽度自动缩放显示，但是如果要将.ipynb的Jupyter文件输出为.md的Mardown文件，可能会产生乱码。因此，可以使用`from IPython.display import HTML`，将要显示的数据转换为HTML格式数据后，会解决上述两种问题。


```python
import pandas as pd

dataFp_dic={
    "ublic_Health_Statistics_byCommunityArea_fp":r'./data/Public_Health_Statistics_Selected_public_health_indicators_by_Chicago_community_area.csv',
    "Boundaries_Community_Areas_current":r'./data/ChicagoCommunityAreas/ChicagoCommunityAreas.shp',    
}

pubicHealth_Statistic=pd.read_csv(dataFp_dic["ublic_Health_Statistics_byCommunityArea_fp"])

# 中英对照表（字段映射表）
PubicHealth_Statistic_columns={'Community Area':'社区', 
                                'Community Area Name':'社区名',
                                'Birth Rate':'出生率',
                                'General Fertility Rate':'一般生育率',
                                'Low Birth Weight':'低出生体重',
                                'Prenatal Care Beginning in First Trimester':'产前3个月护理', 
                                'Preterm Births':'早产',
                                'Teen Birth Rate':'青少年生育率',
                                'Assault (Homicide)':'攻击（杀人）',
                                'Breast cancer in females':'女性乳腺癌',
                                'Cancer (All Sites)':'癌症', 
                                'Colorectal Cancer':'结肠直肠癌',
                                'Diabetes-related':'糖尿病相关',
                                'Firearm-related':'枪支相关',
                                'Infant Mortality Rate':'婴儿死亡率', 
                                'Lung Cancer':'肺癌',
                                'Prostate Cancer in Males':'男性前列腺癌',
                                'Stroke (Cerebrovascular Disease)':'中风(脑血管疾病)',
                                'Childhood Blood Lead Level Screening':'儿童血铅水平检查',
                                'Childhood Lead Poisoning':'儿童铅中毒',
                                'Gonorrhea in Females':'女性淋病', 
                                'Gonorrhea in Males':'男性淋病', 
                                'Tuberculosis':'肺结核',
                                'Below Poverty Level':'贫困水平以下', 
                                'Crowded Housing':'拥挤的住房', 
                                'Dependency':'依赖',
                                'No High School Diploma':'没有高中文凭', 
                                'Per Capita Income':'人均收入',
                                'Unemployment':'失业',
                                }

def print_html(df,row_numbers=5):    
    '''
    function - 在Jupyter中打印DataFrame格式数据为HTML
    
    Params:
        df - 需要打印的DataFrame或GeoDataFrame格式数据；DataFrame
        row_numbers - 打印的行数，如果为正，从开始打印如果为负，从末尾打印；int
        
    Returns:
        转换后的HTML格式数据；
     '''
    from IPython.display import HTML
    
    if row_numbers>0:
        return HTML(df.head(row_numbers).to_html())
    else:
        return HTML(df.tail(abs(row_numbers)).to_html())
print_html(pubicHealth_Statistic,6)
```



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Community Area</th>
      <th>Community Area Name</th>
      <th>Birth Rate</th>
      <th>General Fertility Rate</th>
      <th>Low Birth Weight</th>
      <th>Prenatal Care Beginning in First Trimester</th>
      <th>Preterm Births</th>
      <th>Teen Birth Rate</th>
      <th>Assault (Homicide)</th>
      <th>Breast cancer in females</th>
      <th>Cancer (All Sites)</th>
      <th>Colorectal Cancer</th>
      <th>Diabetes-related</th>
      <th>Firearm-related</th>
      <th>Infant Mortality Rate</th>
      <th>Lung Cancer</th>
      <th>Prostate Cancer in Males</th>
      <th>Stroke (Cerebrovascular Disease)</th>
      <th>Childhood Blood Lead Level Screening</th>
      <th>Childhood Lead Poisoning</th>
      <th>Gonorrhea in Females</th>
      <th>Gonorrhea in Males</th>
      <th>Tuberculosis</th>
      <th>Below Poverty Level</th>
      <th>Crowded Housing</th>
      <th>Dependency</th>
      <th>No High School Diploma</th>
      <th>Per Capita Income</th>
      <th>Unemployment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Rogers Park</td>
      <td>16.4</td>
      <td>62.0</td>
      <td>11.0</td>
      <td>73.0</td>
      <td>11.2</td>
      <td>40.8</td>
      <td>7.7</td>
      <td>23.3</td>
      <td>176.9</td>
      <td>25.3</td>
      <td>77.1</td>
      <td>5.2</td>
      <td>6.4</td>
      <td>36.7</td>
      <td>21.7</td>
      <td>33.7</td>
      <td>364.7</td>
      <td>0.5</td>
      <td>322.5</td>
      <td>423.3</td>
      <td>11.4</td>
      <td>22.7</td>
      <td>7.9</td>
      <td>28.8</td>
      <td>18.1</td>
      <td>23714</td>
      <td>7.5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>West Ridge</td>
      <td>17.3</td>
      <td>83.3</td>
      <td>8.1</td>
      <td>71.1</td>
      <td>8.3</td>
      <td>29.9</td>
      <td>5.8</td>
      <td>20.2</td>
      <td>155.9</td>
      <td>17.3</td>
      <td>60.5</td>
      <td>3.7</td>
      <td>5.1</td>
      <td>36.0</td>
      <td>14.2</td>
      <td>34.7</td>
      <td>331.4</td>
      <td>1.0</td>
      <td>141.0</td>
      <td>205.7</td>
      <td>8.9</td>
      <td>15.1</td>
      <td>7.0</td>
      <td>38.3</td>
      <td>19.6</td>
      <td>21375</td>
      <td>7.9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Uptown</td>
      <td>13.1</td>
      <td>50.5</td>
      <td>8.3</td>
      <td>77.7</td>
      <td>10.3</td>
      <td>35.1</td>
      <td>5.4</td>
      <td>21.3</td>
      <td>183.3</td>
      <td>20.5</td>
      <td>80.0</td>
      <td>4.6</td>
      <td>6.5</td>
      <td>50.5</td>
      <td>25.2</td>
      <td>41.7</td>
      <td>353.7</td>
      <td>0.5</td>
      <td>170.8</td>
      <td>468.7</td>
      <td>13.6</td>
      <td>22.7</td>
      <td>4.6</td>
      <td>22.2</td>
      <td>13.6</td>
      <td>32355</td>
      <td>7.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Lincoln Square</td>
      <td>17.1</td>
      <td>61.0</td>
      <td>8.1</td>
      <td>80.5</td>
      <td>9.7</td>
      <td>38.4</td>
      <td>5.0</td>
      <td>21.7</td>
      <td>153.2</td>
      <td>8.6</td>
      <td>55.4</td>
      <td>6.1</td>
      <td>3.8</td>
      <td>43.1</td>
      <td>27.6</td>
      <td>36.9</td>
      <td>273.3</td>
      <td>0.4</td>
      <td>98.8</td>
      <td>195.5</td>
      <td>8.5</td>
      <td>9.5</td>
      <td>3.1</td>
      <td>25.6</td>
      <td>12.5</td>
      <td>35503</td>
      <td>6.8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>North Center</td>
      <td>22.4</td>
      <td>76.2</td>
      <td>9.1</td>
      <td>80.4</td>
      <td>9.8</td>
      <td>8.4</td>
      <td>1.0</td>
      <td>16.6</td>
      <td>152.1</td>
      <td>26.1</td>
      <td>49.8</td>
      <td>1.0</td>
      <td>2.7</td>
      <td>42.4</td>
      <td>15.1</td>
      <td>41.6</td>
      <td>178.1</td>
      <td>0.9</td>
      <td>85.4</td>
      <td>188.6</td>
      <td>1.9</td>
      <td>7.1</td>
      <td>0.2</td>
      <td>25.5</td>
      <td>5.4</td>
      <td>51615</td>
      <td>4.5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>Lake View</td>
      <td>13.5</td>
      <td>38.7</td>
      <td>6.3</td>
      <td>79.1</td>
      <td>8.1</td>
      <td>15.8</td>
      <td>1.4</td>
      <td>20.1</td>
      <td>126.9</td>
      <td>13.0</td>
      <td>38.5</td>
      <td>1.8</td>
      <td>2.2</td>
      <td>32.5</td>
      <td>17.0</td>
      <td>24.4</td>
      <td>179.2</td>
      <td>0.4</td>
      <td>81.8</td>
      <td>357.6</td>
      <td>3.2</td>
      <td>10.5</td>
      <td>1.2</td>
      <td>16.5</td>
      <td>2.9</td>
      <td>58227</td>
      <td>4.7</td>
    </tr>
  </tbody>
</table>



在数据合并时，公共健康数据以"Community Area"为对位列，社区边界数据以"area_numbe"为对位列，使用`df.merge()`方法完成融合。打印两组数据，其中肺癌（Lung Cancer）为每100,000人所占的比例。GeoDataFrame格式数据在打印时如果需要在图上每个Polygon上标注数据增加地图的信息量，可以按照下述代码处理。


```python
import geopandas as gpd
from mpl_toolkits.axes_grid1 import make_axes_locatable
import matplotlib.pyplot as plt

community_area=gpd.read_file(dataFp_dic["Boundaries_Community_Areas_current"])
print(community_area.dtypes) 
community_area.area_numbe=community_area.area_numbe.astype('int64')
print("_"*50)
print("boundaries_community.area_numbe dtype:",community_area.area_numbe.dtypes)
pubicHealth_gpd=community_area.merge(pubicHealth_Statistic,left_on='area_numbe', right_on='Community Area')

fig, axs=plt.subplots(1,2,figsize=(40, 40))

# 打印第1组数据
divider=make_axes_locatable(axs[0]) 
cax_0=divider.append_axes("left", size="5%", pad=0.1) # 配置图例参数
pubicHealth_gpd.plot(column='Lung Cancer',ax=axs[0],cax=cax_0,legend=True,cmap='OrRd')  # 如果提示如下错误，则需要按照信息安装对应的库,ImportError: The descartes package is required for plotting polygons in geopandas. You can install it using 'conda install -c conda-forge descartes' or 'pip install descartes'.
axs[0].set_title("Lung Cancer-Per 100,000 males (age adjusted)",fontsize=20)
pubicHealth_gpd.apply(lambda x: axs[0].annotate(text=x["Lung Cancer"], xy=x.geometry.centroid.coords[0], ha='center'),axis=1) #增加标注
axs[0].tick_params(axis='both', labelsize=15)
cb_ax_0=axs[0].figure.axes[2]
cb_ax_0.tick_params(labelsize=15)

# 打印第2组数据
divider=make_axes_locatable(axs[1])
cax_1=divider.append_axes("right", size="5%", pad=0.1)
pubicHealth_gpd.plot(column='Per Capita Income',ax=axs[1],cax=cax_1,legend=True,cmap='OrRd')
axs[1].set_title("Per Capita Income-2011 inflation-adjusted dollars",fontsize=20)
pubicHealth_gpd.apply(lambda x: axs[1].annotate(text=x["Community Area Name"], xy=x.geometry.centroid.coords[0], ha='center'),axis=1)
axs[1].tick_params(axis='both', labelsize=15)
cb_ax_1=axs[1].figure.axes[3]
cb_ax_1.tick_params(labelsize=15)

plt.show()
```

    area           float64
    area_num_1      object
    area_numbe      object
    comarea        float64
    comarea_id     float64
    community       object
    perimeter      float64
    shape_area     float64
    shape_len      float64
    geometry      geometry
    dtype: object
    __________________________________________________
    boundaries_community.area_numbe dtype: int64
    


<img src="./imgs/2_1_3/output_38_1.png" height='auto' width='auto' title="caDesign">    
   


#### 2）公共健康数据的相关性分析

观察上图，似乎肺癌的发生比例与收入多少有关联，收入高的区域发生肺癌的机率低，相反，收入低的区域，发生肺癌的机率高。粗略的观察只能够给出一个初步不是十分确定的判断结果，如果需要以数据分析的方式证明之间是否存在关联，则需要作相关性分析。


```python
pubicHealth_Statistic_mapping={'Community Area':'CommunityArea', 
                                'Community Area Name':'CommunityArea_Name',
                                'Birth Rate':'Birth_Rate',
                                'General Fertility Rate':'General_FertilityRate',
                                'Low Birth Weight':'Low_BirthWeight',
                                'Prenatal Care Beginning in First Trimester':'PrenatalCareBeginning_inFirstTrimester', 
                                'Preterm Births':'Preterm_Births',
                                'Teen Birth Rate':'TeenBirth_Rate',
                                'Assault (Homicide)':'Assault_Homicide',
                                'Breast cancer in females':'BreastCancer_infemales',
                                'Cancer (All Sites)':'Cancer_AllSites', 
                                'Colorectal Cancer':'Colorectal_Cancer',
                                'Diabetes-related':'Diabetes_related',
                                'Firearm-related':'Firearm_related',
                                'Infant Mortality Rate':'InfantMortality_Rate', 
                                'Lung Cancer':'Lung_Cancer',
                                'Prostate Cancer in Males':'ProstateCancer_inMales',
                                'Stroke (Cerebrovascular Disease)':'Stroke_CerebrovascularDisease',
                                'Childhood Blood Lead Level Screening':'ChildhoodBloodLeadLevel_Screening',
                                'Childhood Lead Poisoning':'ChildhoodLead_Poisoning',
                                'Gonorrhea in Females':'Gonorrhea_inFemales', 
                                'Gonorrhea in Males':'Gonorrhea_inMales', 
                                'Tuberculosis':'Tuberculosis',
                                'Below Poverty Level':'BelowPoverty_Level', 
                                'Crowded Housing':'Crowded_Housing', 
                                'Dependency':'Dependency',
                                'No High School Diploma':'NoHighSchool_Diploma', 
                                'Per Capita Income':'PerCapita_Income',
                                'Unemployment':'Unemployment',
                                }

pubicHealth_rename=pubicHealth_gpd.rename(columns=pubicHealth_Statistic_mapping)

pubicHealth_extract_columns=[
       'Birth_Rate','General_FertilityRate', 'Low_BirthWeight',
       'PrenatalCareBeginning_inFirstTrimester', 'Preterm_Births',
       'TeenBirth_Rate', 'Assault_Homicide', 'BreastCancer_infemales',
       'Cancer_AllSites', 'Colorectal_Cancer', 'Diabetes_related',
       'Firearm_related', 'InfantMortality_Rate', 'Lung_Cancer',
       'ProstateCancer_inMales', 'Stroke_CerebrovascularDisease',
       'ChildhoodBloodLeadLevel_Screening', 'ChildhoodLead_Poisoning',
       'Gonorrhea_inFemales', 'Gonorrhea_inMales', 'Tuberculosis',    
       'BelowPoverty_Level', 'Crowded_Housing', 'Dependency',
       'NoHighSchool_Diploma', 'PerCapita_Income', 'Unemployment'    
        ]

pubicHealth_extract=pubicHealth_rename[pubicHealth_extract_columns].select_dtypes(include=np.number)
publicHealth_correlation=pubicHealth_extract.corr()
print_html(publicHealth_correlation)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Birth_Rate</th>
      <th>General_FertilityRate</th>
      <th>Low_BirthWeight</th>
      <th>PrenatalCareBeginning_inFirstTrimester</th>
      <th>Preterm_Births</th>
      <th>TeenBirth_Rate</th>
      <th>Assault_Homicide</th>
      <th>BreastCancer_infemales</th>
      <th>Cancer_AllSites</th>
      <th>Colorectal_Cancer</th>
      <th>Diabetes_related</th>
      <th>Firearm_related</th>
      <th>InfantMortality_Rate</th>
      <th>Lung_Cancer</th>
      <th>ProstateCancer_inMales</th>
      <th>Stroke_CerebrovascularDisease</th>
      <th>ChildhoodBloodLeadLevel_Screening</th>
      <th>ChildhoodLead_Poisoning</th>
      <th>Gonorrhea_inFemales</th>
      <th>Tuberculosis</th>
      <th>BelowPoverty_Level</th>
      <th>Crowded_Housing</th>
      <th>Dependency</th>
      <th>NoHighSchool_Diploma</th>
      <th>PerCapita_Income</th>
      <th>Unemployment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Birth_Rate</th>
      <td>1.000000</td>
      <td>0.810334</td>
      <td>0.108179</td>
      <td>-0.178847</td>
      <td>0.004334</td>
      <td>0.612710</td>
      <td>0.188536</td>
      <td>-0.022201</td>
      <td>-0.001289</td>
      <td>-0.097507</td>
      <td>0.303831</td>
      <td>0.162722</td>
      <td>0.051716</td>
      <td>-0.063567</td>
      <td>0.130114</td>
      <td>0.126423</td>
      <td>0.481725</td>
      <td>0.210016</td>
      <td>0.126671</td>
      <td>0.279755</td>
      <td>0.249764</td>
      <td>0.603219</td>
      <td>0.115771</td>
      <td>0.594249</td>
      <td>-0.377653</td>
      <td>0.149439</td>
    </tr>
    <tr>
      <th>General_FertilityRate</th>
      <td>0.810334</td>
      <td>1.000000</td>
      <td>0.142189</td>
      <td>-0.134292</td>
      <td>0.122235</td>
      <td>0.655528</td>
      <td>0.293843</td>
      <td>0.058089</td>
      <td>0.164279</td>
      <td>0.022977</td>
      <td>0.352294</td>
      <td>0.285562</td>
      <td>0.165320</td>
      <td>0.026357</td>
      <td>0.196524</td>
      <td>0.241311</td>
      <td>0.490693</td>
      <td>0.303624</td>
      <td>0.250984</td>
      <td>0.144972</td>
      <td>0.173846</td>
      <td>0.655826</td>
      <td>0.524558</td>
      <td>0.714008</td>
      <td>-0.661131</td>
      <td>0.275589</td>
    </tr>
    <tr>
      <th>Low_BirthWeight</th>
      <td>0.108179</td>
      <td>0.142189</td>
      <td>1.000000</td>
      <td>-0.532546</td>
      <td>0.843100</td>
      <td>0.622936</td>
      <td>0.747493</td>
      <td>0.425041</td>
      <td>0.759791</td>
      <td>0.657992</td>
      <td>0.664107</td>
      <td>0.671880</td>
      <td>0.677792</td>
      <td>0.671295</td>
      <td>0.713754</td>
      <td>0.525439</td>
      <td>0.416265</td>
      <td>0.388319</td>
      <td>0.737727</td>
      <td>0.125231</td>
      <td>0.681049</td>
      <td>-0.080532</td>
      <td>0.479581</td>
      <td>0.056962</td>
      <td>-0.394564</td>
      <td>0.702227</td>
    </tr>
    <tr>
      <th>PrenatalCareBeginning_inFirstTrimester</th>
      <td>-0.178847</td>
      <td>-0.134292</td>
      <td>-0.532546</td>
      <td>1.000000</td>
      <td>-0.413383</td>
      <td>-0.579170</td>
      <td>-0.591605</td>
      <td>-0.142970</td>
      <td>-0.385923</td>
      <td>-0.315996</td>
      <td>-0.470152</td>
      <td>-0.540972</td>
      <td>-0.466319</td>
      <td>-0.353354</td>
      <td>-0.430259</td>
      <td>-0.395817</td>
      <td>-0.328470</td>
      <td>-0.516579</td>
      <td>-0.653985</td>
      <td>-0.270147</td>
      <td>-0.516145</td>
      <td>-0.074951</td>
      <td>-0.192405</td>
      <td>-0.078027</td>
      <td>0.200858</td>
      <td>-0.542603</td>
    </tr>
    <tr>
      <th>Preterm_Births</th>
      <td>0.004334</td>
      <td>0.122235</td>
      <td>0.843100</td>
      <td>-0.413383</td>
      <td>1.000000</td>
      <td>0.549843</td>
      <td>0.742019</td>
      <td>0.435020</td>
      <td>0.780207</td>
      <td>0.722153</td>
      <td>0.650790</td>
      <td>0.732584</td>
      <td>0.672087</td>
      <td>0.675119</td>
      <td>0.642100</td>
      <td>0.539608</td>
      <td>0.289144</td>
      <td>0.379146</td>
      <td>0.800491</td>
      <td>0.001197</td>
      <td>0.550906</td>
      <td>-0.140244</td>
      <td>0.571350</td>
      <td>-0.023655</td>
      <td>-0.364807</td>
      <td>0.646550</td>
    </tr>
  </tbody>
</table>



通过上述相关性计算，已经计算了两两之间的相关系数。为了更加清晰的查看相关系数，可以同时打印散点图（通常在计算相关系数前打印散点图，查看两个变量间的线性关系），或查看相关系数变化的热力图。


```python
import plotly.graph_objects as go

fig = go.Figure(data=go.Splom(
                dimensions=[dict(label=k,values=list(v.values())) for k,v in pubicHealth_extract.to_dict().items()],
                diagonal_visible=False, # remove plots on diagonal
                ))
fig.update_layout(
    title='公共健康数据',
    dragmode='select',
    width=1800,
    height=1800,
    hovermode='closest',
)

fig.show()
```

<img src="./imgs/2_1_3/output_42_0.png" height='auto' width='auto' title="caDesign">


```python
import seaborn as sns

plt.figure(figsize=(20, 20))
sns.heatmap(publicHealth_correlation,annot=True, fmt=".2f", linewidths=.5,)
plt.show()
```


<img src="./imgs/2_1_3/output_43_0.png" height='auto' width='auto' title="caDesign">    
  


不管是散点图还是热力图，以及相关系数矩阵，在观察多个两两之间相关性时，都不是很容易观察所关注的对象。因此可以根据分析的目的将需要关注的变量提取出来，只关注该变量与其它所有变量的相关系数。


```python
import numpy as np

economic_factors=['BelowPoverty_Level', 'Crowded_Housing', 'Dependency', 'NoHighSchool_Diploma', 'PerCapita_Income', 'Unemployment']
publicHealth_indicator=publicHealth_correlation[economic_factors]
publicHealth_indicator_columns=publicHealth_indicator.T.columns.to_numpy()

plt.rcdefaults()
plt.rcParams.update({'font.size':9})

nrows=3
ncols=2
fig, axs=plt.subplots(nrows=nrows,ncols=ncols,figsize=(10*2, 10*2))  
y_pos=np.arange(len(publicHealth_indicator_columns))

i=0
for idx in [(row,col) for row in range(nrows) for col in range(ncols)]:
    axs[idx].barh(y_pos, publicHealth_indicator[economic_factors[i]].to_numpy(), align='center') 
    axs[idx].set_yticks(y_pos)
    axs[idx].set_yticklabels(publicHealth_indicator_columns)
    axs[idx].invert_yaxis()  # labels read top-to-bottom
    axs[idx].set_xlabel(economic_factors[i])
    # axs.set_title(title_str)
    for index, value in enumerate(publicHealth_indicator[economic_factors[i]].to_numpy()):
        if value>=0:
            axs[idx].text(value, index, str(round(value,2)),horizontalalignment='left')
        else:
            axs[idx].text(value, index, str(round(value,2)),horizontalalignment='right')
    i+=1
    
plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.1,wspace=0.4)
plt.show()
```


<img src="./imgs/2_1_3/output_45_0.png" height='auto' width='auto' title="caDesign">    
   


虽然计算了两两公共健康指标之间的相关系数，但这并不代表该样本所计算得出的相关性能够反映总体是否也存在样本所得出的现象，这个时候需要关注假设检验计算得出的P值，将P值不满足显著性水平为0.05的两两变量剔除掉，所得到的结果即为能够反映总体相关性的部分。首先用所关注的变量作为index索引，为所有的经济变量`economic_factors=['BelowPoverty_Level', 'Crowded_Housing', 'Dependency', 'NoHighSchool_Diploma', 'PerCapita_Income', 'Unemployment']`，方便后续的数据分析。


```python
from scipy import stats

disease_columns=[       
       'Birth_Rate','General_FertilityRate', 'Low_BirthWeight',
       'PrenatalCareBeginning_inFirstTrimester', 'Preterm_Births',
       'TeenBirth_Rate', 'Assault_Homicide', 'BreastCancer_infemales',
       'Cancer_AllSites', 'Colorectal_Cancer', 'Diabetes_related',
       'Firearm_related', 'InfantMortality_Rate', 'Lung_Cancer',
       'ProstateCancer_inMales', 'Stroke_CerebrovascularDisease',
       'ChildhoodBloodLeadLevel_Screening', 'ChildhoodLead_Poisoning',
       'Gonorrhea_inFemales', 'Tuberculosis',
        ]
pubicHealth_pearsonr={}

for factor in economic_factors:
    disease_temp={}
    for disease in disease_columns:
        desease_series=pd.to_numeric(pubicHealth_extract[disease],errors='coerce')
        factor_series=pd.to_numeric(pubicHealth_extract[factor],errors='coerce')
        mask=pd.notna(desease_series)&pd.notna(factor_series)
        
        disease_temp[disease]=stats.pearsonr(pd.to_numeric(factor_series[mask],errors='ignore'),pd.to_numeric(desease_series[mask],errors='ignore'))
    pubicHealth_pearsonr[factor]=disease_temp
pubicHealth_pearsonr_df=pd.DataFrame.from_dict(pubicHealth_pearsonr, orient='index').stack().to_frame(name='corr_pV')

pubicHealth_pearsonr_df['correlation']=pubicHealth_pearsonr_df.corr_pV.apply(lambda row:row[0])
pubicHealth_pearsonr_df['p_value']=pubicHealth_pearsonr_df.corr_pV.apply(lambda row:row[1])

print_html(pubicHealth_pearsonr_df)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>corr_pV</th>
      <th>correlation</th>
      <th>p_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">BelowPoverty_Level</th>
      <th>Birth_Rate</th>
      <td>(0.24976406537114698, 0.028475743860025216)</td>
      <td>0.249764</td>
      <td>2.847574e-02</td>
    </tr>
    <tr>
      <th>General_FertilityRate</th>
      <td>(0.17384625982353086, 0.13051281178487212)</td>
      <td>0.173846</td>
      <td>1.305128e-01</td>
    </tr>
    <tr>
      <th>Low_BirthWeight</th>
      <td>(0.6810485083790098, 9.384671983490509e-12)</td>
      <td>0.681049</td>
      <td>9.384672e-12</td>
    </tr>
    <tr>
      <th>PrenatalCareBeginning_inFirstTrimester</th>
      <td>(-0.5161448655124206, 1.5503295893976403e-06)</td>
      <td>-0.516145</td>
      <td>1.550330e-06</td>
    </tr>
    <tr>
      <th>Preterm_Births</th>
      <td>(0.5509056684724956, 2.086303329322895e-07)</td>
      <td>0.550906</td>
      <td>2.086303e-07</td>
    </tr>
  </tbody>
</table>



以相关系数的值为横坐标，以相关系数对应的P值为纵坐标，以0.05作为显著性水平，打印散点图，可以较为直观的查看哪些两两公共健康指标的相关系数能够反映或者不能够反映总体的现象。


```python
pubicHealth_pearsonr_resetIdx=pubicHealth_pearsonr_df.reset_index()
significance_level=0.05
fig=plt.figure(figsize=(30, 10))
ax=fig.add_subplot(111, facecolor='#FFFFCC')
X=pubicHealth_pearsonr_resetIdx.correlation
Y=pubicHealth_pearsonr_resetIdx.p_value
ax.plot(X, Y, 'o')
ax.axhline(y=significance_level,color='r',linestyle='--',label='significance_level')
i=0
for x,y in zip (X,Y):
    label=pubicHealth_pearsonr_resetIdx.loc[i].level_0+' : '+pubicHealth_pearsonr_resetIdx.loc[i].level_1
    plt.annotate(label, # this is the text
                 (x,y), # this is the point to label
                 textcoords="offset points", # how to position the text
                 xytext=(0,10), # distance from text to points (x,y)
                 ha='center', # horizontal alignment can be left, right or center
                 rotation=90
                )
    
    i+=1
plt.annotate('alpha level=%.2f'%significance_level,xy=(-0.7,significance_level),xytext=(-0.75, 0.15),arrowprops=dict(facecolor='black', shrink=0.01),horizontalalignment='right',size=15)
plt.xlabel("correlation",fontsize=20)
plt.ylabel("p-value",fontsize=20)    
plt.tick_params(axis='both', labelsize=15)
plt.show()
```


<img src="./imgs/2_1_3/output_49_0.png" height='auto' width='auto' title="caDesign">   


提取P值小于等于0.05的所有行，并以柱状图的形式打印相关系数结果。


```python
pubicHealth_pearsonr_alpha=pubicHealth_pearsonr_df[pubicHealth_pearsonr_df.p_value<=0.05]
print_html(pubicHealth_pearsonr_alpha)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>corr_pV</th>
      <th>correlation</th>
      <th>p_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">BelowPoverty_Level</th>
      <th>Birth_Rate</th>
      <td>(0.24976406537114698, 0.028475743860025216)</td>
      <td>0.249764</td>
      <td>2.847574e-02</td>
    </tr>
    <tr>
      <th>Low_BirthWeight</th>
      <td>(0.6810485083790098, 9.384671983490509e-12)</td>
      <td>0.681049</td>
      <td>9.384672e-12</td>
    </tr>
    <tr>
      <th>PrenatalCareBeginning_inFirstTrimester</th>
      <td>(-0.5161448655124206, 1.5503295893976403e-06)</td>
      <td>-0.516145</td>
      <td>1.550330e-06</td>
    </tr>
    <tr>
      <th>Preterm_Births</th>
      <td>(0.5509056684724956, 2.086303329322895e-07)</td>
      <td>0.550906</td>
      <td>2.086303e-07</td>
    </tr>
    <tr>
      <th>TeenBirth_Rate</th>
      <td>(0.6600382763346119, 6.598083934767818e-11)</td>
      <td>0.660038</td>
      <td>6.598084e-11</td>
    </tr>
  </tbody>
</table>




```python
import plotly.graph_objects as go
import random

def generate_colors():    
    '''
    function - 生成颜色列表或者字典
    
    Returns:
        hex_colors_only - 16进制颜色值列表；list
        hex_colors_dic - 颜色名称：16进制颜色值；dict
        rgb_colors_dic - 颜色名称：(r,g,b)；dict
    '''
    import matplotlib
    
    hex_colors_dic={}
    rgb_colors_dic={}
    hex_colors_only=[]
    for name, hex in matplotlib.colors.cnames.items():
        hex_colors_only.append(hex)
        hex_colors_dic[name]=hex
        rgb_colors_dic[name]=matplotlib.colors.to_rgb(hex)
    return hex_colors_only,hex_colors_dic,rgb_colors_dic

generate_colors,_,_=generate_colors()

economic_factors=['BelowPoverty_Level', 'Crowded_Housing', 'Dependency', 'NoHighSchool_Diploma', 'PerCapita_Income', 'Unemployment']
fig = go.Figure()

for idx,data in pubicHealth_pearsonr_alpha.groupby(level=0):
    fig.add_trace(go.Bar(x=disease_columns,
                    y=data.correlation,
                    name=idx,
                    marker_color=random.choice(generate_colors)
                    ))

fig.update_layout(
    title='Public health indicators_correlation,p-value<0.05',
    xaxis_tickfont_size=14,
    yaxis=dict(
        title='correlation)',
        titlefont_size=16,
        tickfont_size=14,
    ),
    barmode='group',
    bargap=0.1, # gap between bars of adjacent location coordinates.
    bargroupgap=0.5 # gap between bars of the same location coordinate.
)
fig.show()
```

<img src="./imgs/2_1_3/output_52_0.png" height='auto' width='auto' title="caDesign">

---

注释（Notes）：

① scipy.stats.t，a Student’s T continuous random variable.（<https://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.stats.t.html>）。

② Chicago Data Portal，CDP，为芝加哥城开放数据门户，可免费下载数据用于相关分析，其中许多数据集每天至少更新一次或数次（<https://data.cityofchicago.org/>）。

参考文献（References）:

[1] Timothy C.Urdan(蒂莫西•C•厄丹),彭志文译.白话统计学[M].中国人民大学出版社.2013,12.第3版. <Timothy C. Urdan.Statistics in plain English[M]. New York: Routledge, 2010>.

[2] (日)高桥 信著,陈刚译.株式会社TREND-PRO漫画制作.漫画统计学[M].科学出版社.北京,2019.8.

[3] Dawn Griffiths.Head First Statistics: A Brain-Friendly Guide[M]. Sebastopol:O'Reilly Media.September, 2008.
