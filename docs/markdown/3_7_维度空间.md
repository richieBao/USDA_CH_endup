> Created on Mon Jul 17 19:26:58 2023 @author: Richie Bao-caDesign设计(cadesign.cn)

# 3.7 维度空间

## 3.7.1 流形学习（Manifold learning）


流形学习是一类借鉴了拓扑流形概念的降维方法，“流形”是在局部与欧式空间同胚的空间，即高维输入空间在局部具有欧式空间的性质，能用欧式距离来进行距离计算，这给降维方法带来启发，若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看起来复杂，但在局部上仍具有欧式空间的性质，因此，可以在局部建立降维映射关系，然后将局部映射关系推广至全局，实现降维或不同维度的映射<sup>[1]234</sup>。流形学习为非线性降维，包括将高维数据映射到低维潜在流形上的各种相关技术算法，其目的是在低维空间中可视化数据，或者学习映射发现数据的内在特征结构，这包括从高维到低维的嵌入，也可为从低维到高维的映射<sup>[2, 3]</sup>。


`sklearn`库的`manifold`模块集成了`Isomap`、`Locally Linear Embedding`、`Modified Locally Linear Embedding`、`Hessian Eigenmapping`、`Spectral Embedding`、`Local Tangent Space Alignment`、`Multi-dimensional Scaling (MDS)`和`t-distributed Stochastic Neighbor Embedding (t-SNE)`等多种算法。


```python
%load_ext autoreload 
%autoreload 2 
%reload_ext autoreload
import usda.manifold as usda_manifold
import usda.demo as usda_demo
from usda.database import gpd2postSQL,postSQL2gpd
import usda.utils as usda_utils
import usda.data_visual as usda_vis
import usda.pano_projection_transformation as usda_pano
import usda.models as usda_model

from sklearn.datasets import load_digits
from sklearn import manifold
from sklearn.datasets import make_moons,make_circles
from matplotlib import pyplot as plt, colors

import numpy as np
import pandas as pd
from scipy.linalg import eigh,eig,det
from numpy import linalg as LA

from scipy.spatial.distance import pdist, squareform
from sklearn.decomposition import KernelPCA
import plotly.graph_objects as go

import os
import pickle
from tqdm import tqdm
import random
import  matplotlib

from sklearn import covariance
from sklearn import cluster
import geopandas as gpd
from matplotlib.collections import LineCollection

import pylab
import umap.umap_ as umap
import plotly.express as px
from sklearn.cluster import DBSCAN
```

以`sklearn`库的[Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py)<sup>①</sup>示例为引，查看比较各类嵌入技术在数字数据集上的表现结果。


```python
digits = load_digits(n_class=6)
X, y = digits.data, digits.target
n_samples, n_features = X.shape
print(X.shape)
n_neighbors = 30

fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))
for idx, ax in enumerate(axs.ravel()):
    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)
    ax.axis("off")
_ = fig.suptitle("A selection from the 64-dimensional digits dataset", fontsize=16)
```

    (1083, 64)
    


<img src="./imgs/3_7/output_4_1.png" height='auto' width='auto' title="caDesign">    


```python
projections,timing,embeddings=usda_manifold.manifold_learning_illustration(X,y) 
```

    Computing Random projection embedding...
    Computing Truncated SVD embedding...
    Computing Linear Discriminant Analysis embedding...
    Computing Isomap embedding...
    Computing Standard LLE embedding...
    Computing Modified LLE embedding...
    Computing Hessian LLE embedding...
    Computing LTSA LLE embedding...
    Computing MDS embedding...
    Computing Random Trees embedding...
    Computing Spectral embedding...
    Computing t-SNE embeedding...
    Computing NCA embedding...
    

打印查看 64 维的数字图像映射到 2 维平面空间上的分布。


```python
fig, axes = plt.subplots(4,4,figsize=(20,20))

for idx,name in enumerate(timing):
    title = f"{name} (time {timing[name]:.3f}s)"
    ax=axes.flatten()[idx]
    usda_manifold.plot_embedding(projections[name],y,digits,title,ax)
    
for i in range(16-13):
    axes.flatten().flat[-(i+1)].set_visible(False)

fig.tight_layout()
plt.show()
```


<img src="./imgs/3_7/output_7_0.png" height='auto' width='auto' title="caDesign">    
    


## 3.7.2 MDS，Multidimensional Scaling（多维尺度变换）和 SMACOF（Scaling by MAjorizing a COmplicated Function） 算法<sup>[4, 5]</sup>

### 3.7.2.1 MDS 算法解析

多维尺度变换可以分为度量型（Metric MDS）和非度量型（Non-metric MDS），通常情况下，度量型样本点之间距离为真实距离，例如可以表征空间位置的欧几里得距离；非度量型样本点之间的距离或不相似性通常反应样本点之间的排序而不是真实距离，例如鸢尾花数据特征之间的距离等。关于不同方式的距离度量可以查看*标记距离*部分。

一个数据集有 N 个样本（或图$G$的顶点/节点），用集合表示为$\mathcal{N}:=\{1, \ldots, N\}$，如果给定样本点之间的两两距离（distances）或不相似性（dissimilarities）$\left\{\delta_{m n}\right\}_{(m, n) \in \mathcal{E}}$，式中$\mathcal{E} \subseteq\{(m, n) \mid 1 \leq m<n \leq N\}$，那么经典 MDS 是寻求 P-维嵌入向量（embedding vectors）$\left\{\mathbf{x}_n\right\}_{n=1}^N $，嵌入向量$\mathbf{X} \in \mathbb{R}^{N \times P}$，通过求解以下非凸（non-convex）优化问题进行估计：

$ \hat{\mathbf{X}}=\arg \min _{\mathbf{X}} \sum_{1 \leq m<n \leq N} w_{m n}\left(\delta_{m n}-\left\|\mathbf{x}_m-\mathbf{x}_n\right\|_2\right)^2$ （1）

式中$w_{m n}$是与$\delta_{m n}$度量相关联的权重，并且如果$(m, n) \notin \mathcal{E}$，$w_{m n}$则为0。对于非零的权重，根据应用目的，可以通过多种方式选择，通常简单的设置为1。公式（1）中的目标函数（objective function）称为应力函数（stress function），表示为$\sigma(\mathbf{X})$。可以看到，式（1）中得到的最优$\hat{\mathbf{X}}$并不唯一，且表现出平移、旋转和反射的模糊性。

式（1）中的应力最小问题是非凸的，可以用 SMACOF 算法求解到局部最优，展开应力函数得到：

$\sigma(\mathbf{X})=\sum_{m<n} w_{m n}\left(\delta_{m n}^2+\left\|\mathbf{x}_m-\mathbf{x}_n\right\|^2-2 \delta_{m n}\left\|\mathbf{x}_m-\mathbf{x}_n\right\|\right)$    （2）

$=\sum_{m<n} w_{m n} \delta_{m n}^2+\operatorname{tr}\left(\mathbf{X}^T \mathbf{L} \mathbf{X}\right)-2 \operatorname{tr}\left(\mathbf{X}^T \mathbf{B}(\mathbf{X}) \mathbf{X}\right)$   （3）

式中，

$ {[\mathbf{L}]_{m n}= \begin{cases}-w_{m n} & m \neq n \\
\sum_{k=1}^m w_{m k} & m=n\end{cases} }$                               （4）

${[\mathbf{B}(\mathbf{X})]_{m n}= \begin{cases}-\frac{w_{m n} \hat{\delta}_{m n}}{\left\|\mathbf{x}_m-\mathbf{x}_m\right\|} & m \neq n, \mathbf{x}_m \neq \mathbf{x}_n \\
0 & m \neq n, \mathbf{x}_m=\mathbf{x}_n \\
-\sum_{k=1}^m[\mathbf{B}(\mathbf{X})]_{m k} & m=n\end{cases} } $                                      (5)

SMACOF 算法的工作原理是用线性函数迭代的最大化式（3）的最后一项，进而相对于$\mathbf{X}$为最小化应力函数。初始$\hat{\mathbf{X}}^{(0)}$为随机值，随后每$k$次迭代的 SMACOF 更新执行下述公式，

$\hat{\mathbf{X}}^{(k+1)}-\arg \min _{\mathbf{X}} \operatorname{tr}\left(\mathbf{X}^T \mathbf{L X}\right)-2 \operatorname{tr}\left(\mathbf{X}^T \mathbf{B}\left(\mathbf{X}^{(k)}\right) \hat{\mathbf{X}}^{(k)}\right) $  （6）


$=\mathbf{L}^{\dagger} \mathbf{B}\left(\hat{\mathbf{X}}^{(k)}\right) \hat{\mathbf{X}}^{(k)}$  （7）

因为$\mathbf{B}(\mathbf{X}) \mathbf{X} $位于$\mathbf{L}$的值域空间（range space），因此有公式（7）。因为$\mathbf{L}$是秩亏（rank-deﬁcient）的，所以式（6）的解不唯一。然而，当权值$\left\{w_{m n}\right\}$指定一个全连接图$\mathcal{G}:=(\{1, \ldots, N\}, \mathcal{E})$，$\mathbf{L}$和$\mathbf{B}(\mathbf{X})$有秩$N-1$，且$\mathbf{L}$的零空间为1，因此式（6）任何解的形式为$\mathbf{L}^{\dagger} \mathbf{B}\left(\hat{\mathbf{X}}^{(k)}\right) \hat{\mathbf{X}}^{(k)}+\mathbf{1} c \text { for } c \in \mathbb{R}$。此外，如果$\mathbf{X}^{(0)}$初始值以原点为中心，即$\mathbf{1}^T \mathbf{X}^{(0)}=\mathbf{0}$，式（7）的更新需确保$\mathbf{1}^T \mathbf{X}^{(k)}=0 \text { for all } k \geq 1$。

`sklearn`库的[class sklearn.manifold.MDS](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html)<sup>②</sup>(n_components=2, *, metric=True, n_init=4, max_iter=300, verbose=0, eps=0.001, n_jobs=None, random_state=None, dissimilarity='euclidean', normalized_stress='warn')实现了 MDS 流形学习（Manifold Learning），[sklearn.manifold.smacof](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.smacof.html)<sup>③</sup>(dissimilarities, *, metric=True, n_components=2, init=None, n_init=8, n_jobs=None, max_iter=300, verbose=0, eps=0.001, random_state=None, return_n_iter=False, normalized_stress='warn')提供了`SMACOF`算法，[对应上述公式的核心代码](https://github.com/scikit-learn/scikit-learn/blob/7f9bad99d/sklearn/manifold/_mds.py#L372)<sup>④</sup>为：

```python
    if init is None:
        # Randomly choose initial configuration
        X = random_state.uniform(size=n_samples * n_components)
        X = X.reshape((n_samples, n_components))
    else:
        # overrides the parameter p
        n_components = init.shape[1]
        if n_samples != init.shape[0]:
            raise ValueError(
                "init matrix should be of shape (%d, %d)" % (n_samples, n_components)
            )
        X = init

    old_stress = None
    ir = IsotonicRegression()
    for it in range(max_iter):
        # Compute distance and monotonic regression
        dis = euclidean_distances(X)

        if metric:
            disparities = dissimilarities
        else:
            dis_flat = dis.ravel()
            # dissimilarities with 0 are considered as missing values
            dis_flat_w = dis_flat[sim_flat != 0]

            # Compute the disparities using a monotonic regression
            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)
            disparities = dis_flat.copy()
            disparities[sim_flat != 0] = disparities_flat
            disparities = disparities.reshape((n_samples, n_samples))
            disparities *= np.sqrt(
                (n_samples * (n_samples - 1) / 2) / (disparities**2).sum()
            )

        # Compute stress
        stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2
        if normalized_stress:
            stress = np.sqrt(stress / ((disparities.ravel() ** 2).sum() / 2))
        # Update X using the Guttman transform
        dis[dis == 0] = 1e-5
        ratio = disparities / dis
        B = -ratio
        B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)
        X = 1.0 / n_samples * np.dot(B, X)

        dis = np.sqrt((X**2).sum(axis=1)).sum()
        if verbose >= 2:
            print("it: %d, stress %s" % (it, stress))
        if old_stress is not None:
            if (old_stress - stress / dis) < eps:
                if verbose:
                    print("breaking at iteration %d with stress %s" % (it, stress))
                break
        old_stress = stress / dis
```

* 距离计算

MDS方法`dissimilarity`参数默认计算为`euclidean`欧几里得距离，如果对于样本点$x_i$有特征值（坐标值）$x_{i 1}, \cdots, x_{i t}$，那么样本点$x_i$和$x_j$之间的欧几里得距离为$d_{i j}=\left[\sum_{l=1}^t\left(x_{i l}-x_{j l}\right)^2\right]^{1 / 2} $，在`sklearn`库中，由`sklearn.metrics.pairwise.euclidean_distances`方法计算；MDS 同样适用于一般距离度量函数，例如 MDS 作者 Kruskal, J. B.<sup>[6]</sup>论文中提到的 Minkowski $L_p$， 公式为$d_{i j}=\left[\sum_{l=1}^t\left|x_{i l}-x_{j l}\right|^r\right]^{1 / r}$。如果$r \geqq 1.0$，该距离函数满足三角不等式成为一个真实的距离函数；当$r=2.0$时，为欧几里得距离；$r=1.0$时，该度量为“城市街区距离”或“曼哈顿度量”（Manhattan metric），公式为$d_{i j}=\sum_{l=1}^t\left|x_{i l}-x_{j l}\right|$；当$r=\infty$时，公式变为$d_{i j}=\max _l\left|x_{i l}-x_{jl}\right|$。

* 单调回归（monotone regression）

MDS 的核心思想是转化后的嵌入空间（低维空间）样本点之间的距离保持与原始空间（高纬空间）样本点之间的距离一致或尽可能接近，那么最小化应力函数迭代时，嵌入空间样本点间距离的排序，可以通过单调回归，例如代码中的`IsotonicRegression()`保序回归（Isotonic Regression）方法保持其与原始空间样本点距离（排序）一致。

**Isotonic Regression**

Isotonic Regression 是将非递减实（值）函数拟合到一维数据中，求解如下问题：$minimize \sum_i w_i (y_i - \hat{y}_i)^2$，服从当$X_i \le X_j$时，$\hat{y}_i \le \hat{y}_j$，式中，权重$w_i$严格为正，$x$和$y$为任意实数。Isotonic Regression 依据均方误差由训练数据产生一系列接近目标值$y$的预测值$\hat y_i$。这些预测值经过插值以预测未知数据，如果超出$X$的最大和最小值，将以空值替代，因此 Isotonic Regression 的预测形成了一个分段函数。

下述为`sklearn`库提供的 [Isotonic Regression 示例](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_isotonic_regression.html)<sup>⑤</sup>，输入$X$为一个递增序列，输入$y$在整体递增趋势下局部次序随机变化，当应用 Isotonic Regression 后，预测值$\hat y$成为一个按照$C$排序的单调递增序列。


```python
_,_,ir=usda_demo.demo_isotonic_regression(n=50)
ir
```


<img src="./imgs/3_7/output_9_0.png" height='auto' width='auto' title="caDesign">   




<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>IsotonicRegression(out_of_bounds=&#x27;clip&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">IsotonicRegression</label><div class="sk-toggleable__content"><pre>IsotonicRegression(out_of_bounds=&#x27;clip&#x27;)</pre></div></div></div></div></div>



### 3.7.2.2 街道空间多维景观指数的 MDS 降维试验

使用*城市空间“语言”试验*视域景观指数和 POI 相关指数计算结果数据，对特征数 12 维的样本数据用 MDS 降维至2维，观察降维后样本点的分布情况。


```python
__C=usda_utils.AttrDict()
args=__C

__C.db=usda_utils.AttrDict() 
__C.db.UN='postgres'
__C.db.PW='123456'
__C.db.DB='visual_metrics'
__C.db.GC='geometry' 
__C.db.db_info=dict(geom_col=args.db.GC,myusername=args.db.UN,mypassword=args.db.PW,mydatabase=args.db.DB)

__C.epsg_xian=32649
```

读取数据。


```python
metrics_inall=postSQL2gpd(table_name='metrics_inall',**args.db.db_info)
metrics_inall.sort_values(by=['fn_idx'],inplace=True)
metrics_inall.dropna(inplace=True)
metrics_inall.reset_index(drop=True,inplace=True)
```

    __________________________________________________
    The data has been read from PostgreSQL database. The table name is metrics_inall.
    

MDS 降维。


```python
fields=['Green view index','Sky view factor', 'Ground view index','Equilibrium degree', 'Perimeter area ratio(mn)','Shape index(mn)', 'Fractal dimension(mn)', 'Color richness index','Key point size(0-10]', 'Key point size(10-20]','Key point size(30-40]', 'Key point size(20-30]',]
metrics_selection_df=metrics_inall[fields]

embedding=manifold.MDS(n_components=2, normalized_stress='auto')
X_transformed=embedding.fit_transform(metrics_selection_df.to_numpy())

metrics_inall[['embedding_x','embedding_y']]=pd.DataFrame(X_transformed)
# metrics_inall.to_crs(args.epsg_xian,inplace=True)
metrics_inall[['lon','lat']]=metrics_inall.apply(lambda row:[row.geometry.x,row.geometry.y],axis=1,result_type='expand')
metrics_inall.tail(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Green view index</th>
      <th>Sky view factor</th>
      <th>Ground view index</th>
      <th>Equilibrium degree</th>
      <th>number of patches</th>
      <th>Perimeter area ratio(mn)</th>
      <th>Shape index(mn)</th>
      <th>Fractal dimension(mn)</th>
      <th>Color richness index</th>
      <th>Key point size(0-10]</th>
      <th>...</th>
      <th>fn_key</th>
      <th>fn_idx</th>
      <th>geometry</th>
      <th>frank_e</th>
      <th>num</th>
      <th>num_diff</th>
      <th>embedding_x</th>
      <th>embedding_y</th>
      <th>lon</th>
      <th>lat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>563</th>
      <td>8.876533</td>
      <td>34.406317</td>
      <td>49.708967</td>
      <td>49.21693</td>
      <td>223.0</td>
      <td>248.194856</td>
      <td>1.249436</td>
      <td>1.033388</td>
      <td>58.475149</td>
      <td>205</td>
      <td>...</td>
      <td>dongxistreet</td>
      <td>579</td>
      <td>POINT (108.96774 34.26069)</td>
      <td>64.362740</td>
      <td>41</td>
      <td>-1.0</td>
      <td>-19.157652</td>
      <td>95.471806</td>
      <td>108.967744</td>
      <td>34.260691</td>
    </tr>
    <tr>
      <th>564</th>
      <td>8.703033</td>
      <td>33.874200</td>
      <td>50.756650</td>
      <td>49.15724</td>
      <td>188.0</td>
      <td>270.015482</td>
      <td>1.202361</td>
      <td>1.026970</td>
      <td>64.774178</td>
      <td>146</td>
      <td>...</td>
      <td>dongxistreet</td>
      <td>580</td>
      <td>POINT (108.96779 34.26075)</td>
      <td>63.841717</td>
      <td>42</td>
      <td>-34.0</td>
      <td>-63.913861</td>
      <td>138.190518</td>
      <td>108.967795</td>
      <td>34.260749</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 24 columns</p>
</div>



通过指数聚类贡献度计算得知绿视率（Green view index，GVI）分值最高，因此数据打印时，样本点颜色表示为 GVI 数值。同时，折线部分为按样本点实际地理位置坐标标准化后相对位置打印结果；折现上部的散点为 MDS 降维至2维后值的标准化相对位置。降维后嵌入空间（2维）样本点的两两距离应该接近于原空间（12 维）样本点的两两距离，这从结果打印中也能够观察到，具有相似值的 GVI 样本点在嵌入空间中彼此相互靠近成簇。


```python
norm_lon=usda_utils.normalize_by_meanNstd(metrics_inall[['lon']])[0]
norm_lat=usda_utils.normalize_by_meanNstd(metrics_inall[['lat']])[0]

norm_x=usda_utils.normalize_by_meanNstd(metrics_inall[['embedding_x']])[0]
norm_y=usda_utils.normalize_by_meanNstd(metrics_inall[['embedding_y']])[0]+10
```


```python
fig,ax=plt.subplots(figsize=(20,5))

color=usda_vis.vals4color_cmap(metrics_inall['Green view index'].values)
ax.scatter(norm_lon,norm_lat,s=1,color=color)
ax.scatter(norm_x,norm_y,s=5,color=color,)

for idx,(s_lon,s_lat,e_x,e_y) in pd.concat([norm_lon,norm_lat,norm_x,norm_y],axis=1).iterrows():
    dx=e_x-s_lon
    dy=e_y-s_lat
    ax.arrow(s_lon,s_lat,dx,dy,width=0.001,head_width=0.01,head_length=0.01,color='gray',alpha=0.2,linestyle='--')
    
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.show()
```

<img src="./imgs/3_7/output_18_0.png" height='auto' width='auto' title="caDesign">
    


## 3.7.3 KPCA（Kernel to PCA（Principal Component Analysis））（核主成分分析）

### 3.7.3.1 特征值和特征向量几何意义图解

* 特征值和特征向量

在流形学习诸多方法中均涉及到线性变换，将向量左乘变换矩阵，使得该向量的长度和方向发生变化，投射到一个新的空间位置。KPCA 流形学习方法则应用到线性变换（映射）下特征向量和特征值的计算。对于线性变换可以查看*编程线性代数*部分，这里进一步图示特征值和特征向量的空间几何意义，有助于对流形学习各类算法的理解。

在线性代数中，线性变换的特征向量（eigenvector，characteristic vector）是一个非零向量，对应每一特征向量存在一个标量，即特征值（eigenvalue），用$\lambda$表示，例如对变换矩阵（transformation matrix）$A= \begin{bmatrix}2 & 1 \\1 & 2 \end{bmatrix} $，用`numpy`或`SciPy`库提供的工具计算特征值和特征向量，得到特征值为1和3， 对应特征值的特征向量为`[-0.707,0.70710678]`和`[0.707,0.707]`（为矩阵的列，` eigenvectors[:, i]`）。在几何中，变换矩阵旋转、拉伸、或剪切它作用的向量，而特征值是拉伸特征向量的因子。如果特征值为负，则反方向拉伸。

定义：设$A$是$n$阶矩阵，如果数$\lambda$和$n$维非零列向量$x$使关系式$Ax=\lambda x$成立，那么，这样的数$\lambda$称为矩阵$A$的特征值，非零向量$x$称为$A$对应于特征值$\lambda$的特征向量<sup>[7]117</sup>。`A @ x == eigvals[1] * x`结果为真，验证了该定义关系式$Ax=\lambda x$，即转换矩阵$A$把特征向量$x$转化成它自身的标倍量（scalar multiple）$\lambda$。


```python
A=np.array([[2,1],[1,2]])
eigvals, eigvecs=LA.eigh(A)
print(eigvals, '\n',eigvecs)

print('-'*50)
x=eigvecs[:,1].reshape(2,1)
print(A @ x == eigvals[1] * x)
```

    [1. 3.] 
     [[-0.70710678  0.70710678]
     [ 0.70710678  0.70710678]]
    --------------------------------------------------
    [[ True]
     [ True]]
    

* 如果$\lambda$为实数（real）且$\lambda > 1$，则转化矩阵$A$具有将向量$x$拉伸特征值$\lambda$倍的效果。如下图左1，将向量`[3,3]`，拉伸了3倍，变为向量`[9，9]`；

* 如果$0<\lambda <1$，则转化矩阵$A$具有将向量$x$缩小特征值$\lambda$倍的效果。如下图左2，将向量`[3,3]`，缩小了0.5倍，变为向量`[1.5,1.5]`；

* 如果$\lambda <0$，效果缩放大小同前2条，但是方向取反。如下图左3，将向量`[-8,2]`，缩小了0.5倍，变为向量`[4,-1]`<sup>[8]451</sup>；

如果向量不位于特征向量的方向上，例如下图右1向量`[-8,2]`，经转换矩阵的缩放同时，也发生旋转，变为向量`[-14,-4]`。


```python
fig, axs=plt.subplots(1, 4,figsize=(20,4.5))
usda_demo.demo_eigvals_eigvecs_1d(np.array([[2,1],[1,2]]) ,np.array([3,3]),ax=axs[0])     
usda_demo.demo_eigvals_eigvecs_1d(np.array([[0.5,0],[0,0.5]]) ,np.array([3,3]),ax=axs[1])
usda_demo.demo_eigvals_eigvecs_1d(np.array([[-0.5,0],[0,-0.5]]) ,np.array([-8,2]),ax=axs[2])
usda_demo.demo_eigvals_eigvecs_1d(np.array([[2,1],[1,2]]) ,np.array([-8,2]),ax=axs[3])
plt.show()
```


<img src="./imgs/3_7/output_22_0.png" height='auto' width='auto' title="caDesign">    


类似上述单个向量（1维）的变换，同时变换向量矩阵（2维）观察图像变化，同样用变换矩阵$A= \begin{bmatrix}2 & 1 \\1 & 2 \end{bmatrix} $试验。从下图得知，位于2个特征向量方向上的点沿所属方向缩放，如点1和4，点2（重合）和3；点5不位于任一特征向量方向上，因此除了缩放，也发生了旋转。


```python
usda_demo.demo_eigvals_eigvecs_2d()
```

    A:
    [[2 1]
     [1 2]]
    eigenvalue:
    [1. 3.]
    eigenvector:
    [[-0.70710678  0.70710678]
     [ 0.70710678  0.70710678]]
    

<img src="./imgs/3_7/output_24_1.png" height='auto' width='auto' title="caDesign">
 

* 对角化和特征分解（Diagonalization and the eigendecomposition）

假设变换矩阵$A$的特征向量构成一组基向量（Base Vector），即$A$有$n$个线性无关的特征向量$v_1,v_2,\cdots,v_n$，对应的特征值为$\lambda_1,\lambda_2,\cdots,\lambda_n$。定义一个方阵$Q$，其列向量是$A$的$n$个线性无关的特征向量，$Q=\left[\begin{array}{llll}\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n\end{array}\right] $；因为$Q$的每一个列都是$A$的特征向量，因此$A Q=\left[\begin{array}{llll}\lambda_1 \mathbf{v}_1 & \lambda_2 \mathbf{v}_2 & \cdots & \lambda_{\mathrm{n}} \mathbf{v}_n\end{array}\right]$；定义一个对角矩阵$\Lambda$，其中每一个对角元素$\Lambda_{ij}$是与$Q$第$i$列相关联的特征值，有$A Q=Q \Lambda$；因为$Q$的列向量线性无关，所以$Q$是可逆的，等式两边同时乘以$Q$有$A=Q A Q^{-1} $，或者为$Q^{-1} A Q=\Lambda$。

因此，变换矩阵$A$可以分解成一个由它的特征向量组成的矩阵和一个特征值沿着对角线的对角矩阵，及特征向量矩阵的逆矩阵，即为特征分解。本质上，变换矩阵$A$和对角矩阵$\Lambda$为用两个不同的基表示的同一线性变换。当将线性变换表示为$\Lambda$时，特征向量被用作基<sup>[9]</sup>。
    
用`SciPy`库的`eig`方法分解变换矩阵$A$，并用`U @ np.diag(D) @ np.linalg.inv(U)`验证了特征分解的结果。    


```python
A=np.array([[2,1],[1,2]])
D,U=eig(A)
print(D,'\n',U,'\n','-'*50)
print(U @ np.diag(D) @ np.linalg.inv(U))
```

    [3.+0.j 1.+0.j] 
     [[ 0.70710678 -0.70710678]
     [ 0.70710678  0.70710678]] 
     --------------------------------------------------
    [[2.+0.j 1.+0.j]
     [1.+0.j 2.+0.j]]
    

### 3.7.3.2 核函数（kernel function）与核技巧（kernel trick）<sup>[10]</sup>

核函数定义，设$ \mathcal{X}$是输入空间（input space）（欧式空间$\mathbf{R}^n$的子集或离散集合），又设$\mathcal{H}$为特征空间（feature space）（希尔伯特空间（Hilbert space）），如果存在一个从$ \mathcal{X}$到$\mathcal{H}$的映射$\phi(x): \mathcal{X} \rightarrow \mathcal{H}$，使得对所有$x, z \in \mathcal{X}$，函数$K(x, z)$满足条件$K(x, z)=\phi(x) \cdot \phi(z)$，则称$K(x, z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x) \cdot \phi(z)$为$\phi(x)$和$\phi(z)$的内积。

核技巧的想法是，在学习与预测中只定义核函数$K(x, z)$，而不显示地定义映射函数$ \phi$，通常，直接计算$K(x, z)$比较容易，而通过$\phi(x)$和$\phi(z)$计算$K(x, z)$并不容易实现。$\phi$是输入空间$mathbf{R}^n$到特征空间$\mathcal{H}$的映射，特征空间$\mathcal{H}$一般为高维空间，甚至无穷维。对于给定的核$K(x, z)$，特征空间$\mathcal{H}$和映射函数$\phi$的取法并不唯一，可以取不同的特征空间，即便同一特征空间里也可以取不同的映射。

通常核函数是正定核函数（positive definite kernel function），常用的核函数有多项式核函数（polynomial kernel function）、高斯核函数（Gaussian kernel function）（或径向基函数（radial basis function，RBF））和双曲正切核函数（hyperbolic tangent (sigmoid) kernel function）等。多项式核函数公式为$\kappa\left(x^{(i)}, x^{(j)}\right)=\left(x^{(i)^T} x^{(j)}+\theta\right)^p$，式中，$\theta$为阈值，$p$为指数值，均由用户指定；RBF 公式为$\kappa\left(\boldsymbol{x}^{(i)}, \boldsymbol{x}^{(j)}\right)=\exp \left(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^2}{2 \sigma^2}\right)$，引入变量$\gamma=\frac{1}{2 \sigma^2}$，则公式书写为$\kappa\left(x^{(i)}, x^{(j)}\right)=\exp \left(-\gamma\left\|x^{(i)}-x^{(j)}\right\|^2\right) $；双曲正切核函数公式为$\kappa\left(\boldsymbol{x}^{(i)}, \boldsymbol{x}^{(j)}\right)=\tanh \left(\eta \boldsymbol{x}^{(i)^T} \boldsymbol{x}^{(j)}+\theta\right)$。

### 3.7.3.3 KPCA<sup>[11][12]169-179</sup>算法解析

KPCA 是基于 PCA 的一种非线性形式，将$d$维的样本（解释变量）数据通过核函数（Kernel function）映射到一个更高维度的特征空间（feature space）(记为$\mathcal{F}$）中实现类的线性可分，再应用 PCA 方法降维。

* 映射

将样本数据$x \in \mathbb{R}^k$映射到$d$维子空间上，定义非线性映射函数$\phi: \mathbb{R}^k \rightarrow \mathbb{R}^d \quad(d \gg k) $。例如，样本集$X$为包含2个特征（$k=2$）的列向量，将其映射到3维空间中可以表述为：$\mathbf{x}=\left[x_1, x_2\right]^T \\ \downarrow \phi \\ \mathbf{z}=\left[x_1^2, \sqrt{2 x_1 x_2}, x_2^2\right]^T$。如下图对应的2维输入空间样本到3维特征空间的映射结果，在高维特征空间实现样本类的线性可分，表面映射函数$\phi$能够处理非线性问题。


```python
usda_demo.demo_KPCA_2d_to_3d_mapping()
```


<img src="./imgs/3_7/output_29_0.png" height='auto' width='auto' title="caDesign">   


* 特征空间的特征向量

定义原始输入空间样本$\mathbf{X}=\left[\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_{\mathbf{n}}\right] $，每一列表示一个样本，每个样本点$\mathbf{x}_i$为$K$维列向量，$\mathbf{X}$中共$\mathbf{N}$个样本，为$K \times N$大小矩阵。将$\mathbf{X}$用映射函数$\phi$映射到高维特征空间$\mathcal{F}$中，得到一个$D \times N$大小的新矩阵$\phi(\mathbf{X})=\left[\phi\left(\mathbf{x}_1\right), \phi\left(\mathbf{x}_2\right), \ldots, \phi\left(\mathbf{x}_{\mathbf{n}}\right)\right]$。

在 PCA 中，两个特征$k$和$j$之间的协方差为$\sigma_{j k}=\frac{1}{n} \sum_{i=1}^n\left(x_j^{(i)}-\mu_j\right)\left(x_k^{(i)}-\mu_k\right)$；标准中心化样本特征，使得$\mu_j=0$和$\mu_k=0$，简化协方差公式为$\sigma_{j k}=\frac{1}{n} \sum_{i=1}^n x_j^{(i)} x_k^{(i)}$，即为样本的点积（内积）；表述为一般形式$\Sigma=\frac{1}{n} \sum_{i=1}^n x^{(i)} x^{(i)^T}=\frac{1}{n} \mathbf{X}^{\mathbf{T}} \mathbf{X} $。

由上述公式，映射到特征空间的样本协方差为$\Sigma=\frac{1}{n} \sum_{i=1}^n \phi\left(x^{(i)}\right) \phi\left(x^{(i)}\right)^T=\frac{1}{n} \phi(\boldsymbol{X})^T \phi(\boldsymbol{X})$，为一个$D \times D$的矩阵，为获得特征向量（主成分），从协方差矩阵推导公式有：

$\Sigma v=\lambda v \\ \Rightarrow \frac{1}{n} \sum_{i=1}^n \phi\left(x^{(i)}\right) \phi\left(x^{(i)}\right)^T v=\lambda v \\ \Rightarrow \frac{1}{n} \phi(\boldsymbol{X})^T \phi(\boldsymbol{X})v=\lambda v（1）$，式中，$\lambda$和$v$为协方差矩阵$\Sigma$的特征值和特征向量。因为没有显示的定义映射函数$\phi$，无法计算$\phi\left(x^{(i)}\right) \phi\left(x^{(i)}\right)^T$，因此（1）式无法直接求解，需要借助核技巧解决。

* 应用核函数与核技巧

继续推导（1）式有$v=\frac{1}{n \lambda} \sum_{i=1}^n \phi\left(x^{(i)}\right) \phi\left(x^{(i)}\right)^T v$，因为式中$\phi\left(x^{(i)}\right)^T v$为标量，因此有$v=\frac{1}{n} \sum_{i=1}^n \boldsymbol{a}^{(i)} \phi\left(x^{(i)}\right)=\lambda \phi(\boldsymbol{X})^T \boldsymbol{a} （2）$，式中$\boldsymbol{a}$为$N$维列向量，$\alpha=\left[\alpha_1, \alpha_2, \ldots, \alpha_n\right]^T$；

将（2）式带回（1）式得$\frac{1}{n} \phi(\boldsymbol{X})^T \phi(X) \phi(\boldsymbol{X})^T \boldsymbol{a}=\lambda \phi(\boldsymbol{X})^T \boldsymbol{a} （3）$；

将（3）式等号两侧均乘以$\phi(\boldsymbol{X})$得：

$\frac{1}{n} \phi(\boldsymbol{X}) \phi(\boldsymbol{X})^T \phi(\boldsymbol{X}) \phi(\boldsymbol{X})^T \boldsymbol{a}=\lambda \phi(\boldsymbol{X}) \phi(\boldsymbol{X})^T \boldsymbol{a} \\ \Rightarrow \frac{1}{n} \phi(\boldsymbol{X}) \phi(\boldsymbol{X})^T \boldsymbol{a}=\lambda \boldsymbol{a} \\ \Rightarrow \frac{1}{n} \boldsymbol{K} \boldsymbol{a}=\lambda \boldsymbol{a}$，式中，$\boldsymbol{K}$为相似性（similarity）（核）矩阵，$\boldsymbol{K}=\phi(\boldsymbol{X}) \phi(\boldsymbol{X})^T$。

* KPCA 降维的代码实现

1. 计算输入空间样本的欧几里德距离的平方，用`ScipPy`库的`pdist`n维空间中观测值之间的成对距离计算方法实现，对应代码为

```python
sq_dists = pdist(X, 'sqeuclidean')
```

2. 将两两距离转换成一个方阵，对应代码为

```python
mat_sq_dists = squareform(sq_dists)
```


3. 用高斯核函数计算核矩阵$\boldsymbol{K}$，对应代码为

```python
K = exp(-gamma * mat_sq_dists)
```

4. 使用公式$K^{\prime}=K-\mathbf{1}_n K-K 1_n+\mathbf{1}_n K 1_n$，中心化$\boldsymbol{K}$，对应代码为

```python
N = K.shape[0]
one_n = np.ones((N,N)) / N
K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)
```

5. 计算$\boldsymbol{K}$的特征向量和特征值，对应代码为

```python
eigvals, eigvecs = eigh(K)
eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]
```

6. 返回前$n$个特征向量，对应代码为

```python
X_pc = np.column_stack([eigvecs[:, i] for i in range(n_components)])
```

试验数据计算结果如下。


```python
X, y = make_moons(n_samples=500,noise=0.05)
X_kpca,eigvals,eigvecs,K=usda_manifold.rbf_kernel_pca(X, gamma=15, n_components=2)
print(eigvals.shape,eigvecs.shape,K.shape)

fig,axes=plt.subplots(1,2,figsize=(8,4))
axes[0].scatter(X[y==0, 0], X[y==0, 1],color='orangered', marker='o', alpha=0.5)
axes[0].scatter(X[y==1, 0], X[y==1, 1],color='dodgerblue', marker='o', alpha=0.5)

axes[1].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='orangered', marker='o', alpha=0.5)
axes[1].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],color='dodgerblue', marker='o', alpha=0.5)
plt.tight_layout()
plt.show()
```

    (500,) (500, 500) (500, 500)
    


<img src="./imgs/3_7/output_31_1.png" height='auto' width='auto' title="caDesign">    


### 3.7.3.4 街道空间单一景观指数的 KPCA 映射试验

同样使用*城市空间“语言”试验*视域景观指数和 POI 相关指数计算结果数据，但仅选择 POI 均衡度`frank_e`一个指数，通过将该一个维度的样本数据映射到特征空间后，再降维到2个维度，观察样本点线性可分的分离状态。KPCA 方式的维度变换结果可以辅助观察样本点自身属性特征的集聚分布结构，有助于对城市某一属性的空间分布特征做出判断。

使用`sklearn`库提供的`KernelPCA`方法计算，计算时如果配置`n_components=None`，返回值将为特征向量的所有方向。


```python
fields=['frank_e']
metrics_selection_df=metrics_inall[fields]

kernel_pca=KernelPCA(n_components=None, kernel="rbf", gamma=15, fit_inverse_transform=True, alpha=0.1)
kernel_pca=kernel_pca.fit_transform(metrics_selection_df.to_numpy())
print(kernel_pca.shape)
kernel_pca_df=pd.DataFrame(kernel_pca[:,:2],columns=['x','y'])
print(kernel_pca_df.shape)

norm_x=usda_utils.normalize_by_meanNstd(kernel_pca_df[['x']])[0]
norm_y=usda_utils.normalize_by_meanNstd(kernel_pca_df[['y']])[0]+10

kernel_pca_df[['lon','lat']]=metrics_inall.apply(lambda row:[row.geometry.x,row.geometry.y],axis=1,result_type='expand')
norm_lon=usda_utils.normalize_by_meanNstd(kernel_pca_df[['lon']])[0]
norm_lat=usda_utils.normalize_by_meanNstd(kernel_pca_df[['lat']])[0]
```

    (565, 249)
    (565, 2)
    

下述可视化图表样本点间的边也表述了均衡度值的大小，浅色的值小，表明该区域 POI 行业服务类型单一；深色的值大，表明该区域 POI 行业服务类型信息熵高，各类行业服务类型丰富均衡混杂。从 KPCA 映射结果中可以明显的分割成几段区域，代表样本点各自区域特征的显著性。


```python
fig,ax=plt.subplots(figsize=(20,5))

color4edges=usda_vis.vals4color_cmap(metrics_selection_df['frank_e'].values,cmap='gist_gray',vmin=metrics_selection_df['frank_e'].min(), vmax=metrics_selection_df['frank_e'].max())
for idx,(s_lon,s_lat,e_x,e_y) in pd.concat([norm_lon,norm_lat,norm_x,norm_y],axis=1).iterrows():
    dx=e_x-s_lon
    dy=e_y-s_lat
    ax.arrow(s_lon,s_lat,dx,dy,width=0.001,head_width=0.01,head_length=0.01,color=color4edges[idx],alpha=0.2,linestyle='--')

color=usda_vis.vals4color_cmap(metrics_selection_df['frank_e'].values,cmap='gist_ncar',vmin=metrics_selection_df['frank_e'].min(), vmax=metrics_selection_df['frank_e'].max())
ax.scatter(norm_lon,norm_lat,s=10,color=color)
ax.scatter(norm_x,norm_y,s=20,color=color,)
    
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.show()
```


<img src="./imgs/3_7/output_35_0.png" height='auto' width='auto' title="caDesign">    

提取前3个主成分，在三维空间中查看数据点的分布。


```python
fig = go.Figure(data=[go.Scatter3d(
    x=kernel_pca[:,0],
    y=kernel_pca[:,1],
    z=kernel_pca[:,2],
    mode='markers',
    hovertemplate='frank_e:<b>%{customdata[0]:.3f}</b><br><b>fn_idx:%{customdata[1]}</b><extra></extra>',
    customdata=metrics_inall[['frank_e','fn_idx']],
    marker=dict(
        size=12,
        color=color,  
        opacity=0.8,        
    )
)])

# tight layout
fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))
fig.show()
```

<img src="./imgs/3_7/3_7_04.png" height='auto' width='auto' title="caDesign">

## 3.7.4   Isomap

### 3.7.4.1 Isomap 算法解析

在研究全球气候模式、空间分布模式特征时，通常要寻找隐藏在高维观测中有意义的低维结构。Tenenbaum, J. B.等人<sup>[13]</sup>提出了 Isomap（isometric feature mappin） 降维的方法，该方法使用易于度量的局部度量信息（local metric information）来学习数据集潜在的全局几何结构（global geometry）。与经典的主成分分析（PCA）和多维尺度变换（MDS）等经典技术不同，Isomap 可以发现复杂的自然观测背后的非线性自由度，且有效的计算全局最优解，对重要的一类数据流形，保证渐近收敛到真实结构。

从下述 Isomap 试验生成的3维数据（“瑞士卷”，Swiss roll）可以观察到，两点间的最近距离不是直线距离，而是通过测地线（geodesic）或最短路径（shortest path）测量的距离。将其路径结果映射在2维平面上，获得了反应了流形的真实低维几何形状。距离度量上分为两步，第一步是根据指定的邻近点数量或者半径提取每个样本点的邻近点集合（可用`sklearn.neighbors.NearestNeighbors`方法计算），及计算样本点到各邻近点的距离（例如 欧几里德距离）（可用`sklearn.metrics.pairwise_distances`方法计算），即局部度量信息；第二步是根据样本点的邻近点距离矩阵，计算所有样本点两两间最短距离矩阵（可用`scipy.sparse.csgraph.shortest_path`方法计算），一般通常用 Floyd-Warshall 算法、 Dijkstra 算法，Bellman-Ford 算法或 Johnson 算法等，即全局几何结构。

获得了样本点两两间距离矩阵后，就可以用 MDS 或者 KPCA 等流形学习方法降维，例如`sklearn`提供的`Isomap`方法使用了 KPCA 的方式。


```python
d_isomap=usda_demo.Demo_Isomap(n_components=2)      
d_isomap.agglomerative_clustering()
d_isomap.isomap_G()
```

    Compute structured hierarchical clustering...
    Elapsed time: 0.06s
    Number of points: 1000
    

查看试验生成的三维原始数据，及示范的一个两点间的最短路径。


```python
d_isomap.plot_3d()
```

<img src="./imgs/3_7/output_41_0.png" height='auto' width='auto' title="caDesign">


用 Isomap 降维至2维，样本点间的距离保持了高维空间中的真实距离。


```python
d_isomap.plot_2d()
```


<img src="./imgs/3_7/output_43_0.png" height='auto' width='auto' title="caDesign">    



### 3.7.4.2 街道视域全景语义分割图的 Isomap 降维和嵌入空间的主成分分布

在前文的试验中是使用由街道视域全景语义分割图计算的各类视域景观指数和 POI 相关指数作为输入数据。此次则直接使用全景语义分割图作为数据输入，用 Isomap 降至2维后查看样本点的分布，并根据样本点的位置嵌入分割图像，查看图像在嵌入空间主成分的分布情况。

读取*城市空间“语言”试验*中生成的全景分割图标签数据。


```python
seg_label_root=r'G:\data\pano_dongxistreet\pano_seg\seg_label'
fns=os.listdir(seg_label_root)
data_lst=[]
for i, fn in tqdm(enumerate(fns)):
    with open(os.path.join(seg_label_root,fn),'rb') as f:
        seg_label=pickle.load(f)
    data_lst.append(seg_label)
    
random.shuffle(data_lst)    
data=np.stack(data_lst)    
print(data.shape)
print(data[0])
```

    566it [00:00, 1488.49it/s]

    (566, 512, 1024)
    [[22 22 22 ... 22 22 22]
     [22 22 22 ... 22 22 22]
     [22 22 22 ... 22 22 22]
     ...
     [27 27 27 ... 27 27 27]
     [27 27 27 ... 27 27 27]
     [27 27 27 ... 27 27 27]]
    

    
    

定义对应语义分割图标签的配色方案。


```python
levels=list(usda_pano.label_color_name.keys())+[28]
colors=[[i/255 for i in v[0]] for v in usda_pano.label_color_name.values()]
cmap, norm = matplotlib.colors.from_levels_and_colors(levels,colors)
cmap
```

<img src="./imgs/3_7/3_7_05.png" height='auto' width='auto' title="caDesign">

打印查看部分全景语义分割图。


```python
fig, ax = plt.subplots(3, 7, subplot_kw=dict(xticks=[], yticks=[]), figsize=(20,4))
for i, axi in enumerate(ax.flat):
    _ = axi.imshow(data[i], cmap=cmap);
```


<img src="./imgs/3_7/output_49_0.png" height='auto' width='auto' title="caDesign">    



定义`plot_components()`方法，传入流形学习模型（本例为 Isomap），将语义分割图嵌入到主成分分布位置，方便可视化分析不同样本点区域分割图像的特征。从分布结果来看，样本点的 Isomap 降维保留了高维空间样本点之间的距离关系（流形结构），可以根据降维后样本点的分布结构，应用聚类等手段推断提取样本的特征分类。


```python
fig, ax=plt.subplots(figsize=(20, 20))
data_=data.reshape(data.shape[0],-1)
usda_model.plot_components(data_, model=manifold.Isomap(n_components=2, n_neighbors=8), images=data, thumb_frac=0.1,cmap=cmap,ax=ax,img_scale=0.1)
```


<img src="./imgs/3_7/output_51_0.png" height='auto' width='auto' title="caDesign">    



## 3.7.5 LLE，Locally Linear Embedding（局部线性嵌入）

### 3.7.5.1 拉格朗日乘数/乘子法（Lagrange multiplier）

在数学优化问题中，拉格朗日乘子法（以数学家 Joseph-Louis Lagrange 命名）是一种寻找受一个或多个条件（方程）约束的（多元）函数局部最大值和最小值，即极值的策略。这种方法将一个有$n$个变量与$k$个约束条件的最优化问题转化为一个有$n+k$个变量的方程组极值问题，且其变量不再受任何约束。该方法由条件引入的新未知标量称为拉格朗日乘数，从而找到能让设出的隐函数微分为零的未知数的值。该方法概况如下：为找到受$g(x)=0$条件约束下$f(x)$函数的最大或最小值，构造拉格朗日函数（Lagrangian function），$\mathcal{L}(x, \lambda) \equiv f(x)+\lambda \cdot g(x)$，并找到以$x$和$\lambda $为函数$\mathcal{L}$的驻点（stationary points ），即极值点，该函数的所有偏导数为零，有$\frac{\partial \mathcal{L}}{\partial x}=0 \quad$ 并 $\quad \frac{\partial \mathcal{L}}{\partial \lambda}=0 $，相当于$\frac{\partial f(x)}{\partial x}+\lambda \cdot \frac{\partial g(x)}{\partial x}=0$ 并 $g(x)=0$<sup>[14]</sup>。
    
例如，最大化函数$f(x,y)=x+y$，并服从$x^2+y^2=1$。由条件方程得出$g(x, y)=x^2+y^2-1=0$，因此拉格朗日函数为$\mathcal{L}(x, y, \lambda)  =f(x, y)+\lambda \cdot g(x, y) \\ =x+y+\lambda\left(x^2+y^2-1\right)$，当$g(x,y)$为0时，等价于$f(x,y)$。
    
计算梯度有：$\nabla_{x, y, \lambda} \mathcal{L}(x, y, \lambda)  =\left(\frac{\partial \mathcal{L}}{\partial x}, \frac{\partial \mathcal{L}}{\partial y}, \frac{\partial \mathcal{L}}{\partial \lambda}\right) \\ =\left(1+2 \lambda x, 1+2 \lambda y, x^2+y^2-1\right)$，得出，$\nabla_{x, y, \lambda} \mathcal{L}(x, y, \lambda)=0 \Leftrightarrow\left\{\begin{array}{l} 1+2 \lambda x=0 \\ 1+2 \lambda y=0 \\ x^2+y^2-1=0 \end{array}\right.$，式中，最后一个方程为条件约束。由前两个方程得出，$x=y=-\frac{1}{2 \lambda}, \quad \lambda \neq 0$，将其结果代入最后一个方程得到$\frac{1}{4 \lambda^2}+\frac{1}{4 \lambda^2}-1=0$，因此$\lambda= \pm \frac{1}{\sqrt{2}}$。
    
由上述计算得出$\mathcal{L}$的驻点为$\left(\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2},-\frac{1}{\sqrt{2}}\right), \quad\left(-\frac{\sqrt{2}}{2},-\frac{\sqrt{2}}{2}, \frac{1}{\sqrt{2}}\right) $，代入目标函数$f(x,y)$，得到$f\left(\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}\right)=\sqrt{2}, \quad f\left(-\frac{\sqrt{2}}{2},-\frac{\sqrt{2}}{2}\right)=-\sqrt{2} $，即服从约束条件下的最大和最小值。
    
将该案例写入为代码，对应$\mathcal{L}$的公式为`x + y + L * (x**2 + y**2 - 1)`，用`SciPy`的`fsolve`方法解算，示例结果如下。


```python
usda_demo.demo_lagrange_multiplier_xy3D()
```


<img src="./imgs/3_7/output_53_0.png" height='auto' width='auto' title="caDesign">    



### 3.7.5.2 LLE 算法解析

许多科学领域依赖于探索性数据分析和可视化，因此对大量多元高维的数据分析提出了降维的需求，即找到高维数据的紧凑表示（compact representations）。Roweis, S. T.等人<sup>[15]</sup>提出了局部线性嵌入（locally linear embedding，LLE），一种无监督学习算法，用于降维计算。与局部降维的聚类方法不同，LLE 将其高维输入映射到一个低维的全局坐标系中，且其优化不涉及局部最小值。利用线性重构的局部对称性，LLE 能够学习非线性流形的全局结构。

下图<sup>[15]</sup>为 LLE 算法的图解，其计算主要分为3个步骤，

1. 计算每一个数据点$\vec{X}_{\mathrm{i}}$的近邻点，例如使用K-近邻模型（k-nearest neighbors algorithm，k-NN），可以指定近邻点数量 ；
2. 由$\vec{X}_{\mathrm{i}}$的近邻点线性组合加权重构$\vec{X}_{\mathrm{i}}$，通过求解均方误差损失函数（目标函数）$\varepsilon(W)=\sum_{\mathrm{i}}\left|\vec{X}_{\mathrm{i}}-\sum_{\mathrm{j}} W_{\mathrm{ij}} \vec{X}_{\mathrm{j}}\right|^2$  计算权重$W_{\mathrm{ij}}$；
3. 计算由$W_{\mathrm{ij}}$重构的低维嵌入向量$\vec{Y}_{\mathrm{i}}$，通过寻找$M_{\mathrm{ij}}=\delta_{\mathrm{ij}}-W_{\mathrm{ij}}-W_{\mathrm{ji}}+\sum_{\mathrm{k}} W_{\mathrm{ki}} W_{\mathrm{kj}}$ 中稀疏对称矩阵的的最小本征模（eigenmodes）来最小化$\Phi(Y)=\sum_{\mathrm{i}}\left|\vec{Y}_{\mathrm{i}}-\sum_{\mathrm{j}} W_{\mathrm{ij}} \vec{Y}_{\mathrm{j}}\right|^2$完成。

LLE 的基本思想是，在低维嵌入空间中使用与高维输入空间相同的构造权值。虽然权重$W_{\mathrm{ij}}$和向量$Y_{\mathrm{i}}$是用线性代数的方法计算得到，但是由数据点的近邻点线性重构的约束将导致高度非线性的嵌入。

<img src="./imgs/3_7/3_7_01.png" height='auto' width=500 title="caDesign"> 

对 LLE 算法的详细推导过程引用 Ghojogh, B.<sup>[16]</sup>等人对 LLE 综述的论文。

#### 1) $k$-近邻

利用数据点两两之间的欧式距离构建数据点的 kNN 最近邻网络，其中每个数据点有$k$个邻近点，$ \boldsymbol{x}_{i j} \in \mathbb{R}^d$表示数据点$x_i$的第$j$个近邻点；并且矩阵$\mathbb{R}^{d \times k} \ni \boldsymbol{X}_i:=\left[x_{i 1}, \ldots, x_{i k}\right]$包含$x_i$的$k$个近邻数据点。

#### 2）由近邻点线性重构

根据每个数据点的近邻点求出其线性重构的权值。在高维输入空间中，该线性重构的目标函数（表达式）为$\underset{\widetilde{\boldsymbol{W}}}{\operatorname{minimize}} \varepsilon(\widetilde{\boldsymbol{W}}):=\sum_{i=1}^n\left\|\boldsymbol{x}_i-\sum_{j=1}^k \widetilde{w}_{i j} \boldsymbol{x}_{i j}\right\|_2^2 \\ \text { subject to } \sum_{j=1}^k \tilde{w}_{i j}=1, \quad \forall i \in\{1, \ldots, n\}  （1）$ ，式中， $\mathbb{R}^{n \times k} \ni \widetilde{\boldsymbol{W}}:=\left[\tilde{\boldsymbol{w}}_1, \ldots, \tilde{\boldsymbol{w}}_{\mathrm{n}}\right]^{\top} $包括权值$\mathbb{R}^k \ni \widetilde{\boldsymbol{w}}_i:=\left[\widetilde{w}_{i 1}, \ldots, \tilde{w}_{i k}\right]^{\top} $，为第$i$个数据点的第$k$个邻近点线性重构的权值，且$\boldsymbol{x}_{i j} \in \mathbb{R}^d $为第$i$个数据点的第$j$个邻近点。

约束条件$\sum_{j=1}^k \widetilde{w}_{i j}=1$表示每个数据点的线性重构权值之和为1。注意，某些权值可能为负，且非常大的正负权值可以相互抵消，也使得权值总和为1，这一事实会导致某些权值爆炸的问题。然而，因为优化问题的解有一个封闭形式，权值爆炸的问题不会发生。如果迭代求解，则权重值会积累增长并可能导致爆炸。目标函数可重写为$\varepsilon(\widetilde{\boldsymbol{W}})=\sum_{i=1}^n\left\|\boldsymbol{x}_i-\boldsymbol{X}_i \tilde{\boldsymbol{w}}_i\right\|_2^2 （2）$。

约束条件$\sum_{j=1}^k \widetilde{w}_{i j}=1$意味着$\mathbf{1}^{\top} \tilde{\boldsymbol{w}}_i=1$，因此，$x_i=x_i 1^{\top} \tilde{w}_i$，简化目标函数$\varepsilon(\widetilde{\boldsymbol{W}}) $有，$ \left\|\boldsymbol{x}_i-\boldsymbol{X}_i \tilde{\boldsymbol{w}}_i\right\|_2^2=|| \boldsymbol{x}_i \mathbf{1}^{\top} \tilde{\boldsymbol{w}}_i-\boldsymbol{X}_i \tilde{\boldsymbol{w}}_i \|_2^2 \\  =||\left(\boldsymbol{x}_i \boldsymbol{1}^{\top}-\boldsymbol{X}_i\right) \tilde{\boldsymbol{w}}_i \|_2^2 \\ =\widetilde{\boldsymbol{w}}_i^{\top}\left(\boldsymbol{x}_i \mathbf{1}^{\top}-\boldsymbol{X}_i\right)^{\top}\left(x_i \mathbf{1}^{\top}-\boldsymbol{X}_i\right) \widetilde{\boldsymbol{w}}_i \\ =\widetilde{\boldsymbol{w}}_i^{\top} \boldsymbol{G}_i \widetilde{\boldsymbol{w}}_i $，式中，$\boldsymbol{G}_i$为格拉姆矩阵（gram matrix），定义为$\mathbb{R}^{k \times k} \ni \boldsymbol{G}_i:=\left(\boldsymbol{x}_i \mathbf{1}^{\top}-\boldsymbol{X}_i\right)^{\top}\left(\boldsymbol{x}_i \mathbf{1}^{\top}-\boldsymbol{X}_i\right) （3）$.

最终，公式（1）可以写为：$\underset{\left\{\tilde{w}_i\right\}_{i=1}^n}{\operatorname{minimize}} \sum_{i=1}^n \widetilde{\boldsymbol{w}}_i^{\top} \boldsymbol{G}_i \widetilde{\boldsymbol{w}}_i $， subject to $\mathbf{1}^{\top} \tilde{\boldsymbol{w}}_i=1, \quad \forall i \in\{1, \ldots, n\} （4）$。

对公式（4）利用拉格朗日乘子法，得到$\mathcal{L}=\sum_{i=1}^n \widetilde{\boldsymbol{w}}_i^{\top} \boldsymbol{G}_i \widetilde{\boldsymbol{w}}_i-\sum_{i=1}^n \lambda_i\left(\mathbf{1}^{\top} \widetilde{\boldsymbol{w}}_i-1\right) $。

将拉格朗日函数的导数设为0，给出，$\mathbb{R}^k \ni \frac{\partial \mathcal{L}}{\partial \widetilde{\boldsymbol{w}}_i}=2 \boldsymbol{G}_i \widetilde{\boldsymbol{w}}_i-\lambda_i \mathbf{1} \stackrel{\text { set }}{=} \mathbf{0}, \\ \Longrightarrow \widetilde{\boldsymbol{w}}_i=\frac{1}{2} \boldsymbol{G}_i^{-1} \lambda_i \mathbf{1}=\frac{\lambda_i}{2} \boldsymbol{G}_i^{-1} \mathbf{1}  （5）$

$\mathbb{R} \ni \frac{\partial \mathcal{L}}{\partial \lambda}=\mathbf{1}^{\top} \widetilde{\boldsymbol{w}}_i-1 \stackrel{\text { set }}{=} 0 \Longrightarrow \mathbf{1}^{\top} \widetilde{\boldsymbol{w}}_i=1 （6）$

由公式（5）和（6）得出，$\frac{\lambda_i}{2} \mathbf{1}^{\top} \boldsymbol{G}_i^{-1} \mathbf{1}=1 \Longrightarrow \lambda_i=\frac{2}{\mathbf{1}^{\top} \boldsymbol{G}_i^{-1} \mathbf{1}} （7）$

由公式（5）和（7）得出，$\widetilde{\boldsymbol{w}}_i=\frac{\lambda_i}{2} G_i^{-1} 1=\frac{G_i^{-1} 1}{\mathbf{1}^{\top} G_i^{-1} 1} （8）$。

由公式（3）可知，矩阵$\boldsymbol{G}_i \in \mathbb{R}^{k \times k}$的秩最多等于$min(k,d)$。如果$d<k$，$\boldsymbol{G}_i$是奇异的，且$\boldsymbol{G}_i$应该被替换为$\boldsymbol{G}_i+\epsilon \boldsymbol{I}$，式中，$\epsilon$是一个很小的正数。通常数据是高维的（$k \ll d)$），例如图像数据，因此如果$\boldsymbol{G}_i$是全秩的，对它进行反向不会有任何问题。这种对$\boldsymbol{G}$主对角线的强化在 LLE 中称为正则化。这种数值方法广泛应用于流形和子空间的学习中。

#### 3）线性嵌入

在上一步高维输入空间找到了局部数据点线性重构的权值，在低维嵌入空间使用与输入空间相同的权值来嵌入数据，这种线性嵌入可以表述为以下优化问题：

$\underset{\boldsymbol{Y}}{\operatorname{minimize}} \sum_{i=1}^n\left\|\boldsymbol{y}_i-\sum_{j=1}^n w_{i j} \boldsymbol{y}_j\right\|_2^2, \\ \text { subject to } \frac{1}{n} \sum_{i=1}^n y_i \boldsymbol{y}_i^{\top}=\boldsymbol{I} \\ \sum_{i=1}^n y_i=\mathbf{0}（9）$，式中，$\boldsymbol{I} $为单位矩阵。$\left.\mathbb{R}^{n \times p} \ni \boldsymbol{Y}:=\mid \boldsymbol{y}_1, \ldots, \boldsymbol{y}_n\right]^{\top}$为按行堆叠嵌入的数据点，$\boldsymbol{y}_i \in \mathbb{R}^p$是第$i$个嵌入的数据点，且$w_{i j}$是来自于高维输入空间线性重构的权值，为$x_i$近邻点$x_j$的权值，非近邻点的权值全部配置为0，$ w_{i j}:= \begin{cases}\widetilde{w}_{i j} & \text { if } \boldsymbol{x}_j \in k \mathrm{NN}\left(\boldsymbol{x}_i\right) \\ 0 & \text { otherwise. }\end{cases} （10）$

公式（9）中的第2个约束条件确保嵌入数据点的均值为0。第1个和第2个约束共同满足嵌入数据点有单位协方差（unit covariance）。

假设，$\mathbb{R}^n \ni \boldsymbol{w}_i:=\left[w_{i 1}, \ldots, w_{i n}\right]^{\top}$，且$\mathbb{R}^n \ni \mathbf{1}_i:=[0, \ldots, 1, \ldots, 0]^{\top}$为第$i$个元素为1，其它元素值均为0的向量。公式（9）目标函数可以重述为$\sum_{i=1}^n\left\|\boldsymbol{y}_i-\sum_{j=1}^n w_{i j} \boldsymbol{y}_j\right\|_2^2=\sum_{i=1}^n\left\|\boldsymbol{Y}^{\top} \mathbf{1}_i-\boldsymbol{Y}^{\top} \boldsymbol{w}_i\right\|_2^2$，用矩阵形式表示为$\sum_{i=1}^n\left\|\boldsymbol{Y}^{\top} \mathbf{1}_i-\boldsymbol{Y}^{\top} \boldsymbol{w}_i\right\|_2^2=\left\|\boldsymbol{Y}^{\top} \boldsymbol{I}-\boldsymbol{Y}^{\top} \boldsymbol{W}^{\top}\right\|_F^2 \\  =\left\|\boldsymbol{Y}^{\top}(\boldsymbol{I}-\boldsymbol{W})^{\top}\right\|_F^2  （11）$，式中，$\mathbb{R}^{n \times n} \ni \boldsymbol{W}:=\left[\boldsymbol{w}_1, \ldots, \boldsymbol{w}_n\right]^{\top}$的第$i$行包含第$i$个数据点的所有权值；$ \|\cdot\|_F$表示矩阵的弗罗贝尼乌斯（Frobenius）范数。公式（11）简化为：$\left\|\boldsymbol{Y}^{\top}(\boldsymbol{I}-\boldsymbol{W})^{\top}\right\|_F^2=\operatorname{\boldsymbol{tr}}\left((\boldsymbol{I}-\boldsymbol{W}) \boldsymbol{Y} \boldsymbol{Y}^{\top}(\boldsymbol{I}-\boldsymbol{W})^{\top}\right) \\ =\operatorname{\boldsymbol{tr}}\left(\boldsymbol{Y}^{\top}(\boldsymbol{I}-\boldsymbol{W})^{\top}(\boldsymbol{I}-\boldsymbol{W}) \boldsymbol{Y}\right) \\ =\operatorname{\boldsymbol{tr}}\left(\boldsymbol{Y}^{\top} \boldsymbol{M} \boldsymbol{Y}\right) （12）$，式中$\operatorname{\boldsymbol{tr}}(.)$表示矩阵的迹，且$\mathbb{R}^{n \times n} \ni \boldsymbol{M}:=(\boldsymbol{I}-\boldsymbol{W})^{\top}(\boldsymbol{I}-\boldsymbol{W}) （13）$。

注意到，由公式（1），矩阵$\boldsymbol{W}$的列加和为1可知$ (\boldsymbol{I}-\boldsymbol{W})$是矩阵$\boldsymbol{W}$的拉普拉斯式（Laplacian）。因此根据公式（13），矩阵$\boldsymbol{M}$能够看作权值矩阵拉普拉斯式的格拉姆矩阵（gram matrix）。

最终，公式（9）可以写为$\underset{\boldsymbol{Y}}{\operatorname{minimize}} \operatorname{\boldsymbol{tr}}\left(\boldsymbol{Y}^{\top} \boldsymbol{M}  \boldsymbol{Y}\right) \text {, } \\ \text { subject to } \frac{1}{n} \boldsymbol{Y}^{\top} \boldsymbol{Y}=\boldsymbol{I} \\ \boldsymbol{Y}^{\top} \mathbf{1}=\mathbf{0} （14）$，式中，$\mathbf{1}$和$\mathbf{0}$的维数分别为$\mathbb{R}^n$和$\mathbb{R}^p$。公式（14）可以理解为输入数据$\boldsymbol{X}$和嵌入数据$\boldsymbol{Y}$之间依赖关系的最大化。如果忽略第2个约束条件，公式（14）的拉格朗日函数为：$\mathcal{L}=\operatorname{tr}\left(\boldsymbol{Y}^{\top} M Y\right)-\operatorname{tr}\left(\boldsymbol{\Lambda}^{\top}\left(\frac{1}{n} \boldsymbol{Y}^{\top} \boldsymbol{Y}-\boldsymbol{I}\right)\right)$，式中，$ \Lambda \in \mathbb{R}^{n \times n} $是一个包含拉格朗日乘子的对角矩阵。将拉格朗日函数的导数设为0，给出，$\mathbb{R}^{n \times p} \ni \frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}}=2 \boldsymbol{M} \boldsymbol{Y}-\frac{2}{n} \boldsymbol{Y} \boldsymbol{\Lambda} \stackrel{\text { set }}{=} \mathbf{0} \\ \Longrightarrow M Y=Y\left(\frac{1}{n} \boldsymbol{\Lambda}\right)（15）$，这是$\boldsymbol{M}$的特征向量问题。因此$\boldsymbol{Y}$的列是$\boldsymbol{M}$的特征向量，其中特征值为$\frac{1}{n} \boldsymbol{\Lambda}$的对角元素。

由于公式（14）是一个最小化问题，所以$\boldsymbol{Y}$的列应该按照从小到大的特征值进行排序。此外，$\boldsymbol{M}$中的$(\boldsymbol{I}-\boldsymbol{W})$是权值$\boldsymbol{W}$的拉普拉斯矩阵。在线性代数和图论中，如果一个图有$k$个不相交的连通部分，那么它的拉普拉斯矩阵有$k$个零特征值。因为kNN图，或者$\boldsymbol{W}$是一个连通图，$(\boldsymbol{I}-\boldsymbol{W})$有一个零特征值，其特征向量为$\mathbf{1}=[1,1, \ldots, 1]^{\top}$。在对特征向量按从最小到最大的特征值进行排序后，需要忽略第一个特征值为零的特征向量，取$\boldsymbol{M}$的$p$个最小非零特征值的特征向量作为$\boldsymbol{Y} \in \mathbb{R}^{n \times p}$的列。

### 3.7.5.3 街道 POI 行业类别空间分布结构 

[Visualizing the stock market structure](https://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py)<sup>⑥</sup>使用了几种无监督学习技术从历史报价的变化中提取股票市场结构。将这一方法应用到街道空间 POI 行业类别相关性结构分析中，可以得到街道业态分布的关系结构。

应用*城市空间“语言”试验*部分处理过的 POI 数据，并用定义的`street_poi_structure()`方法提取街道每一个位置点，给定半径（500m）内 POI 数据按一级分类各类别的数量， 进而求出每一位置点各个类别占当前位置点总 POI 数量的比例。


```python
__C=usda_utils.AttrDict()
args=__C

__C.db=usda_utils.AttrDict() 
__C.db.UN='postgres'
__C.db.PW='123456'
__C.db.DB='visual_metrics'
__C.db.GC='geometry' 
__C.db.db_info=dict(geom_col=args.db.GC,myusername=args.db.UN,mypassword=args.db.PW,mydatabase=args.db.DB)
```


```python
clipped_pois_fn=r'G:\data\POI_dongxistreet\clipped_poi_gdf.gpkg'
poi_gdf=gpd.read_file(clipped_pois_fn)
metrics_merge_gdf=postSQL2gpd(table_name='metrics_merge',**args.db.db_info)
coordi_df=metrics_merge_gdf.to_crs(poi_gdf.crs)

pos_poi_idxes_gdf,pos_poi_feature_vector_gdf=usda_pano.street_poi_structure(poi=poi_gdf,position=coordi_df,distance=500)
```

    __________________________________________________
    The data has been read from PostgreSQL database. The table name is metrics_merge.
    

    100%|██████████| 566/566 [01:36<00:00,  5.84it/s]
    


```python
gpd2postSQL(pos_poi_feature_vector_gdf,table_name='pos_poi_feature_vector',**args.db.db_info) 
```

    __________________________________________________
    The GeoDataFrame has been written to the PostgreSQL database.The table name is pos_poi_feature_vector.
    

写入数据库，方便调用。


```python
pos_poi_feature_vector_gdf=postSQL2gpd(table_name='pos_poi_feature_vector',**args.db.db_info)
pos_poi_feature_vector_gdf.tail(3)
```

    __________________________________________________
    The data has been read from PostgreSQL database. The table name is pos_poi_feature_vector.
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>geometry</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>...</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>563</th>
      <td>POINT (309138.206 3793057.251)</td>
      <td>7.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>13.0</td>
      <td>15.0</td>
      <td>11.0</td>
      <td>4.0</td>
      <td>6.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>564</th>
      <td>POINT (309146.205 3793057.353)</td>
      <td>7.0</td>
      <td>5.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>13.0</td>
      <td>15.0</td>
      <td>10.0</td>
      <td>4.0</td>
      <td>6.0</td>
      <td>...</td>
      <td>2.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>565</th>
      <td>POINT (309154.205 3793057.456)</td>
      <td>7.0</td>
      <td>5.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>13.0</td>
      <td>14.0</td>
      <td>10.0</td>
      <td>4.0</td>
      <td>6.0</td>
      <td>...</td>
      <td>2.0</td>
      <td>11.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 26 columns</p>
</div>



查看 POI 数据一级分类标签对应的整数编码。


```python
poi_classificationName=usda_pano.poi_classificationName
print(poi_classificationName)
```

    {0: 'delicacy', 1: 'hotel', 2: 'shopping', 3: 'lifeService', 4: 'beauty', 5: 'spot', 6: 'entertainment', 7: 'sports', 8: 'education', 9: 'media', 10: 'medicalTreatment', 11: 'carService', 12: 'trafficFacilities', 13: 'finance', 14: 'realEstate', 15: 'corporation', 16: 'government', 17: 'entrance', 18: 'naturalFeatures', 19: 'administrativeLandmarks', 20: 'address', 21: 'road', 22: 'busStop', 23: 'subwayStop', 24: 'businessDistrict'}
    

计算每一位置点一级分类各类别数量占比，并除以其标准差归一化处理。


```python
cols=[str(i) for i in list(poi_classificationName.keys())]
poi_num=pos_poi_feature_vector_gdf[cols].to_numpy()+1
X=poi_num/poi_num.sum(axis=1)[:,None]
X /= X.std(axis=0)
print(X.shape)
```

    (566, 25)
    

协方差矩阵（covariance matrix）的逆矩阵，通常称为精度矩阵（precision matrix），与偏相关矩阵（partial correlation matrix）成正比，其给出了部分独立关系，即如果两个特征在其它特征上是条件独立的，则精度矩阵中相应的系数为零。这就是为什么估计稀疏精度矩阵是有意义的：通过学习数据的独立关系为条件，更好的估计协方差矩阵。这被称为协方差选择（covariance selection）。在小样本情况下，即参数`n_samples`是参数`n_features`的数量级或更小，稀疏逆协方差估计器（sparse inverse covariance estimators ）往往比收缩协方差估计器（shrunk covariance estimators）工作得更好。然而，在相反的情况下，如果各个特征间相关性很高，则会引起数值上的不稳地。另外，与收缩协方差估计器不同，稀疏逆协方差估计器能够恢复非对角结构。`GraphicalLasso`方法使用$L1$惩罚加强精度矩阵的稀疏性，参数`alpha`值越高，精度矩阵越稀疏；相应的`GraphicalLassoCV`方法可使用交叉验证自动配置`alpha`参数<sup>[17]</sup>。


```python
alphas=np.logspace(-1.5, 1, num=10) # Return numbers spaced evenly on a log scale.
edge_model=covariance.GraphicalLassoCV(alphas=alphas) 
edge_model.fit(X)
```

<img src="./imgs/3_7/3_7_06.png" height='auto' width='auto' title="caDesign">


```python
print(f'---covariance matrix:\n{edge_model.covariance_[:2]}\n---precision matrix:\n{edge_model.precision_[:2]}')
```

    ---covariance matrix:
    [[ 1.00000000e+00  2.52449861e-01 -8.13242161e-03 -1.12667801e-06
       2.71112537e-02 -1.20745417e-01  4.40653861e-02  1.95542545e-02
      -3.84202137e-02  5.09840049e-03 -3.52740955e-01 -6.08977938e-03
       0.00000000e+00 -3.47908633e-01 -1.51052575e-01 -5.73324093e-07
      -1.91494601e-02 -1.26217666e-02 -1.26218600e-02 -1.26219251e-02
      -1.26219703e-02 -1.26219046e-02 -1.26219803e-02 -1.26220302e-02
      -1.26220924e-02]
     [ 2.52449861e-01  1.00000000e+00 -3.70717527e-02 -2.93142231e-06
       1.70702429e-02 -2.78792475e-01  7.89998860e-02  1.20174350e-02
      -4.36633276e-02  2.26520909e-02 -1.14570509e-01 -2.38152757e-02
       0.00000000e+00 -2.60774625e-01 -2.82061911e-01 -1.49169064e-06
      -1.78193503e-02 -4.76883404e-02 -4.76883300e-02 -4.76883404e-02
      -4.76883352e-02 -4.76879429e-02 -4.76881171e-02 -4.76881866e-02
      -4.76882980e-02]]
    ---precision matrix:
    [[ 1.31552604 -0.17091593  0.          0.         -0.          0.
      -0.         -0.01253992  0.         -0.          0.3892473   0.
      -0.          0.34799309  0.09109158  0.          0.          0.
       0.          0.          0.          0.          0.          0.
       0.        ]
     [-0.17091593  1.25557047  0.          0.         -0.          0.27579507
      -0.         -0.          0.         -0.          0.          0.
      -0.          0.19774892  0.28442421  0.          0.          0.00980012
       0.00980036  0.00980035  0.00980026  0.00979957  0.00980007  0.00980028
       0.00980056]]
    

使用 AP（Affinity Propagation）聚类反应街道所有位置点各个 POI 一级分类组成比例关系的协方差矩阵，自动计算聚类簇数量，获得一级行业分类在整条街道上具有类似影响的关系。


```python
_, labels = cluster.affinity_propagation(edge_model.covariance_, random_state=0)
n_labels = labels.max()
labels 
```




    array([0, 0, 5, 1, 4, 3, 0, 5, 4, 4, 3, 5, 2, 3, 3, 1, 4, 5, 5, 5, 5, 5,
           5, 5, 5], dtype=int64)



查看各簇对应一级行业分类名的结果。


```python
poi_names=np.array(list(poi_classificationName.values()))
for i in range(n_labels + 1):
    print(f"Cluster {i + 1}: {', '.join(poi_names[labels == i])}")
```

    Cluster 1: delicacy, hotel, entertainment
    Cluster 2: lifeService, corporation
    Cluster 3: trafficFacilities
    Cluster 4: spot, medicalTreatment, finance, realEstate
    Cluster 5: beauty, education, media, government
    Cluster 6: shopping, sports, carService, entrance, naturalFeatures, administrativeLandmarks, address, road, busStop, subwayStop, businessDistrict
    

出于可视化目的，使用 LLE 流形学习，以一级行业分类为“样本”，以各个位置对应分类的频数比例为特征，即高维输入空间数据，降至2维，即低维嵌入空间。用`LocallyLinearEmbedding`方法计算。


```python
node_position_model = manifold.LocallyLinearEmbedding(n_components=2, eigen_solver="dense", n_neighbors=7)
embedding = node_position_model.fit_transform(X.T).T
```

打印最终网络，其中顶点代表一级行业分类，同时，

* 聚类标签颜色用于定义顶点颜色；
* 稀疏协方差模型用于显示边的强度；
* 2维嵌入向量用于定位顶点位置。

从结果可以得知，对该街道的业态分布，

* 在聚类结果上，delicacy，hotel 和 entertainment 在街道业态组成上具有相似影响，其它各簇同；
* 在基于精度矩阵的一级行业类别相关性上，government 和 education 具有较高的关联，其次为 delicacy 和 medical treatment 及 finance等；
* 对 LLE 流形学习结果，shopping，subway stop，business district，bus stop，administrtive landmarks等等集聚在一起，表面这些业态在所有位置点的 POI 组成比例上相似。其它相互邻近的点意义同。


```python
usda_manifold.partial_correlations_embedding2Dgraph(embedding,edge_model,labels,poi_names,figsize=(7,7))
```


<img src="./imgs/3_7/output_77_0.png" height='auto' width='auto' title="caDesign">    
    


## 3.7.6 t-SNE 和 UMAP

### 3.7.6.1 t-SNE 算法解析

Hinton, G. E.等人<sup>[18]</sup>描述了一种概率方法，称为 SNE（Stochastic Neighbor Embedding），在保持样本点邻里关系条件下将由高维输入空间向量或两两不相似度（pairwise dissimilarities）描述的对象映射到低维嵌入空间中。以高维输入空间各个样本点为中心的高斯密度定义该样本点与所有样本点的概率分布。嵌入的目的是映射到低维嵌入空间的对象与高维输入空间对象尽可能具有相似的分布。衡量低维嵌入空间和高维输入空间对象分布的差异则使用 Kullback-Leibler 散度（divergences）/距离，计算各个对应样本点之间的距离和为最小作为目标函数，并应用梯度调整低维嵌入空间样本点的位置。基于 SNE，van der Maaten 等人<sup>[19]</sup>提出了t-SNE（ t-Distributed Stochastic Neighbor Embedding），通过构建对称的 SNE 解决因为不对称导致梯度计算复杂的情况；通过在低维嵌入空间中引入 t-分布 替换高斯分布，使用更加偏重长尾分布的方式将距离转化为概率分布，可以减轻不同类别的簇“拥挤”在一起，不容易区分的问题。

* 高维输入空样本点两两间距离概率计算

假设有$N$个高维对象$\mathbf{x}_1, \ldots, \mathbf{x}_N$，t-SNE 首先计算与 $\mathbf{x}_i$和$\mathbf{x}_j$两两之间相似度（距离）成正比的概率$p_{ij}$，对于$i \neq j$，有$\mathrm{p}_{\mathrm{j} \mid \mathrm{i}}=\frac{\exp \left(-\left\|\mathbf{x}_{\mathrm{i}}-\mathbf{x}_{\mathrm{j}}\right\|^2 / 2 \sigma_{\mathrm{i}}^2\right)}{\sum_{\mathrm{k} \neq \mathrm{i}} \exp \left(-\left\|\mathbf{x}_{\mathrm{i}}-\mathbf{x}_{\mathrm{k}}\right\|^2 / 2 \sigma_{\mathrm{i}}^2\right)}$，且$p_{i \mid i}=0$，注意到对所有$i$，有$\sum_j p_{j \mid i}=1$。

数据点$x_j$到数据点$x_i$的距离是$p_{j \mid i}$条件概率，即$x_i$选择$x_j$为其邻近点，当邻近点的选择与它们之间以$x_i$为高斯分布中心的概率密度成比例。定义高维输入空间中的联合概率$p_{ij}$为对称条件概率，有$p_{i j}=\frac{p_{j \mid i}+p_{i \mid j}}{2 n}$。同样有，$p_{i i}=0$，$\sum_{i j} p_{i j}=1$。

对应到 [van der Maaten 提供的代码](https://lvdmaaten.github.io/tsne/)，其高维输入空间样本点间距离计算对应代码为：

```python
(n, d) = X.shape
sum_X = np.sum(np.square(X), 1)
D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)
```

高维输入空间样本点间距离高斯分布概率计算对应代码为：

```python
def Hbeta(D=np.array([]), beta=1.0):
    """
        Compute the perplexity and the P-row for a specific value of the
        precision of a Gaussian distribution.
    """

    # Compute P-row and corresponding perplexity
    P = np.exp(-D.copy() * beta)
    sumP = sum(P)
    H = np.log(sumP) + beta * np.sum(D * P) / sumP
    P = P / sumP
    return H, P

# Compute P-values
P = x2p(X, 1e-5, args['perplexity'])
P = P + np.transpose(P)
P = P / np.sum(P)
P = P * 4.	# early exaggeration
P = np.maximum(P, 1e-12)
```

* 低维嵌入空间随机初始化样本点两两间距离概率计算

t-SNE 的目标是将高维输入空间对象映射到低维嵌入空间，得到$\mathbf{y}_1, \ldots, \mathbf{x}_N$，$\mathbf{y}_i \in \mathbb{R}^d$，$d$通常为2或3维。与$Y$两两样本点之间距离成比例的概率尽可能的接近高维输入空间的$p_{ij}$，因此在低维嵌入空间使用类似的方法计算，只是将高斯分布替换为t-分布，从而能够保留更多具有较远距离的相似度。定义$q_{i j}=\frac{\left(1+\left\|y_i-y_j\right\|^2\right)^{-1}}{\sum_{k \neq l}\left(1+\left\|y_k-y_l\right\|^2\right)^{-1}}$。

低维嵌入空样本点（随机初始化）距离和概率计算对应代码为：

```python
Y = np.random.randn(n, no_dims)
# Compute pairwise affinities
sum_Y = np.sum(np.square(Y), 1)
num = -2. * np.dot(Y, Y.T)
num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))
num[range(n), range(n)] = 0.
Q = num / np.sum(num)
Q = np.maximum(Q, 1e-12)
```

* 高低维度空间样本点距离概率相似性计算和优化迭代

测度高维输入空间和低维嵌入空间数据点两两间距离概率相似性，用 Kullback-Leibler 散度计算，公式为： $C=K L(P \| Q)=\sum_i \sum_j p_{i j} \log \frac{p_{i j}}{q_{i j}}$。

根据上述目标函数，使用梯度下降法，寻找低维嵌入空间数据点的向量，梯度为： $\frac{\delta C}{\delta y_i}=4 \sum_j\left(p_{i j}-q_{i j}\right)\left(y_i-y_j\right)\left(1+\left\|y_i-y_j\right\|^2\right)^{-1}$。

梯度计算对应代码为：

```python
# Compute gradient
PQ = P - Q
for i in range(n):
    dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)
```

从提供的示范代码中也可以看到，t-SNE 使用了预降维的技巧，先将高维数据降至低维，例如示范中将 784 维降至 50 维后再用 t-SNE 方法降维至指定的 2 维，达到优化的目的；同时，示范代码中对 概率乘以了一个倍数，例如 4，以避免因为概率过小导致的优化太慢的问题。t-SNE 主要用于高维数据的可视化，也可以从可视结果中估计到数据可能的分类数，及关联特征间的关系。

t-SNE 伪代码为：

```algorithm
% WGAN
\begin{algorithm}
\caption{Simple version of t-Distributed Stochastic Neighbor Embedding.}
\begin{algorithmic}
\STATE \textbf{Data}: data set $X=\left\{x_1, x_2, \ldots, x_n\right\}$,
\STATE cost function parameters: perplexity Perp,
\STATE optimization parameters: number of iterations $T$, learning rate $\eta$, momentum $\alpha(t)$.
\STATE \textbf{Result}: low-dimensional data representation $\mathcal{Y}^{(T)}=\left\{y_1, y_2, \ldots, y_n\right\}$.
\PROCEDURE{t-SNE}{}
\STATE compute pairwise afﬁnities $p_{j \mid i}$ with perplexity Perp (using Equation 1)
\STATE set $p_{i j}=\frac{p_{j \mid i}+p_{i \mid j}}{2 n}$
\STATE sample initial solution $\mathcal{Y}^{(0)}=\left\{y_1, y_2, \ldots, y_n\right\}$ from $\mathcal{N}\left(0,10^{-4} I\right)$
\FOR{$t=1$ to $T$}
\STATE compute low-dimensional afﬁnities  $q_{i j}$ (using Equation 4)
\STATE compute gradient $\frac{\delta C}{\delta y}$ (using Equation 5)
\STATE set $\mathcal{Y}^{(t)}=\mathcal{Y}^{(t-1)}+\eta \frac{\delta C}{\delta \mathcal{Y}}+\alpha(t)\left(\mathcal{Y}^{(t-1)}-\mathcal{Y}^{(t-2)}\right)$
\ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
```

使用 t-SNE 作者提供的示例数据和代码，计算如下。


```python
X=np.loadtxt(r"C:\Users\richie\omen_richiebao\omen_code\tsne_python\mnist2500_X.txt")
labels=np.loadtxt(r"C:\Users\richie\omen_richiebao\omen_code\tsne_python\mnist2500_labels.txt")
print(X.shape)
```

    (2500, 784)
    


```python
Y=usda_manifold.tsne(X,2,max_iter=1000,print_interval=200)
```

    Preprocessing the data using PCA...
    Computing pairwise distances...
    Mean value of sigma: 2.5733192000 of 2500...
    Iteration 1000: error is 0.979305


```python
pylab.scatter(Y[:, 0], Y[:, 1], 20, labels,cmap='Dark2')
pylab.show()
```


<img src="./imgs/3_7/output_83_0.png" height='auto' width='auto' title="caDesign">    


类似 t-SNE，McInnes, L.等人<sup>[20]</sup>提出了UMAP（Uniform Manifold Approximation and Projection）流形学习方法，该算法基于黎曼几何（Riemannian geometry）和代数拓扑（algebraic topology）的理论框架构建，在可视化质量上可与 t-SNE 相媲美，并且可以保留更多的全局结构，同时具有更好的运行性能。UMAP 的实现可以使用[UMAP](https://umap-learn.readthedocs.io/en/latest/index.html)<sup>⑦</sup>库。

用 UMAP 方法重复试验上述 t-SNE 输入数据，可以观察到 UMAP 表现出相对更好的团聚结果。


```python
reducer=umap.UMAP(n_components=2)
embedding=reducer.fit_transform(X)
```


```python
pylab.scatter(embedding[:, 0], embedding[:, 1], 20, labels,cmap='Dark2')
pylab.show()
```


<img src="./imgs/3_7/output_86_0.png" height='auto' width='auto' title="caDesign">    
    


### 3.7.6.2 街道空间多维景观指数的 UMAP 空间特征区段划分

再次使用街道空间多维景观指数数据，用 UMAP 方法结合 DBSCAN 聚类寻找多维景观指数在街道空间中分布的特征区段。

提取待降维的数据字段。


```python
fields=['Green view index','Sky view factor', 'Ground view index','Equilibrium degree', 'Perimeter area ratio(mn)','Shape index(mn)', 'Fractal dimension(mn)', 'Color richness index','Key point size(0-10]', 'Key point size(10-20]','Key point size(30-40]', 'Key point size(20-30]','frank_e', 'num']
print(f'fields num = {len(fields)}')
metrics_selection_df=metrics_inall[fields]
X=metrics_selection_df.to_numpy()
```

    fields num = 14
    

使用 UMAP 降维至3维。


```python
reducer=umap.UMAP(n_components=3)
embedding=reducer.fit_transform(X)

metrics_inall_copy=metrics_inall.copy(deep=True)
metrics_inall_copy[['embedding_x','embedding_y','embedding_z']]=embedding
```

用 DBSCAN 聚类降维结果数据，通过调整参数`eps`，控制聚类簇数量。


```python
clustering = DBSCAN(eps=0.6, min_samples=3).fit(embedding)
print(np.unique(clustering.labels_))
metrics_inall_copy['dbscan']=clustering.labels_
```

    [0 1 2 3 4 5 6 7]
    

按降维向量打印三维点，用聚类降维结果表示为点的颜色，观察数据点的分布情况。


```python
fig_3d=px.scatter_3d(
    metrics_inall_copy, x='embedding_x',y='embedding_y',z='embedding_z',  
    color='dbscan',
    width=800, height=800,
    hover_data=['fn_idx'],
    )
fig_3d.update_traces(marker_size=5)
fig_3d.show()
```

<img src="./imgs/3_7/3_7_03.png" height='auto' width='auto' title="caDesign">

为了观察实际街道空间下聚类降维后的结果，按照实际经纬度坐标的相对位置打印点并赋予聚类的颜色，得知多维景观指数的综合特征将街道划分成了多个区段，可以将该种方法拓展到类似的城市空间特征模式结构寻找问题上。


```python
metrics_inall_copy[['lon','lat']]=metrics_inall.apply(lambda row:[row.geometry.x,row.geometry.y],axis=1,result_type='expand')
norm_lon=usda_utils.normalize_by_meanNstd(metrics_inall_copy[['lon']])[0]
norm_lat=usda_utils.normalize_by_meanNstd(metrics_inall_copy[['lat']])[0]
```


```python
fig,ax=plt.subplots(figsize=(20,1))

color=usda_vis.vals4color_cmap(metrics_inall_copy['dbscan'].values,cmap='Accent',vmin=metrics_inall_copy['dbscan'].min(), vmax=metrics_inall_copy['dbscan'].max())
ax.scatter(norm_lon,norm_lat,s=10,color=color) 
    
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.show()
```


<img src="./imgs/3_7/output_97_0.png" height='auto' width='auto' title="caDesign">    



---

注释（Notes）：

①  Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…，（<https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py>）。

②  class sklearn.manifold.MDS，（<https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html>）。

③  sklearn.manifold.smacof，（<https://scikit-learn.org/stable/modules/generated/sklearn.manifold.smacof.html>）。

④  （sklearn）MDS 代码，（<https://github.com/scikit-learn/scikit-learn/blob/7f9bad99d/sklearn/manifold/_mds.py#L372>）。

⑤  Isotonic Regression 示例，（<https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_isotonic_regression.html>）。

⑥  Visualizing the stock market structure，（<https://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py>）。

⑦  UMAP Python 库，（<https://umap-learn.readthedocs.io/en/latest/index.html>）。

参考文献（References）:

[1] 周志华 著. 机器学习, 北京: 清华大学出版社, 2016年1月.(ISBN 978-7-302-206853-6)

[2] Lawrence, N. D. (2012). A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models. Journal of Machine Learning Research, 13(51), 1609–1638. Retrieved from http://jmlr.org/papers/v13/lawrence12a.html

[3] Lee, John A.; Verleysen, Michel (2007). Nonlinear Dimensionality Reduction. Springer. ISBN 978-0-387-39350-6.

[4] Rajawat, K., & Kumar, S. (2016). Stochastic Multidimensional Scaling. arXiv [Math.OC]. Retrieved from http://arxiv.org/abs/1612.07089

[5] Kruskal,J. B. (1964). Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1), 1–27. doi:10.1007/bf02289565

[6] Kruskal, J. B. (1964). Nonmetric multidimensional scaling: A numerical method. Psychometrika, 29(2), 115–129. doi:10.1007/bf02289694 

[7] 同济大学数学系编,工程数学：线性代数（第5版）[M]. 高等教育出版社, 2007.05. ISBN： 9787040212181

[8] Burden, R. L., Faires, J. D., & Burden, A. M. (2015). Numerical Analysis. Retrieved from https://books.google.com/books?id=9DV-BAAAQBAJ

[9] Eigenvalues and eigenvectors_Wikipedia, <https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#cite_note-FOOTNOTEPressTeukolskyVetterlingFlannery200738-28>

[10] 李航.统计学习方法[M].北京,清华大学出版社.2012.03.ISBN: 9787302275954

[11] Schölkopf, B., Smola, A., & Müller, K.-R. (1997). Kernel principal component analysis. In W. Gerstner, A. Germond, M. Hasler, & J.-D. Nicoud (Eds.), Artificial Neural Networks --- ICANN’97 (pp. 583–588). Berlin, Heidelberg: Springer Berlin Heidelberg.

[12] Raschka, S., & Mirjalili, V. (2019). Python Machine Learning: Machine Learning and Deep Learning with Python, Scikit-Learn, and TensorFlow 2, 3rd Edition. Retrieved from https://books.google.com/books?id=n1cjyAEACAAJ

[13] Tenenbaum, J. B., de Silva, V., & Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science (New York, N.Y.), 290(5500), 2319–2323. doi:10.1126/science.290.5500.2319

[14] Lagrange multiplier_Wikipedia, <https://en.wikipedia.org/wiki/Lagrange_multiplier>.

[15] Roweis, S. T., & Saul, L. K. (2000). Nonlinear Dimensionality Reduction by Locally Linear Embedding. Science, 290(5500), 2323–2326. doi:10.1126/science.290.5500.2323

[16] Ghojogh, B., Ghodsi, A., Karray, F., & Crowley, M. (2020). Locally Linear Embedding and its Variants: Tutorial and Survey. arXiv [Stat.ML]. Retrieved from http://arxiv.org/abs/2011.10925

[17] Sparse inverse covariance, <https://scikit-learn.org/stable/modules/covariance.html#sparse-inverse-covariance>

[18] Hinton, G. E., & Roweis, S. (2002). Stochastic Neighbor Embedding. In S. Becker, S. Thrun, & K. Obermayer (Eds.), Advances in Neural Information Processing Systems (Vol. 15). Retrieved from https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf

[19] van der Maaten, L. J. P., & Hinton, G. E. (2008). Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research, 9(nov), 2579-2605.

[20] McInnes, L., Healy, J., & Melville, J. (2020). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. arXiv [Stat.ML]. Retrieved from http://arxiv.org/abs/1802.03426
