Created on Sun Apr  9 11:55:43 2023 @author: Richie Bao-caDesign设计(cadesign.cn)

# 3.4-B 模式生成：可以反映地理空间尺度的 SytleGAN

在研究城市地理空间模式时，对于高分辨率航拍（遥感）影像而言，如果直接应用原始的生成对抗网络（GAN），即使使用具有更好收敛效果的 WGAN，通常生成图像也很难达到理想效果。这主要因为地物对象具有不同的尺度（尺寸），应考虑有与SIFT（Scale-Invariant Feature Transform）（见* 图像特征提取与动态街景视觉感知*一章）算法所体现的尺度空间（scale space），能够改进生成数据（图像）的质量，也能够进一步验证尺度对城市地理空间模式的影响。

GAN 得以快速的发展，被广泛的应用于不同的领域，反应有尺度空间（粒度，图像分辨率）的模型有 PGGAN（Progressive growing of GANs）<sup>[1]</sup> ，及基于 PGGAN 模型发展的 StyleGAN（Style-Based Generator Architecture for Generative Adversarial Networks）系列，例如 StyleGAN<sup>[2]</sup>、StyleGAN2<sup>[3]</sup>、StyleGAN3<sup>[4]</sup>，及基于 StyleGAN 系列用于火星影像生成的 MarsGAN<sup>[5]</sup> 等。此次实验以 StyleGAN（1）为基础，结合作者提供的代码<sup>[6]</sup>（TensorFlow 版本），用迁移的 Pytorch 版本<sup>[7]</sup>计算，从地理空间数据视角解释 StyleGAN，并用于高分辨率航拍影像的生成、亦可用于模式变化及土地覆盖类型（Land Cover,LC）、DEM 等生成实验。

## 3.4.1 StyleGAN 阐释和 NAIP 航拍影像数据生成    


### 3.4.1.1 迁移 StyleGAN
    
为了方便实验，将 StyleGAN（ Pytorch 版本） 迁移至 USDA 库，位于`sda.migrated_project.stylegan`模块之下。迁移库之前需要验证原有库是否运行正常，测试无误后才可迁移。迁移过程中，通常需要根据未来操作模式进行对应的调整，例如 原 StyleGAN 使用[argparse](https://docs.python.org/3/library/argparse.html)配置部分参数。如果未来在 Jupyter[Lab] Notebook 下运行代码（例如用 CoLab 实验），则调整参数输入方式均为 [yacs](https://github.com/rbgirshick/yacs)库提供的属性字典方式，基本同 `usda_utils.AttrDict() `提供的方法，但是增加了更多的功能，方便参数调用、合并、可变（mutable）和不可变（immutable）切换等；同时，需要调整模块、类、函数等调入的路径，配置`__init__.py`文件，保证 USDA 库下可以调用 StyleGAN 代码工具；如果为了增加代码的弹性，例如可以单独调用 StyleGAN 的$G$或$D$网络单独的块（block）或层（layer），如网络块`InputBlock`、`GSynthesisBlock`、`DiscriminatorTop`和`DiscriminatorBlock`等，如单独层`PixelNormLayer`、`Upscale2d`、`Downscale2d`、`EqualizedLinear`、`EqualizedConv2d`、`NoiseLayer`、`StyleMod`、`LayerEpilogue`、`BlurLayer`、`View`、`StddevLayer`和`Truncation`等，则可以将其包括在`__init__.py`下。
    
下述代码为迁移 StyleGAN 至 USDA 库，从 USDA 库调用 StyleGAN 生成 NAIP 航拍影像数据的方式。影像数据分辨率为 512，数量为 20,000张（影像样本数据生成方法具体查看*3.4-A 模式生成：从聚类模式特征到生成对抗网络和计算分析工具的建构*一章）。
    
> 最终计算在 CoLab 中结合 Google driver（个人云存储空间和文件共享平台） 完成。


```python
# IPython extension to reload modules before executing user code.
%load_ext autoreload 
# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.
%autoreload 2 
from usda.migrated_project.stylegan import Stylegan_train
import usda.migrated_project.stylegan as stylegan
import usda.migrated_project.stylegan.data as sgan_data 
import usda.migrated_project.stylegan.extracted_funcs as sgan_gadgets
import usda.migrated_project.stylegan.models as sgan_model
from usda.maths import plot_single_function

import torch
import math
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import random
import torchvision
```

超参数（Hyperparameter）的配置尽量选择原作者已经实验验证可行的参数，如需调整参数，则以原参数为参照调试至满足不同分析目的，或为适应不同 GPU 算力要求。


```python
sgan=Stylegan_train()

sgan.opt.output_dir=r'I:\model_ckpts\styleGAN4naip\sgan'
sgan.opt.dataset.img_dir=r'I:\data\NAIP4StyleGAN\naip_1024' 
sgan.opt.dataset.folder=False
sgan.opt.dataset.resolution=1024
sgan.opt.structure='linear'
sgan.opt.model.gen.mapping_layers = 8 # Mapping network 部分全连接层数量配置，默认为 8（为 StyleGAN 作者参数配置）；StyleGAN代码 Pytorch 版作者默认配置为 4

sgan.opt.sched.batch_sizes = [128, 128, 128, 64, 32, 16, 8, 4, 2] # StyleGAN代码 Pytorch 版作者配置，（batches for oen 1080Ti with 11G memory）
sgan.opt.epochs=[40,80,80,80,80,160,160,160] 

sgan.configuration()
sgan.build_dataset()
sgan.init_network()
```

    2023-04-16 07:34:36 INFO: Using 1 GPUs.
    2023-04-16 07:34:36 INFO: Training on NVIDIA GeForce RTX 2080 with Max-Q Design.
    
    2023-04-16 07:34:39 INFO: Training from scratch...
    


```python
sgan.train()
```

```
2023-04-16 07:34:54 INFO: Starting the training process ... 

2023-04-16 07:34:54 INFO: Currently working on depth: 1
2023-04-16 07:34:54 INFO: Current resolution: 4 x 4
2023-04-16 07:34:54 INFO: Epoch: [1]
2023-04-16 07:36:01 INFO: Elapsed: [0:01:07] Step: 1  Batch: 1  D_Loss: 714.139893  G_Loss: 0.103811
2023-04-16 07:36:06 INFO: Elapsed: [0:01:11] Step: 3  Batch: 3  D_Loss: 409.393951  G_Loss: 0.000000
2023-04-16 07:36:11 INFO: Elapsed: [0:01:16] Step: 6  Batch: 6  D_Loss: 412.592224  G_Loss: 0.000000
2023-04-16 07:36:19 INFO: Elapsed: [0:01:24] Step: 9  Batch: 9  D_Loss: 468.190460  G_Loss: 0.000314
2023-04-16 07:36:26 INFO: Elapsed: [0:01:31] Step: 12  Batch: 12  D_Loss: 371.070770  G_Loss: 0.000432
2023-04-16 07:36:37 INFO: Elapsed: [0:01:42] Step: 15  Batch: 15  D_Loss: 270.685150  G_Loss: 0.002868
2023-04-16 07:36:51 INFO: Elapsed: [0:01:56] Step: 18  Batch: 18  D_Loss: 380.751556  G_Loss: 0.000240
2023-04-16 07:37:05 INFO: Elapsed: [0:02:10] Step: 21  Batch: 21  D_Loss: 278.914978  G_Loss: 0.000945
2023-04-16 07:37:12 INFO: Elapsed: [0:02:17] Step: 24  Batch: 24  D_Loss: 249.346146  G_Loss: 0.010437
2023-04-16 07:37:18 INFO: Time taken for epoch: 0:02:23

2023-04-16 07:37:21 INFO: Saving the model to: I:\model_ckpts\styleGAN4naip\sgan\models\GAN_GEN_0_1.pth

2023-04-16 07:37:31 INFO: Saving the model to: I:\model_ckpts\styleGAN4naip\sgan\models\GAN_GEN_SHADOW_0_1.pth

2023-04-16 07:37:31 INFO: Epoch: [2]
2023-04-16 07:38:42 INFO: Elapsed: [0:03:47] Step: 26  Batch: 1  D_Loss: 297.541382  G_Loss: 0.000000
2023-04-16 07:38:47 INFO: Elapsed: [0:03:53] Step: 28  Batch: 3  D_Loss: 260.714508  G_Loss: 0.002044
2023-04-16 07:38:55 INFO: Elapsed: [0:04:01] Step: 31  Batch: 6  D_Loss: 238.747452  G_Loss: 1.679931
2023-04-16 07:39:02 INFO: Elapsed: [0:04:07] Step: 34  Batch: 9  D_Loss: 198.448822  G_Loss: 6.064939
2023-04-16 07:39:07 INFO: Elapsed: [0:04:12] Step: 37  Batch: 12  D_Loss: 206.848328  G_Loss: 7.724971
2023-04-16 07:39:14 INFO: Elapsed: [0:04:19] Step: 40  Batch: 15  D_Loss: 248.303726  G_Loss: 2.320208
2023-04-16 07:39:22 INFO: Elapsed: [0:04:27] Step: 43  Batch: 18  D_Loss: 230.896790  G_Loss: 1.446756
2023-04-16 07:39:29 INFO: Elapsed: [0:04:35] Step: 46  Batch: 21  D_Loss: 193.671387  G_Loss: 5.729475
2023-04-16 07:39:34 INFO: Elapsed: [0:04:39] Step: 49  Batch: 24  D_Loss: 161.947311  G_Loss: 2.598769
2023-04-16 07:39:38 INFO: Time taken for epoch: 0:02:06

2023-04-16 07:39:38 INFO: Epoch: [3]
...
```

例举了各深度训练，开始和结束轮次生成图像。从各深度训练对应的生成图像可以清晰辨别出图像尺度空间（粒度，分辨率）的作用，及各深度基于上一深度训练结果的变化，虽然当前深度为上一深度像素单元的4倍，但各深度开始生成的图像与上一深度类似，即扩展的4倍栅格单元数值之间基本无差异，随着训练的进行，新扩展的栅格单元数值逐渐向真实图像当前深度的分布靠拢，扩展的4倍栅格单元数值之间产生差异变化。从生成结果可判断，使用 StyleGAN 模型训练高分辨率航拍影像能够获得真实影像数据分布，从而生成逼真的“假”图像。那么推断将其用于不同特定地貌、具有不同特征城市模式样本的训练是可行的。

| 深度（depth）（像素，resolution）  | 开始  | 结束  |
|---|---|---|
| 0 （4） | <img src="./imgs/3_4_b/gen_0_1_1.png" height='auto' width='auto' title="caDesign"> </br>0_1_1 （深度/depth_轮次/epoch_批次/batch） | <img src="./imgs/3_4_b/gen_0_40_144.png" height='auto' width='auto' title="caDesign"> </br>0_40_144  |
| 1 （8）|  <img src="./imgs/3_4_b/gen_1_1_1.png" height='auto' width='auto' title="caDesign"></br> 1_1_1  | <img src="./imgs/3_4_b/gen_1_80_144.png" height='auto' width='auto' title="caDesign"> </br> 1_80_144 |
| 2  (16) | <img src="./imgs/3_4_b/gen_2_1_1.png" height='auto' width='auto' title="caDesign"> </br>2_1_1  | <img src="./imgs/3_4_b/gen_2_80_144.png" height='auto' width='auto' title="caDesign"> </br>2_80_144  |
| 3  (32)|  <img src="./imgs/3_4_b/gen_3_1_1.png" height='auto' width='auto' title="caDesign"> </br> 3_1_1 | <img src="./imgs/3_4_b/gen_3_80_288.png" height='auto' width='auto' title="caDesign"> </br> 3_80_288 |
| 4  (64)|  <img src="./imgs/3_4_b/gen_4_1_1.png" height='auto' width='auto' title="caDesign"> </br>4_1_1  | <img src="./imgs/3_4_b/gen_4_80_567.png" height='auto' width='auto' title="caDesign"></br> 4_80_567  |
| 5  (128)| <img src="./imgs/3_4_b/gen_5_1_1.png" height='auto' width='auto' title="caDesign"> </br> 5_1_1 | <img src="./imgs/3_4_b/gen_5_20_1134.png" height='auto' width='auto' title="caDesign"> </br>5_20_1134  |
| 6  (256)|  <img src="./imgs/3_4_b/gen_6_1_1.png" height='auto' width='auto' title="caDesign"> </br> 6_1_1 | <img src="./imgs/3_4_b/gen_6_7_1.png" height='auto' width='auto' title="caDesign"> </br>6_7_1  |
| 7  (512)|  待 | 待  |

### 3.4.1.2 应用 StyleGAN 于地理空间数据分析上的阐释

基于 StyleGAN 作者 Karras, T.等人的研究论文<sup>[2]</sup>和StyleGAN 的PyTroch版本代码<sup>[7]</sup>， 从地理空间数据分析方向阐释 StyleGAN 架构和关键细节。为了契合应用 StyleGAN 于地理空间数据分析的意义，调整了个别名词表述的方式，例如将“风格（style）”表述为“尺度空间”；对应不同分辨率的风格表述为尺度空间“深度（depth）”；对人脸的风格特征描述，例如人脸姿态、形状、发型特征、纹理和色彩等，转化为对地物或相关地理空间数据表达对象的描述，例如建筑、街道、植被和水体等；相应，不同分辨率的特征描述转化为不同尺度空间深度或不同空间尺度下模式结构特征的描述。
    
PGGAN 网络示意图<sup>[1]</sup>说明 PGGAN 训练从具有$4 \times 4$ 低空间分辨率的的生成器$G$和判别器$D$开始，并随着训练的进行，逐渐向$G$和$D$中添加高分辨率的层（从图左至右），从而提高生成图像的空间分辨率。整个过程，所有图层都保持可训练状态。$N \times N$为在$N \times N$空间分辨率上运行的卷积层。虽然 PGGAN 能够生成高分辨率高品质的图像，但是不能分析不同空间分辨率的特征分布，也不能控制生成图像的样式趋向，这将限制基于尺度深度变化地理空间模式结构特征分析，及限制生成可控有意义的地理空间模式用于规划参考。     
    
<img src="./imgs/3_4_b/3_4_b_04.png" height='auto' width='auto' title="caDesign"> 
    
SytleGAN 则借助构建的`Mapping network`层（下图<sup>[2]</sup>）将各个空间尺度对应的地理空间特征分解开来（即从各个深度表征的图像特征相互纠缠（ entangle）到尽量相互独立的解缠过程（disentangle）），可以分析不同空间尺度特征和控制不同深度对生成（图像）空间模式的影响。
    
因为增加了`Mapping network`层，因此 StyleGAN 的$G$网络总共包括两个部分，`Mapping network，（MN）`映射网络和`Synthesis network，（SN）`合成网络。
    
<img src="./imgs/3_4_b/3_4_b_03.png" height='auto' width='500' title="caDesign">     
    
#### 1) MN 映射网络
    
MN 输入为属于隐藏（潜在）空间（latent space）$\mathcal{Z}$的隐藏特征（编码）（latent code）$\mathbf{z}$（$\mathbf{z} \in \mathcal{Z}$），$\mathbf{z}$即为 512 维的噪声向量。 $\mathbf{z}$通过 MN 的非线性映射网络$f: \mathcal{Z} \rightarrow \mathcal{W} $ （为8层全连接层（fully connected layer，FC，也称为 Dense layer）的多层感知机（ multi-layer perceptron，MLP ））映射到$ \mathbf{w}$（$\mathbf{w} \in \mathcal{W}$）（保持512 维）。图中由$A$表示学习到的仿射变换（affine transform）。$A$被接入到 SN 合成网络，因为从深度0到8（对应空间分辨率为 4、8、16、32、64、128、256、512、1024）总共9个生成阶段，每个阶段对接2个$A$控制向量，因此总共18个空间尺度深度特征控制向量，通过$f$映射可以尽量避免这18个$A$之间不同深度的特征纠缠，即表征地物对象不同深度的分布特征会尽量分离。
    
因为 NAIP 航拍影像的高空分辨率为 1m，样本制作采样的单元大小为512，因此对应到空间尺度深度，深度7（512）的实际地理空间分辨率为1m，由$2^n, n \in \{2,3,4,5,6,7,8\}$对应逆序计算，深度6（256）的地理空间分辨率为4m， 深度5（128）为8m，深度4（64）为16m，深度3（32）为32m，深度2（16）为64m，深度1（8）为128m，至深度0（4）为256m。那么，
    
1. 对于空间尺度深度7，可以表征的地物尺寸约$1 \times 1$m，对象有道路划线、垃圾桶等城市设施、人等；
2. 对于深度6，可以表征的地物尺寸约$4 \times 4$m，对象有车辆、单株植被等；
3. 深度5，可以表征的地物尺寸约$8 \times 8$m，对象有小径、单株植被、建筑顶的空调系统等；
4. 深度4，可以表征的地物尺寸约$16 \times 16$m，对象有独栋住宅等建筑、成片的植被、城市道路等；
5. 深度3，可以表征的地物尺寸约$32 \times 32$m，对象有较大的建筑体、适中的停车场、街头绿地等；
6. 深度2，可以表征的地物尺寸约$54 \times 64$m，对象有大型建筑、室外活动场地、成片的绿地等；
7. 深度1，可以表征的地物尺寸约$128 \times 128$m，对象有小块街区、小块绿地或林冠等；
8. 深度0，可以表征的地物尺寸约$256 \times 256$m，对象有体育场馆等特大建筑、中等街区、中等绿地或活动空间等。
    
通过`sgan_model.GMapping()`方法调出 MN 映射网络，因为配置了`mapping_layers`参数为 8，因此总共有8个全连接层`EqualizedLinear`，该层的定义由`class EqualizedLinear(nn.Module)`类调用`torch.nn.functional.linear(input, weight, bias=None) → Tensor`实现。激活函数使用的为`LeakyReLU`。


```python
mapping_net=sgan_model.GMapping()
mapping_net
```




    GMapping(
      (map): Sequential(
        (pixel_norm): PixelNormLayer()
        (dense0): EqualizedLinear()
        (dense0_act): LeakyReLU(negative_slope=0.2)
        (dense1): EqualizedLinear()
        (dense1_act): LeakyReLU(negative_slope=0.2)
        (dense2): EqualizedLinear()
        (dense2_act): LeakyReLU(negative_slope=0.2)
        (dense3): EqualizedLinear()
        (dense3_act): LeakyReLU(negative_slope=0.2)
        (dense4): EqualizedLinear()
        (dense4_act): LeakyReLU(negative_slope=0.2)
        (dense5): EqualizedLinear()
        (dense5_act): LeakyReLU(negative_slope=0.2)
        (dense6): EqualizedLinear()
        (dense6_act): LeakyReLU(negative_slope=0.2)
        (dense7): EqualizedLinear()
        (dense7_act): LeakyReLU(negative_slope=0.2)
      )
    )



训练深度不同，空间尺度大小不同，输入神经元数量（`input_size`）不同，为了保持不同深度趋于等价学习率的效果，调整（归一化） MN 层权重，深度较浅时（趋向 深度0），具有较大的学习率；深度较深时（趋向 深度8），学习率降低。调整的方式上，通过配置`use_wscale`参数，固定`init_std`或`w_mul`，而变化另一个，两种方式结果相同（即$init\_std \times w\_mul$结果值同）。在类`EqualizedLinear`中对应调整权重大小的位置分别为`self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)`和`F.linear(x, self.weight * self.w_mul, bias)`。

从类`EqualizedLinear`中提取归一化学习率的代码，定义为`equalized_lr()`函数，演示数值变化如下：


```python
def equalized_lr(input_size,gain=np.sqrt(2),use_wscale=True,lrmul=0.8):
    he_std=gain * input_size ** (-0.5)  # He init
    if use_wscale:
        init_std = 1.0 / lrmul
        w_mul = he_std * lrmul
    else:
        init_std = he_std / lrmul
        w_mul = lrmul      
        
    print(f'input size={input_size};\tinit_std={init_std};\tw_mul={w_mul}')    

resolution=[pow(2,n) for n in range(2,11)]    
for res in resolution:
    equalized_lr(res,use_wscale=True)
print('-'*50)
for res in resolution:
    equalized_lr(res,use_wscale=False)       
```

    input size=4;	init_std=1.25;	w_mul=0.5656854249492381
    input size=8;	init_std=1.25;	w_mul=0.40000000000000013
    input size=16;	init_std=1.25;	w_mul=0.28284271247461906
    input size=32;	init_std=1.25;	w_mul=0.20000000000000007
    input size=64;	init_std=1.25;	w_mul=0.14142135623730953
    input size=128;	init_std=1.25;	w_mul=0.10000000000000003
    input size=256;	init_std=1.25;	w_mul=0.07071067811865477
    input size=512;	init_std=1.25;	w_mul=0.05000000000000002
    input size=1024;	init_std=1.25;	w_mul=0.03535533905932738
    --------------------------------------------------
    input size=4;	init_std=0.8838834764831844;	w_mul=0.8
    input size=8;	init_std=0.6250000000000001;	w_mul=0.8
    input size=16;	init_std=0.4419417382415922;	w_mul=0.8
    input size=32;	init_std=0.31250000000000006;	w_mul=0.8
    input size=64;	init_std=0.2209708691207961;	w_mul=0.8
    input size=128;	init_std=0.15625000000000003;	w_mul=0.8
    input size=256;	init_std=0.11048543456039805;	w_mul=0.8
    input size=512;	init_std=0.07812500000000001;	w_mul=0.8
    input size=1024;	init_std=0.05524271728019903;	w_mul=0.8
    

下面构建了数据集和数据加载器，提取一批样本用于实验演示。输入的$\mathbf{z}$对象为`gan_input`，批大小为128，维度为512，均值为0，方差为1标准正态分布的噪声向量，out $_i \sim \mathcal{N}(0,1)$。


```python
batch_size=128
num_workers=4
data_loader=sgan_data.get_data_loader(sgan.dataset,batch_size=batch_size,num_workers=num_workers)
images=next(iter(data_loader))
print(images.shape)
```

    torch.Size([128, 3, 1024, 1024])
    


```python
ngpu=1
device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")

latent_size=sgan.opt.model.gen.latent_size
print(f'latent_size={latent_size}')
gan_input=torch.randn(images.shape[0], latent_size) # .to(device) 
print(gan_input.shape)
```

    latent_size=512
    torch.Size([128, 512])
    


```python
fake_samples=mapping_net(gan_input)
print(fake_samples.shape)
```

    torch.Size([128, 512])
    

#### 2)  SN 合成网络

* 常量输入

SN 的作用是生成图像，不同于 PGGAN 网络是给每一深度的子网络都喂入$A$和$B$，$A$是习得对应深度 MN 映射网络仿射变换的结果$\mathbf{w}$，用于控制尺度空间（不同深度）的特征；$B$是转化后的随机噪声，用于丰富生成图像的细节。SN 的输入移除了噪声向量$\mathbf{z}$（latent code），替代为常量输入（值为1），维度为$4 \times 4 \times 512$，如下代码。


```python
const=torch.nn.Parameter(torch.ones(1, 512, 4, 4)).expand(batch_size,-1,-1,-1)
print(f'const:{const.shape}')
```

    const:torch.Size([128, 512, 4, 4])
    

* 注入噪声向量

在 AdaIN  层之前添加噪声向量，代码如下：

```python
class NoiseLayer(nn.Module):
    """adds noise. noise is per pixel (constant over channels) with per-channel weight"""

    def __init__(self, channels):
        super().__init__()
        self.weight = nn.Parameter(torch.zeros(channels))
        self.noise = None

    def forward(self, x, noise=None):
        if noise is None and self.noise is None:
            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)
        elif noise is None:
            # here is a little trick: if you get all the noise layers and set each
            # modules .noise attribute, you can have pre-defined noise.
            # Very useful for analysis
            noise = self.noise
        x = x + self.weight.view(1, -1, 1, 1) * noise
        return x
```

当为 SN 网络输入层时，`x`为常量`const`，后续其它层则为对应深度卷积（上采样）后的图像。噪声置入的方式为训练的权重缩放噪声向量后与`x`相加。`nn.Parameter`方法将噪声权重值转换为可训练的参数并注册到模型参数中，其初始值为0，因此为了演示实验，在该章开始*迁移 StyleGAN*部分训练了几个轮次，因此噪声向量的值已经发生了改变，这里以输入层部分为例，通过`sgan.style_gan.gen`方法提取 $G$网络，用`G.state_dict()`查看$G$网络全部参数名称和对应的值，找到噪声权重名称（路径），通过`G.g_synthesis.init_block.epi2.top_epi.noise.weight`方法提取噪声权重。


```python
G=sgan.style_gan.gen
G.state_dict().keys()
```




    odict_keys(['g_mapping.map.dense0.weight', 'g_mapping.map.dense0.bias', 'g_mapping.map.dense1.weight', 'g_mapping.map.dense1.bias', 'g_mapping.map.dense2.weight', 'g_mapping.map.dense2.bias', 'g_mapping.map.dense3.weight', 'g_mapping.map.dense3.bias', 'g_mapping.map.dense4.weight', 'g_mapping.map.dense4.bias', 'g_mapping.map.dense5.weight', 'g_mapping.map.dense5.bias', 'g_mapping.map.dense6.weight', 'g_mapping.map.dense6.bias', 'g_mapping.map.dense7.weight', 'g_mapping.map.dense7.bias', 'g_synthesis.init_block.const', 'g_synthesis.init_block.bias', 'g_synthesis.init_block.epi1.top_epi.noise.weight', 'g_synthesis.init_block.epi1.style_mod.lin.weight', 'g_synthesis.init_block.epi1.style_mod.lin.bias', 'g_synthesis.init_block.conv.weight', 'g_synthesis.init_block.conv.bias', 'g_synthesis.init_block.epi2.top_epi.noise.weight', 'g_synthesis.init_block.epi2.style_mod.lin.weight', 'g_synthesis.init_block.epi2.style_mod.lin.bias', 'g_synthesis.blocks.0.conv0_up.weight', 'g_synthesis.blocks.0.conv0_up.bias', 'g_synthesis.blocks.0.conv0_up.intermediate.kernel', 'g_synthesis.blocks.0.epi1.top_epi.noise.weight', 'g_synthesis.blocks.0.epi1.style_mod.lin.weight', 'g_synthesis.blocks.0.epi1.style_mod.lin.bias', 'g_synthesis.blocks.0.conv1.weight', 'g_synthesis.blocks.0.conv1.bias', 'g_synthesis.blocks.0.epi2.top_epi.noise.weight', 'g_synthesis.blocks.0.epi2.style_mod.lin.weight', 'g_synthesis.blocks.0.epi2.style_mod.lin.bias', 'g_synthesis.blocks.1.conv0_up.weight', 'g_synthesis.blocks.1.conv0_up.bias', 'g_synthesis.blocks.1.conv0_up.intermediate.kernel', 'g_synthesis.blocks.1.epi1.top_epi.noise.weight', 'g_synthesis.blocks.1.epi1.style_mod.lin.weight', 'g_synthesis.blocks.1.epi1.style_mod.lin.bias', 'g_synthesis.blocks.1.conv1.weight', 'g_synthesis.blocks.1.conv1.bias', 'g_synthesis.blocks.1.epi2.top_epi.noise.weight', 'g_synthesis.blocks.1.epi2.style_mod.lin.weight', 'g_synthesis.blocks.1.epi2.style_mod.lin.bias', 'g_synthesis.blocks.2.conv0_up.weight', 'g_synthesis.blocks.2.conv0_up.bias', 'g_synthesis.blocks.2.conv0_up.intermediate.kernel', 'g_synthesis.blocks.2.epi1.top_epi.noise.weight', 'g_synthesis.blocks.2.epi1.style_mod.lin.weight', 'g_synthesis.blocks.2.epi1.style_mod.lin.bias', 'g_synthesis.blocks.2.conv1.weight', 'g_synthesis.blocks.2.conv1.bias', 'g_synthesis.blocks.2.epi2.top_epi.noise.weight', 'g_synthesis.blocks.2.epi2.style_mod.lin.weight', 'g_synthesis.blocks.2.epi2.style_mod.lin.bias', 'g_synthesis.blocks.3.conv0_up.weight', 'g_synthesis.blocks.3.conv0_up.bias', 'g_synthesis.blocks.3.conv0_up.intermediate.kernel', 'g_synthesis.blocks.3.epi1.top_epi.noise.weight', 'g_synthesis.blocks.3.epi1.style_mod.lin.weight', 'g_synthesis.blocks.3.epi1.style_mod.lin.bias', 'g_synthesis.blocks.3.conv1.weight', 'g_synthesis.blocks.3.conv1.bias', 'g_synthesis.blocks.3.epi2.top_epi.noise.weight', 'g_synthesis.blocks.3.epi2.style_mod.lin.weight', 'g_synthesis.blocks.3.epi2.style_mod.lin.bias', 'g_synthesis.blocks.4.conv0_up.weight', 'g_synthesis.blocks.4.conv0_up.bias', 'g_synthesis.blocks.4.conv0_up.intermediate.kernel', 'g_synthesis.blocks.4.epi1.top_epi.noise.weight', 'g_synthesis.blocks.4.epi1.style_mod.lin.weight', 'g_synthesis.blocks.4.epi1.style_mod.lin.bias', 'g_synthesis.blocks.4.conv1.weight', 'g_synthesis.blocks.4.conv1.bias', 'g_synthesis.blocks.4.epi2.top_epi.noise.weight', 'g_synthesis.blocks.4.epi2.style_mod.lin.weight', 'g_synthesis.blocks.4.epi2.style_mod.lin.bias', 'g_synthesis.blocks.5.conv0_up.weight', 'g_synthesis.blocks.5.conv0_up.bias', 'g_synthesis.blocks.5.conv0_up.intermediate.kernel', 'g_synthesis.blocks.5.epi1.top_epi.noise.weight', 'g_synthesis.blocks.5.epi1.style_mod.lin.weight', 'g_synthesis.blocks.5.epi1.style_mod.lin.bias', 'g_synthesis.blocks.5.conv1.weight', 'g_synthesis.blocks.5.conv1.bias', 'g_synthesis.blocks.5.epi2.top_epi.noise.weight', 'g_synthesis.blocks.5.epi2.style_mod.lin.weight', 'g_synthesis.blocks.5.epi2.style_mod.lin.bias', 'g_synthesis.blocks.6.conv0_up.weight', 'g_synthesis.blocks.6.conv0_up.bias', 'g_synthesis.blocks.6.conv0_up.intermediate.kernel', 'g_synthesis.blocks.6.epi1.top_epi.noise.weight', 'g_synthesis.blocks.6.epi1.style_mod.lin.weight', 'g_synthesis.blocks.6.epi1.style_mod.lin.bias', 'g_synthesis.blocks.6.conv1.weight', 'g_synthesis.blocks.6.conv1.bias', 'g_synthesis.blocks.6.epi2.top_epi.noise.weight', 'g_synthesis.blocks.6.epi2.style_mod.lin.weight', 'g_synthesis.blocks.6.epi2.style_mod.lin.bias', 'g_synthesis.blocks.7.conv0_up.weight', 'g_synthesis.blocks.7.conv0_up.bias', 'g_synthesis.blocks.7.conv0_up.intermediate.kernel', 'g_synthesis.blocks.7.epi1.top_epi.noise.weight', 'g_synthesis.blocks.7.epi1.style_mod.lin.weight', 'g_synthesis.blocks.7.epi1.style_mod.lin.bias', 'g_synthesis.blocks.7.conv1.weight', 'g_synthesis.blocks.7.conv1.bias', 'g_synthesis.blocks.7.epi2.top_epi.noise.weight', 'g_synthesis.blocks.7.epi2.style_mod.lin.weight', 'g_synthesis.blocks.7.epi2.style_mod.lin.bias', 'g_synthesis.to_rgb.0.weight', 'g_synthesis.to_rgb.0.bias', 'g_synthesis.to_rgb.1.weight', 'g_synthesis.to_rgb.1.bias', 'g_synthesis.to_rgb.2.weight', 'g_synthesis.to_rgb.2.bias', 'g_synthesis.to_rgb.3.weight', 'g_synthesis.to_rgb.3.bias', 'g_synthesis.to_rgb.4.weight', 'g_synthesis.to_rgb.4.bias', 'g_synthesis.to_rgb.5.weight', 'g_synthesis.to_rgb.5.bias', 'g_synthesis.to_rgb.6.weight', 'g_synthesis.to_rgb.6.bias', 'g_synthesis.to_rgb.7.weight', 'g_synthesis.to_rgb.7.bias', 'g_synthesis.to_rgb.8.weight', 'g_synthesis.to_rgb.8.bias', 'truncation.avg_latent'])




```python
G=sgan.style_gan.gen
noise_weight=G.g_synthesis.init_block.epi2.top_epi.noise.weight
print(noise_weight.shape)
noise_weight[:10]
```

    torch.Size([512])
    




    tensor([ 0.0422,  0.0046,  0.0530,  0.0048,  0.0055,  0.0790, -0.0131,  0.0242,
            -0.0042,  0.0362], device='cuda:0', grad_fn=<SliceBackward0>)




```python
x=const.to(device) 
noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)
x = x + noise_weight.view(1, -1, 1, 1) * noise
print(x.shape)
x[0,0,:]
```

    torch.Size([128, 512, 4, 4])
    




    tensor([[0.9824, 1.0417, 1.0030, 1.0089],
            [0.9722, 1.0714, 1.0557, 1.0131],
            [1.0221, 0.9826, 0.9829, 0.9618],
            [0.9928, 0.9859, 1.0202, 0.9527]], device='cuda:0',
           grad_fn=<SliceBackward0>)



* 深度混合正则化（style mixing regularization）

为了降低 SytleGAN 生成器各深度表征特征的相关性，采用了混合正则化的训练技巧。在训练过程中，随机选择两个$\mathbf{z}$，`latents_in`和`latents2`，通过 MN 映射网络`mapping_net`(`g_mapping`)对应得到两个$\mathbf{w}$，为`dlatents_in`和`dlatents2`，给定随机混合概率`mixing_prob`，在任意随机深度位置`mixing_cutoff`，交换前后两段向量。

为演示混合正则化，提取`class Generator(nn.Module)`类中的代码，定义为`depth_mixing_regularization()`函数，通过下述打印信息可以观察算法逻辑。


```python
def depth_mixing_regularization(mapping_net,dlatents_in,latents_in,resolution,depth,mixing_prob=0.9):  
    latents2 = torch.randn(latents_in.shape).to(latents_in.device)
    dlatents2 = mapping_net(latents2)
    num_layers = (int(np.log2(resolution)) - 1) * 2   
    
    print(f'num_layers={num_layers}')
    layer_idx = torch.from_numpy(np.arange(num_layers)[np.newaxis, :, np.newaxis]).to(latents_in.device)
    print(f'layer_idx:\n{layer_idx}')
    cur_layers = 2 * (depth + 1)
    print(f'cur_layers={cur_layers}')
    mixing_cutoff = random.randint(1,cur_layers) if random.random() < mixing_prob else cur_layers
    print(f'mixing_cutoff={mixing_cutoff}')
    
    dlatents_in = dlatents_in.unsqueeze(1).expand(-1, num_layers, -1)    
    dlatents2 = dlatents2.unsqueeze(1).expand(-1, num_layers, -1)
    
    print(f'dlatents_in shape={dlatents_in.shape},dlatents2 shape={dlatents2.shape}')
    dlatents_in = torch.where(layer_idx < mixing_cutoff, dlatents_in, dlatents2)
    print(f'mixed dlatents_in shape={dlatents_in.shape}')
    
    return dlatents_in
    
dlatents_in=depth_mixing_regularization(mapping_net=mapping_net,dlatents_in=fake_samples,latents_in=gan_input,resolution=1024,depth=3)    
```

    num_layers=18
    layer_idx:
    tensor([[[ 0],
             [ 1],
             [ 2],
             [ 3],
             [ 4],
             [ 5],
             [ 6],
             [ 7],
             [ 8],
             [ 9],
             [10],
             [11],
             [12],
             [13],
             [14],
             [15],
             [16],
             [17]]], dtype=torch.int32)
    cur_layers=8
    mixing_cutoff=3
    dlatents_in shape=torch.Size([128, 18, 512]),dlatents2 shape=torch.Size([128, 18, 512])
    mixed dlatents_in shape=torch.Size([128, 18, 512])
    

* AdaIN (adaptive instance normalization)（自适应实例归一化）

AdaIN 层是一个在生成对抗网络和风格化领域中应用非常广泛的归一化层，在风格编码任务中可以替换`BatchNorm2d`（批归一化层）优化效果，其公式为：$\operatorname{AdaIN}\left(\mathbf{x}_i, \mathbf{y}\right)=\mathbf{y}_{s, i} \frac{\mathbf{x}_i-\mu\left(\mathbf{x}_i\right)}{\sigma\left(\mathbf{x}_i\right)}+\mathbf{y}_{b, i}$，其中$\frac{\mathbf{x}_i-\mu\left(\mathbf{x}_i\right)}{\sigma\left(\mathbf{x}_i\right)}$部分通过`torch.nn.InstanceNorm2d`（实例标准化）方法实现；缩放因子$\mathbf{y}_{s, i} $和偏差因子$\mathbf{y}_{b, i}$由一层$\mathbf{w}$经过定义类`class EqualizedLinear(nn.Module)`可学习的仿射变换后，通过`[-1, 2, x.size(1)] + (x.dim() - 2) * [1]`形状，即`[-1, 2, 512, 1, 1]`，生成转化为缩放因子和偏差因子。代码如下：

```python
class StyleMod(nn.Module):
    def __init__(self, latent_size, channels, use_wscale):
        super(StyleMod, self).__init__()
        self.lin = EqualizedLinear(latent_size,
                                   channels * 2,
                                   gain=1.0, use_wscale=use_wscale)

    def forward(self, x, latent):
        style = self.lin(latent)  # style => [batch_size, n_channels*2]

        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]
        style = style.view(shape)  # [batch_size, 2, n_channels, ...]
        x = x * (style[:, 0] + 1.) + style[:, 1]
        return x
```

演示 AdaIN 层，直接通过`sgan_model.StyleMod`方法调用`StyleMod`类，传入噪声向量`x`时，先执行`InstanceNorm2d`再传入。


```python
dlatents_in_range=dlatents_in[:, 0:2]
print(f'dlatents_in_range shape={dlatents_in_range.shape}')

stylemod=sgan_model.StyleMod(latent_size=512, channels=512, use_wscale=True)

m=torch.nn.InstanceNorm2d(512)
x=m(x)
x_stylemod=stylemod(x.to(dlatents_in_range.device),dlatents_in_range[:, 0])
print(x_stylemod.shape)
```

    dlatents_in_range shape=torch.Size([128, 2, 512])
    torch.Size([128, 512, 4, 4])
    

* 卷积上采样

对分辨率为128及其以上深度的图像使用逆卷积`ConvTranspose2d`方法，对于分辨率小于128深度图像使用最近邻上采样，对应的类为`class Upscale2d(nn.Module)`。


```python
upscale2d=sgan_model.Upscale2d(factor=2, gain=1)
upscale2d_x=upscale2d(x_stylemod)
upscale2d_x.shape
```




    torch.Size([128, 512, 8, 8])



*  $\mathcal{W}$中的截断（truncation）

对于训练数据分布明显的低密度区域，$G$很难学习，因此从截断或紧缩的采样空间提取隐藏向量往往会提高平均图像质量。首先计算$\mathcal{W}$的质心，公式为$\overline{\mathbf{w}} =\mathbb{E}_{\mathbf{z} \sim P(\mathbf{z})}[f(\mathbf{z})]$。然后将给定的$\mathbf{w}$与中心的偏差（deviation）缩放为$\mathbf{w}^{\prime}=\overline{\mathbf{w}}+\psi(\mathbf{w}-\overline{\mathbf{w}})$，式中$ \psi <1$，表示压缩倍数。

截断对应定义的类为`class Truncation(nn.Module)`，代码如下：

```python
class Truncation(nn.Module):
    def __init__(self, avg_latent, max_layer=8, threshold=0.7, beta=0.995):
        super().__init__()
        self.max_layer = max_layer
        self.threshold = threshold
        self.beta = beta
        self.register_buffer('avg_latent', avg_latent)

    def update(self, last_avg):
        self.avg_latent.copy_(self.beta * self.avg_latent + (1. - self.beta) * last_avg)

    def forward(self, x):
        assert x.dim() == 3
        interp = torch.lerp(self.avg_latent, x, self.threshold)
        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1).to(x.device)
        return torch.where(do_trunc, interp, x)
```

其中`avg_latent`注册在模型参数的缓冲区，允许PyTorch跟踪并被模型保存，但是不会在训练过程中使用 SGD 学习更新，但可自定义更新，例如上述`update`方法。从模型中提取`avg_latent`参数值用于演示 $\mathcal{W}$的截断逻辑如下。


```python
avg_latent=G.truncation.avg_latent
print(f'avg_latent shape={avg_latent.shape}')
dlatents_in=dlatents_in.to(avg_latent.device)
interp=torch.lerp(avg_latent, dlatents_in, 0.7)
max_layer=8
do_trunc=(torch.arange(dlatents_in.size(1)) <max_layer).view(1, -1, 1).to(dlatents_in.device)
print(f'do_trunc=\n{do_trunc}')
trunc_dlatents_in=torch.where(do_trunc, interp, dlatents_in)
print(f'trunc_dlatents_in shape={trunc_dlatents_in.shape}')
```

    avg_latent shape=torch.Size([512])
    do_trunc=
    tensor([[[ True],
             [ True],
             [ True],
             [ True],
             [ True],
             [ True],
             [ True],
             [ True],
             [False],
             [False],
             [False],
             [False],
             [False],
             [False],
             [False],
             [False],
             [False],
             [False]]], device='cuda:0')
    trunc_dlatents_in shape=torch.Size([128, 18, 512])
    

#### 3) 对应 StyleGAN 网络的代码结构图

Karras, T. 在论文中给出了 StyleGAN 网络简图，可以用于理解网络的基本结构，但是不容易对应到代码细节的书写上和对已有代码结构细节的理解上，于是基于 StyleGAN 的 PyTorch 版本<sup>[7]</sup>，梳理 StyleGAN 网络对应的代码类和函数如下：

<img src="./imgs/3_4_b/stylegan_diagrame.jpg" height='auto' width='auto' title="caDesign"> 

该作者书写代码规范并具有弹性，StyleGAN 包括$G$和$D$两个网络，并对应到各自的类`Generator`和`Discriminator`。`Generator`含三个部分`self.g_mapping`、`self.g_synthesis`和`self.truncation`，对应3个定义的类，以此类推。其中定义`self.g_synthesis`时，以块（`block`）的方式组织代码，有`self.init_block`和`self.blocks`，方便更深层级的组织、调用和条件编写。将前文对$G$的解释，对照到该图对应的类或函数上，可以辅助阅读源代码。

## 3.4.2 用训练的模型生成航拍遥感影像及尺度空间深度特征控制

### 3.4.2.1  用训练的模型生成航拍遥感影像

训练时保存了多个类型网络模型文件，$G$、$D$及各自的优化器等，在应用训练的模型生成图像时仅使用$G$网络。首先构建同训练时相同的$G$网络，配置有相同的参数，读取训练好的模型更新$G$参数后，用生成的噪声向量生成图像。在 USDA 库中定义`G_imgs`类用于由训练好的网络模型生成图像实验。噪声向量生成部分如下代码：

```python
with torch.no_grad():
    point = torch.randn(1, latent_size)
    point = (point / point.norm()) * (latent_size ** 0.5)
    ss_image = gen(point, depth=out_depth, alpha=1)
    # color adjust the generated image:
    ss_image = adjust_dynamic_range(ss_image)
```

生成的数据位于[-1,1]之间，需要经过定义的`adjust_dynamic_range`函数将其映射到[0,1]区间，转化的方法如下：


```python
def adjust_dynamic_range(data, drange_in=(-1, 1), drange_out=(0, 1)):
    """
    adjust the dynamic colour range of the given input data
    :param data: input image data
    :param drange_in: original range of input
    :param drange_out: required range of output
    :return: img => colour range adjusted images
    """
    if drange_in != drange_out:
        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (
                np.float32(drange_in[1]) - np.float32(drange_in[0]))
        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)
        data = data * scale + bias
    return torch.clamp(data, min=0, max=1)
```


下面生成了 64 个影像，较之 WGAN 网络有更好的表现。


```python
g_model_path=r'I:\model_ckpts\StyleGAN_trained_model_8\GAN_GEN_6_1.pth'
pretrained_G=stylegan.G_imgs(g_model_path)
pretrained_G.opt.num_samples=64
g_imgs=pretrained_G.generate_imgs()
```

    Creating generator object ...
    Loading the generator weights from: I:\model_ckpts\StyleGAN_trained_model_8\GAN_GEN_6_1.pth
    Generating scale synchronized images ...
    

    100%|████████████████████████████████████████████████████████████████████████| 64/64 [00:23<00:00,  2.76it/s]
    


```python
g_imgs_strack=torch.stack(g_imgs).squeeze(dim=1)
grid_img=torchvision.utils.make_grid(g_imgs_strack, nrow=8)

fig, ax = plt.subplots(figsize=(10,10))
ax.imshow(grid_img.permute(1, 2, 0))
fig.tight_layout()
plt.show()
```


<img src="./imgs/3_4_b/output_33_0.png" height='auto' width='auto' title="caDesign">    
    


### 3.4.2.2 $\mathcal{W}$截断空间特征分析

当 $ \psi \mapsto  0$时，所有地理空间信息趋向于一个“均值”属性。为$\overline{\mathbf{w}} (\psi =  0)$。对该分布执行插值不容易引起伪影（artifact）。$ \psi$趋向-1（调换了方向），或1时，显现两个截然相反特征的影像信息，为大体量建筑布局和植被森林分布。趋向于0的位置为这两类特征的融合。下述代码定义了11个值 $ \psi$值，从-1到1，步幅为0.2，可以明显观察到这一特征的变化，因此用$\mathcal{W}$截断的方法可以有目的性的控制两类极端特征的融合分布，对于航拍影像而言为建筑和植被的融合程度，及建筑体量和植被破碎化程度的控制。

$\mathcal{W}$截断核心代码如下：

```python
with torch.no_grad():
    latents_np = np.stack([np.random.RandomState(seed).randn(latent_size) for seed in self.opt.seeds])
    latents = torch.from_numpy(latents_np.astype(np.float32))
    dlatents = self.gen.g_mapping(latents).detach().numpy()  
    dlatent_avg = self.gen.truncation.avg_latent.numpy()  

    canvas = Image.new('RGB', (w * len(self.opt.psis), h * len(self.opt.seeds)), 'white')
    for row, dlatent in enumerate(list(dlatents)):
        row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(self.opt.psis, [-1, 1, 1]) + dlatent_avg
        row_dlatents = torch.from_numpy(row_dlatents.astype(np.float32))
        row_images = self.gen.g_synthesis(row_dlatents, depth=self.opt.out_depth, alpha=1)
        for col, image in enumerate(list(row_images)):
            image = adjust_dynamic_range(image)
            image = image.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()
            canvas.paste(Image.fromarray(image, 'RGB'), (col * w, row * h))
```


```python
pretrained_G_trunc=stylegan.G_truncation_imgs(g_model_path)
pretrained_G_trunc.opt.out_depth=5
pretrained_G_trunc.opt.psis=[-1,-0.8,-0.6,-0.4,-0.2,0,0.2,0.4,0.6,0.8,1]
trunc_imgs=pretrained_G_trunc.draw_truncation_trick_figure()

fig, ax = plt.subplots(figsize=(20, 10))
ax.imshow(trunc_imgs)
for x in list(range(128,1408,128)):
    plt.axvline(x=x, color = 'k', linestyle='--')
plt.show()
```

    Creating generator object ...
    Loading the generator weights from: I:\model_ckpts\StyleGAN_trained_model_8\GAN_GEN_6_1.pth
    


<img src="./imgs/3_4_b/output_35_1.png" height='auto' width='auto' title="caDesign">    
    


### 3.4.2.3 地理空间数据尺度空间深度融合可行性

GAN、DCGAN、WGAN、StyleGAN 等众多生成对抗网络所训练的数据集通常为，人脸，如[Celeb-A Faces dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)，[FFHQ](https://github.com/DmitryUlyanov/texture_nets)等；[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)中的各类分类数据，例如鸟类、各类动物、各类交通工具等；及风景、漫画类。用 StyleGAN 于地理空间数据生成影像等各类信息数据，同样可以利用“解缠”尺度空深度信息融合不同特征影像，例如下述实验，通过$\mathcal{W}$截断，配置$ \psi $为-0.5和0.7获取不同分布变换后的特征影像，一类偏于建筑，一类偏于植被自然，通过融合不同深度`[(0,5),(5,10)]`区间，生成了建筑隐于丛林特征的影像。其核心代码如下：

```python
with torch.no_grad():
    latent_size = self.gen.g_mapping.latent_size
    src_latents_np = np.stack([np.random.RandomState(seed).randn(latent_size, ) for seed in self.opt.src_seeds])
    dst_latents_np = np.stack([np.random.RandomState(seed).randn(latent_size, ) for seed in self.opt.dst_seeds])           

    src_latents = torch.from_numpy(src_latents_np.astype(np.float32))
    dst_latents = torch.from_numpy(dst_latents_np.astype(np.float32))


    src_dlatents = self.gen.g_mapping(src_latents)  
    dst_dlatents = self.gen.g_mapping(dst_latents) 

    #------------------------------------------------------------------
    dlatent_avg = self.gen.truncation.avg_latent.numpy() 

    src_dlatents = src_dlatents.detach().numpy() 
    dst_dlatents = dst_dlatents.detach().numpy() 

    src_dlatents = (src_dlatents - dlatent_avg) * np.reshape(self.opt.src_psis, [-1, 1,1]) + dlatent_avg
    dst_dlatents = (dst_dlatents - dlatent_avg) * np.reshape(self.opt.dst_psis, [-1, 1,1]) + dlatent_avg

    src_dlatents = torch.from_numpy(src_dlatents.astype(np.float32))
    dst_dlatents = torch.from_numpy(dst_dlatents.astype(np.float32))
    #------------------------------------------------------------------

    src_images = self.gen.g_synthesis(src_dlatents, depth=self.opt.out_depth, alpha=1)
    dst_images = self.gen.g_synthesis(dst_dlatents, depth=self.opt.out_depth, alpha=1)

    src_dlatents_np = src_dlatents.numpy()
    dst_dlatents_np = dst_dlatents.numpy()
    canvas = Image.new('RGB', (w * (n_col + 1), h * (n_row + 1)), 'white')
    for col, src_image in enumerate(list(src_images)):
        src_image = adjust_dynamic_range(src_image)
        src_image = src_image.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()
        canvas.paste(Image.fromarray(src_image, 'RGB'), ((col + 1) * w, 0))
    for row, dst_image in enumerate(list(dst_images)):
        dst_image = adjust_dynamic_range(dst_image)
        dst_image = dst_image.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()
        canvas.paste(Image.fromarray(dst_image, 'RGB'), (0, (row + 1) * h))

        row_dlatents = np.stack([dst_dlatents_np[row]] * n_col)
        row_dlatents[:, self.opt.style_ranges[row]] = src_dlatents_np[:, self.opt.style_ranges[row]]
        row_dlatents = torch.from_numpy(row_dlatents)

        row_images = self.gen.g_synthesis(row_dlatents, depth=self.opt.out_depth, alpha=1)
        for col, image in enumerate(list(row_images)):
            image = adjust_dynamic_range(image)
            image = image.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()
            canvas.paste(Image.fromarray(image, 'RGB'), ((col + 1) * w, (row + 1) * h))

```


```python
pretrained_G_mixing=stylegan.G_depth_mixing_imgs(g_model_path)
pretrained_G_mixing.opt.out_depth=5
pretrained_G_mixing.opt.src_seeds=[730, 904] 
pretrained_G_mixing.opt.dst_seeds=[387, 190]
pretrained_G_mixing.opt.style_ranges=[(0,5),(5,10)]
pretrained_G_mixing.opt.src_psis=[-0.5,-0.5]
pretrained_G_mixing.opt.dst_psis=[0.7,0.7]

mixing_imgs=pretrained_G_mixing.draw_depth_mixing_figure()
```

    Creating generator object ...
    Loading the generator weights from: I:\model_ckpts\StyleGAN_trained_model_8\GAN_GEN_6_1.pth
    


```python
fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(mixing_imgs)
plt.show()
```


<img src="./imgs/3_4_b/output_38_0.png" height='auto' width='auto' title="caDesign">    
   


---

注释（Notes）：

① argparse，（<https://docs.python.org/3/library/argparse.html>）。

② yacs，（<https://github.com/rbgirshick/yacs>）。

③ eleb-A Faces dataset，（<http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>）。

④ FFHQ，（<https://github.com/DmitryUlyanov/texture_nets>）。

⑤ CIFAR-10，（<https://www.cs.toronto.edu/~kriz/cifar.html>）。

参考文献（References）:

[1] Karras, T., Aila, T., Laine, S. & Lehtinen, J. Progressive Growing of GANs for Improved Quality, Stability, and Variation. (2017).

[2] Karras, T., Laine, S. & Aila, T. A Style-Based Generator Architecture for Generative Adversarial Networks. (2018).

[3] Karras, T. et al. Analyzing and Improving the Image Quality of StyleGAN. (2019).

[4] Karras, T. et al. Alias-Free Generative Adversarial Networks. https://nvlabs.github.io/stylegan3.

[5] MarsGAN, <https://github.com/kheyer/MarsGAN>

[6] StyleGAN — Official TensorFlow Implementation, <https://github.com/NVlabs/stylegan>

[7] StyleGAN.pytorch, <https://github.com/huangzh13/StyleGAN.pytorch>
