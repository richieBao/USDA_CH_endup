> Last updated on Sun Oct 16 2022 @author: Richie Bao

## 2.1.1 POI数据与描述性统计和正态分布

### 2.1.1.1 单个分类POI数据检索与地理空间点地图

在城市空间分析中，涉及到两个方面的数据，一类是物质空间相关数据，进一步可以分为二维平面数据（例如遥感影像，城市地图等）和三维空间数据（例如雷达数据，含高空扫描和地面扫描，及一般意义上各类格式的城市三维模型等）；另一类则是物质空间承载的各类属性数据，包括反映二维平面数据的属性（与二维地理位置结合，可以划分到二维平面数据当中），例如用地类型、自然资源分布、地址名称等；和反映人类、动物及无形物质各类活动性质的数据，例如人们的活动轨迹（出租车和共享单车车行轨迹、夜间灯光、手机基站定位用户数量等）、动物迁徙路径和各类小气候测量指标变化等。

[百度地图开放平台](http://lbsyun.baidu.com/index.php?title=%E9%A6%96%E9%A1%B5)<sup>①</sup>提供地图相关的功能与服务，其Web服务API为开发者提供`http/https`接口。开发者可以通过`http/https`形式发起检索请求，获取返回JSON（JavaScript Object Notation）或XML（Extensible Markup Language ）格式的检索数据。其中[兴趣点（POI，points of interest）](http://lbsyun.baidu.com/index.php?title=lbscloud/poitags)<sup>②</sup>数据，目前包括21个大类，和无数小类，是反映物质空间承载的人类活动属性的数据（业态分布），如下：

| 一级行业分类 | 二级行业分类                                                                                                                                                                         |
|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 美食         |  中餐厅、外国餐厅、小吃快餐店、蛋糕甜品店、咖啡厅、茶座、酒吧、其他                                                                                                                  |
|  酒店        |  星级酒店、快捷酒店、公寓式酒店、民宿、其他                                                                                                                                          |
|  购物        |  购物中心、百货商场、超市、便利店、家居建材、家电数码、商铺、市场、其他                                                                                                              |
|  生活服务    |  通讯营业厅、邮局、物流公司、售票处、洗衣店、图文快印店、照相馆、房产中介机构、公用事业、维修点、家政服务、殡葬服务、彩票销售点、宠物服务、报刊亭、公共厕所、步骑行专用道驿站、其他  |
|  丽人        |  美容、美发、美甲、美体、其他                                                                                                                                                        |
|  旅游景点    |  公园、动物园、植物园、游乐园、博物馆、水族馆、海滨浴场、文物古迹、教堂、风景区、景点、寺庙、其他                                                                                    |
|  休闲娱乐    |  度假村、农家院、电影院、ktv、剧院、歌舞厅、网吧、游戏场所、洗浴按摩、休闲广场、其他                                                                                                 |
|  运动健身    |  体育场馆、极限运动场所、健身中心、其他                                                                                                                                              |
|  教育培训    |  高等院校、中学、小学、幼儿园、成人教育、亲子教育、特殊教育学校、留学中介机构、科研机构、培训机构、图书馆、科技馆、 其他                                                             |
|  文化传媒    |  新闻出版、广播电视、艺术团体、美术馆、展览馆、文化宫、其他                                                                                                                          |
|  医疗        |  综合医院、专科医院、诊所、药店、体检机构、疗养院、急救中心、疾控中心、医疗器械、医疗保健、其他                                                                                      |
|  汽车服务    |  汽车销售、汽车维修、汽车美容、汽车配件、汽车租赁、汽车检测场、其他                                                                                                                  |
|  交通设施    |  飞机场、火车站、地铁站、地铁线路、长途汽车站、公交车站、公交线路、港口、停车场、加油加气站、服务区、收费站、桥、充电站、路侧停车位、普通停车位、接送点、其他                        |
|  金融        |  银行、ATM、信用社、投资理财、典当行、其他                                                                                                                                           |
|  房地产      |  写字楼、住宅区、宿舍、内部楼栋、其他                                                                                                                                                |
|  公司企业    |  公司、园区、农林园艺、厂矿、其他                                                                                                                                                    |
|  政府机构    |  中央机构、各级政府、行政单位、公检法机构、涉外机构、党派团体、福利机构、政治教育机构、社会团体、民主党派、居民委员会、其他                                                          |
|  出入口      |  高速公路出口、高速公路入口、机场出口、机场入口、车站出口、车站入口、门（备注：建筑物和建筑物群的门）、停车场出入口、自行车高速出口、自行车高速入口、自行车高速出入口、其他          |
|  自然地物    |  岛屿、山峰、水系、其他                                                                                                                                                              |
|  行政地标    |  省、省级城市、地级市、区县、商圈、乡镇、村庄、其他                                                                                                                                  |
|  门址        |  门址点、其他  

> 引自：[百度地图开放平台](http://lbsyun.baidu.com/index.php?title=lbscloud/poitags) 更新于2020年5月11日。

#### 1）单个分类检索

检索数据，需要查看百度提供的检索方法，根据要求来配置对应参数实现下载，具体查看[服务文档](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi)<sup>③</sup>。本次实验对应配置检索语句为，矩形区域检索:`http://api.map.baidu.com/place/v2/search?query=银行&bounds=39.915,116.404,39.975,116.414&output=json&ak={您的密钥} //GET请求`，百度地图例举的语句仅包含个别请求参数，实际上提供的行政区划区域检索请求参数约15个左右，可以根据对数据的下载需求来确定使用哪些请求参数。检索服务包括圆形区域检索、地点详情检索和多边形区域检索服务等。此次矩形区域检索（多边形区域检索）的请求参数配置说明如下：
```python
query_dic={
   'query':检索关键字。圆形区域检索和矩形区域内检索支持多个关键字并集检索，不同关键字间以“$”符号分隔，最多支持10个关键字检索，如:”银行$酒店”。如果需要按POI分类进行检索，将分类通过query参数进行设置，例如 query='旅游景点',
   'page_size': 单次召回POI数量，默认为10条记录，最大返回20条。多关键字检索时，返回的记录数为“关键字个数*page_size”，例如 page_size='20', 
   'page_num':分页页码，默认为0, 0代表第一页, 1代表第二页，以此类推。常与page_size搭配使用，例如 page_num='0',
   'scope':检索结果详细程度。取值为1或空，则返回基本信息；取值为2，返回检索POI详细信息，例如 scope='2',
   'bounds':检索矩形区域，为左下角和右上角坐标，多组坐标间以“,”分隔, 例如 str(leftBottomCoordi[1]) + ',' + str(leftBottomCoordi[0]) + ','+str(rightTopCoordi[1]) + ',' + str(rightTopCoordi[0]),
   'output':输出格式为json或者xml，例如output='json',
   'ak':开发者的访问密钥，必填项。v2之前该属性为key,                   
}
```

如需下载数据，请求参数需要申请访问密钥'ak'（注意，需要注册登录)。

首先来配置基本参数，最终要实现的目的是将下载后的数据分别存储为CSV（Comma-Separated Values，逗号（字符）分隔值，以纯文本形式存储表格数据）和JSON（JavaScript Object Notation，轻量级数据交换格式，层次结构简洁清晰）数据格式，因此调入`json`和`csv`库，辅助读写对应格式的文件。因为要通过网页地址来检索数据，因此调入`urllib`库实现相应的URL（Uniform Resource Locator，统一资源定位符，即网络地址）处理。对文件路径的管理则可以使用`os`及`pathlib`库。

> 注意，多边形区域检索目前为高级权限，如有需求，需要在百度地图开放平台上提交工单咨询。读者可以从本书提供的数据下载链接下载该数据，无需自行从百度地图开放平台检索下载。

* 多边形区域检索（矩形区域检索） 


```python
import os

data_path='./data' # 配置数据存储位置
# 定义存储文件名
poi_fn_csv=os.path.join(data_path,'poi_csv.csv')
poi_fn_json=os.path.join(data_path,'poi_json.json')
```

配置请求参数，注意其中`page_num`参数为页数递增，初始参数时，配置页数范围为`page_num_range=range(20)`。保存的文件类型直接由定义的存储文件名的后缀名确定。由于百度API的限制，检索区域内最多返回的POI数据量有限制，造成下载疏漏，因此如果下载区域很大时，最好是将其切分为数个矩形逐一下载，从而增加一个配置参数`partition`实现检索区域的切分次数，如果设置为2，则切分矩形为4份检索区域分别下载。

注意在配置坐标时，使用[百度地图坐标拾取系统](http://api.map.baidu.com/lbsapi/getpoint/index.html)<sup>④</sup>。


```python
bound_coordinate={'leftBottom':[108.776852,34.186027],'rightTop':[109.129275,34.382171]} 
page_num_range=range(20)
partition=4 
query_dic={
    'query':'旅游景点',
    'page_size':'20', 
    'scope':2,
    'ak':'2Zh7jNunzIzKoWx59ucjHLlZ63oI9St0', 
}
```

通过定义一个函数实现百度地图开放平台下，基于矩形区域检索爬取POI数据，方便日后相关项目的代码迁移或者调用，增加代码融合的力度。因此，需要谨慎确定输入参数，避免在调用时还需调整函数内的变量，造成不必要的错误。由百度地图下载数据，其经纬度坐标为百度坐标系，因此需要对其进行转换，调入转换代码文件`coordinate_transformation.py`（从该书配套GitHub仓库下载）。

在坐标转换过程中，调用了两个转换函数，`bd09togcj02(bd_lon, bd_lat)`和`gcj02towgs84(lng, lat)`。当运行检索函数时，需要将坐标转换文件置于与该.ipynb文件（或者读者新建立包含检索函数的.py文件）同一文件夹下，以备调用。


```python
def baiduPOI_dataCrawler(query_dic,bound_coordinate,partition,page_num_range,poi_fn_list=False):
    '''
    function - 百度地图开放平台POI数据检索——多边形区域检索（矩形区域检索）方式
    
    Params:
        query_dic - 请求参数配置字典，详细参考上文或者百度服务文档；dict
        bound_coordinate - 以字典形式配置下载区域；dict
        partition - 检索区域切分次数；int
        page_num_range - 配置页数范围；range()
        poi_fn_list=False - 定义的存储文件名列表；list
        
    Returns:
        None
    '''
    import coordinate_transformation as cc
    import urllib,json,csv,pathlib
    
    urlRoot='http://api.map.baidu.com/place/v2/search?'  # 数据下载网址，查询百度地图服务文档
    # 切分检索区域
    if bound_coordinate:
        xDis=(bound_coordinate['rightTop'][0]-bound_coordinate['leftBottom'][0])/partition
        yDis=(bound_coordinate['rightTop'][1]-bound_coordinate['leftBottom'][1])/partition    
    # 判断是否要写入文件
    if poi_fn_list:
        for file_path in poi_fn_list:
            fP=pathlib.Path(file_path)
            if fP.suffix=='.csv':
                poi_csv=open(fP,'w',encoding='utf-8')
                csv_writer=csv.writer(poi_csv)    
            elif fP.suffix=='.json':
                poi_json=open(fP,'w',encoding='utf-8')
    num=0
    jsonDS=[]  # 存储读取的数据，用于.json格式数据的保存
    # 循环切分的检索区域，逐区下载数据
    print("Start downloading data...")
    for i in range(partition):
        for j in range(partition):
            leftBottomCoordi=[bound_coordinate['leftBottom'][0]+i*xDis,bound_coordinate['leftBottom'][1]+j*yDis]
            rightTopCoordi=[bound_coordinate['leftBottom'][0]+(i+1)*xDis,bound_coordinate['leftBottom'][1]+(j+1)*yDis]
            for p in page_num_range:  
                # 更新请求参数
                query_dic.update({'page_num':str(p),
                                  'bounds':str(leftBottomCoordi[1]) + ',' + str(leftBottomCoordi[0]) + ',' + 
                                           str(rightTopCoordi[1]) + ',' + str(rightTopCoordi[0]),
                                  'output':'json',
                                 })
                
                url=urlRoot+urllib.parse.urlencode(query_dic)
                data=urllib.request.urlopen(url)
                responseOfLoad=json.loads(data.read())     
                # print(url,responseOfLoad.get("message"))
                if responseOfLoad.get("message")=='ok':
                    results=responseOfLoad.get("results") 
                    for row in range(len(results)):
                        subData=results[row]
                        baidu_coordinateSystem=[subData.get('location').get('lng'),subData.get('location').get('lat')]  # 获取百度坐标系
                        Mars_coordinateSystem=cc.bd09togcj02(baidu_coordinateSystem[0], baidu_coordinateSystem[1])  # 百度坐标系-->火星坐标系
                        WGS84_coordinateSystem=cc.gcj02towgs84(Mars_coordinateSystem[0], Mars_coordinateSystem[1])  # 火星坐标系-->WGS84
                        
                        # 更新坐标
                        subData['location']['lat']=WGS84_coordinateSystem[1]
                        subData['detail_info']['lat']=WGS84_coordinateSystem[1]
                        subData['location']['lng']=WGS84_coordinateSystem[0]
                        subData['detail_info']['lng']=WGS84_coordinateSystem[0]   
                        if csv_writer:
                            csv_writer.writerow([subData])  # 逐行写入.csv文件
                        jsonDS.append(subData)
            num+=1       
            print("No."+str(num)+" was written to the .csv file.")
    if poi_json:       
        json.dump(jsonDS,poi_json)
        poi_json.write('\n')
        poi_json.close()
    if poi_csv:
        poi_csv.close()
    print("The download is complete.")

baiduPOI_dataCrawler(query_dic,bound_coordinate,partition,page_num_range,poi_fn_list=[poi_fn_csv,poi_fn_json])  
```

    Start downloading data...
    No.1 was written to the .csv file.
    No.2 was written to the .csv file.
    No.3 was written to the .csv file.
    No.4 was written to the .csv file.
    No.5 was written to the .csv file.
    No.6 was written to the .csv file.
    No.7 was written to the .csv file.
    No.8 was written to the .csv file.
    No.9 was written to the .csv file.
    No.10 was written to the .csv file.
    No.11 was written to the .csv file.
    No.12 was written to the .csv file.
    No.13 was written to the .csv file.
    No.14 was written to the .csv file.
    No.15 was written to the .csv file.
    No.16 was written to the .csv file.
    The download is complete.
    

* 圆形区域检索（百度地图开放平台没有限制该方法）

可设置圆心和半径，检索圆形区域内的地点信息。官方给出的检索方式为：`https://api.map.baidu.com/place/v2/search?query=银行&location=39.915,116.404&radius=2000&output=xml&ak=您的密钥 //GET请求`


```python
def baiduPOI_dataCrawler_circle(query_dic,poi_save_path,page_num_range):
    '''
    function - 百度地图开放平台POI数据检索——圆形区域检索方式
    
    Params:
        query_dic - 请求参数配置字典，详细参考上文或者百度服务文档；dict
        poi_save_path - 存储文件路径；string
        page_num_range - 配置页数范围；range()
        
    Returns:
        None
    '''
    import coordinate_transformation as cc
    import urllib,json,csv,os,pathlib
    from tqdm import tqdm
    
    urlRoot='http://api.map.baidu.com/place/v2/search?' #数据下载网址，查询百度地图服务文档
    poi_json=open(poi_save_path,'w',encoding='utf-8')  
    jsonDS=[]  # 存储读取的数据，用于.json格式数据的保存
    for p in tqdm(page_num_range): 
        # 更新请求参数
        query_dic.update({'page_num':str(p)})
        url=urlRoot+urllib.parse.urlencode(query_dic)
        data=urllib.request.urlopen(url)
        responseOfLoad=json.loads(data.read())     
        if responseOfLoad.get("message")=='ok':
            results=responseOfLoad.get("results") 
            for row in range(len(results)):
                subData=results[row]
                baidu_coordinateSystem=[subData.get('location').get('lng'),subData.get('location').get('lat')]  # 获取百度坐标系
                Mars_coordinateSystem=cc.bd09togcj02(baidu_coordinateSystem[0], baidu_coordinateSystem[1])  # 百度坐标系-->火星坐标系
                WGS84_coordinateSystem=cc.gcj02towgs84(Mars_coordinateSystem[0],Mars_coordinateSystem[1])  # 火星坐标系-->WGS84

                # 更新坐标
                subData['location']['lat']=WGS84_coordinateSystem[1]
                subData['detail_info']['lat']=WGS84_coordinateSystem[1]
                subData['location']['lng']=WGS84_coordinateSystem[0]
                subData['detail_info']['lng']=WGS84_coordinateSystem[0]  
                jsonDS.append(subData)
    if poi_json:       
        json.dump(jsonDS,poi_json)
        poi_json.write('\n')
        poi_json.close()
    print("The download is complete.")                          
    
page_num_range=range(20)
query_dic={
    'location':'34.265708,108.953431',
    'radius':1000,
    'query':'旅游景点',   
    'page_size':'20',
    'scope':2, 
    'output':'json',
    'ak':'YuN8HxzYhGNfNLGX0FVo3NU3NOrgSNdF'        
}    
poi_save_path='./data/poi_circle.json'
baiduPOI_dataCrawler_circle(query_dic,poi_save_path,page_num_range)
```

    100%|██████████| 20/20 [00:05<00:00,  3.82it/s]

    The download is complete.
    

    
    


```python
import json

with open(poi_save_path) as f:
    poi_circle=json.load(f)
print(poi_circle[0]) 
```

    {'name': '西安城墙', 'location': {'lat': 34.25321785828024, 'lng': 108.94356940023889}, 'address': '陕西省西安市碑林区南大街', 'province': '陕西省', 'city': '西安市', 'area': '碑林区', 'street_id': '6e7e1320b113d8e3bea82230', 'telephone': '(029)87272792', 'detail': 1, 'uid': '6e7e1320b113d8e3bea82230', 'detail_info': {'distance': 874, 'tag': '旅游景点;文物古迹', 'navi_location': {'lng': 108.95260787133, 'lat': 34.259100951615}, 'type': 'scope', 'detail_url': 'http://api.map.baidu.com/place/detail?uid=6e7e1320b113d8e3bea82230&output=html&source=placeapi_v2', 'overall_rating': '4.3', 'comment_num': '200', 'shop_hours': '08:00-22:00', 'children': [], 'lat': 34.25321785828024, 'lng': 108.94356940023889}}
    

> 行政区划区域检索、地点详情检索等方法可以查看[服务文档](http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi)。

#### 2）将CSV格式的POI数据转换为`pandas`的DataFrame格式数据

读取已经保存的`poi_csv.csv`文件时，因为在文件保存时，使用的是`csv`和`json`库，因此读取时仍旧使用对应的库。经常使用的`pandas`库中也有`read_csv()`和`read_json()`等方法。在将数据存储为CSV格式文件，因为保存的数据格式可能存在差异，因此`pandas`读取CSV或者JSON文件最好是其自身所存储的文件，数据格式能够对应。如果直接读取上述保存的POI的.csv文件，则会出现错误。只有知道数据格式，才能够有目的的提取数据，读取每一行的数据格式如下： 

```
{
    "name": "昆明池遗址",
    "location": {
        "lat": 34.210991,
        "lng": 108.779778
    },
    "address": "西安市长安区昆明池七夕公园内",
    "province": "陕西省",
    "city": "西安市",
    "area": "长安区",
    "detail": 1,
    "uid": "c7332cd7fbcc0d82ebe582d9",
    "detail_info": {
        "tag": "旅游景点;景点",
        "navi_location": {
            "lng": 108.7812626866,
            "lat": 34.217484892966
        },
        "type": "scope",
        "detail_url": "http://api.map.baidu.com/place/detail?uid=c7332cd7fbcc0d82ebe582d9&amp;output=html&amp;source=placeapi_v2",
        "overall_rating": "4.3",
        "comment_num": "77",
        "children": []
    }
}
```


读取.csv数据之后，直接使用CSV格式数据来提取和分析数据并不方便。最为常用的数据格式是`NumPy`库提供的数组（array）和`pandas`提供的DataFrame及Series。其中`NumPy`的数据组织形式更倾向于科学计算，为数阵形式，每一数组为同一数据类型；而`pandas`的DataFrame与地理信息数据中的属性表类似，其列（column）可以理解为属性字段（field），每一列的数据类型相同，因此一个DataFrame可以包含多种数据类型。就POI的CSV格式数据，将其转换为DataFrame的数据格式是最好的。进一步而言，在城市空间数据分析方法中，更多的数据是基于地理空间位置的，因此对于具有地理属性的数据，最好能够以地理信息系统常用的数据格式来处理。在处理为DataFrame数据格式之后，则进一步应用`GeoPandas`等地理信息库来处理地理空间信息数据。

在数据格式转换时可能会出现错误，例如原始数据可能存在错误的数据格式：
```
{'name': '励进海升酒店-多功能厅', 'location': {'lat': 34.218525, 'lng': 108.891524}, 'address': '西安市高新区沣惠南路34号励进海升酒店4层', 'province': '陕西省', 'city': '西安市', 'a{'name': '红蚂蚁少儿创意美术馆'", " 'location': {'lat': 34.306666", " 'lng': 108.822465}", " 'address': '陕西省西安市未央区后围寨启航佳苑B区3层商铺'", " 'province': '陕西省'", " 'city': '西安市'", " 'area': '未央区'", " 'telephone': '18209227178", "15229372642'", " 'detail': 1", " 'uid': 'e3fd730bb528b40015c9050c'", " 'detail_info': {'tag': '文化传媒;美术馆'", " 'type': 'scope'", " 'detail_url': 'http://api.map.baidu.com/place/detail?uid=e3fd730bb528b40015c9050c&amp;output=html&amp;source=placeapi_v2'", " 'overall_rating': '0.0'", ' \'children\': []}}
```

其中`'a{'name': '红蚂蚁少儿创意美术馆'",`为多余的部分，并不符合任何格式语法，因此在数据处理时，需要对此做出反应。通常使用`try/except`语句，最好返回可以检索的信息，加以处理，避免数据损失。


```python
def csv2df(poi_fn_csv):
    '''
    function-转换CSV格式的POI数据为pandas的DataFrame
    
    Params:
        poi_fn_csv - 存储有POI数据的CSV格式文件路径             
        
    Returns:
        poi_df - DataFrame(pandas)
    '''
    import pandas as pd
    from benedict import benedict   # benedict库是dict的子类，支持键列表（keylist）/键路径（keypath）。用该库的flatten方法展平嵌套字典，用于DataFrame数据结构
    import csv
        
    n=0
    with open(poi_fn_csv, newline='',encoding='utf-8') as csvfile:
        poi_reader=csv.reader(csvfile)
        poi_dict={}    
        poiExceptions_dict={}
        for row in poi_reader:    
            if row:
                try:
                    row_benedict=benedict(eval(row[0]))  # 用eval方法，将字符串字典"{}"转换为字典{}
                    flatten_dict=row_benedict.flatten(separator='_')  # 展平嵌套字典
                    poi_dict[n]=flatten_dict
                except:                    
                    print("incorrect format of data_row number:%s"%n)                    
                    poiExceptions_dict[n]=row
            n+=1
            # if n==5:break  # 因为循环次数比较多，在调试代码时，可以设置停止的条件，节省时间与方便数据查看
    poi_df=pd.concat([pd.DataFrame(poi_dict[d_k].values(),index=poi_dict[d_k].keys(),columns=[d_k]).T for d_k in poi_dict.keys()], sort=True,axis=0)
    print("_"*50)
    for col in poi_df.columns:
        try:
            poi_df[col]=pd.to_numeric(poi_df[col])
        except:
            print("%s data type is not converted..."%(col))
    print("_"*50)
    print(".csv to DataFrame completed!")
    return poi_df
```

几乎Python的所有数据类型，列表、字典、集合和类等都可以用pickle来序列化存储，`pandas`提供了写入`pandas.DataFrame.to_pickle`，读取`pandas.DataFrame.read_pickle`的方法，因此为了避免每次将CSV转换为DataFrame，可以将`pandas`类型的文件按`pandas`提供的方法加以存储，或写入数据库。


```python
poi_fn_csv='./data/poi_csv.csv'
poi_df=csv2df(poi_fn_csv)
poi_df.to_pickle('./data/poi_df.pkl')

print("_"*50)
print(poi_df.columns)
poi_df.head()
```

    incorrect format of data_row number:4
    incorrect format of data_row number:28
    incorrect format of data_row number:36
    __________________________________________________
    address data type is not converted...
    area data type is not converted...
    city data type is not converted...
    detail_info_detail_url data type is not converted...
    detail_info_indoor_floor data type is not converted...
    detail_info_tag data type is not converted...
    detail_info_type data type is not converted...
    name data type is not converted...
    province data type is not converted...
    street_id data type is not converted...
    telephone data type is not converted...
    uid data type is not converted...
    __________________________________________________
    .csv to DataFrame completed!
    __________________________________________________
    Index(['address', 'area', 'city', 'detail', 'detail_info_checkin_num',
           'detail_info_children', 'detail_info_comment_num',
           'detail_info_detail_url', 'detail_info_facility_rating',
           'detail_info_favorite_num', 'detail_info_hygiene_rating',
           'detail_info_image_num', 'detail_info_indoor_floor', 'detail_info_lat',
           'detail_info_lng', 'detail_info_navi_location_lat',
           'detail_info_navi_location_lng', 'detail_info_overall_rating',
           'detail_info_price', 'detail_info_service_rating', 'detail_info_tag',
           'detail_info_type', 'location_lat', 'location_lng', 'name', 'province',
           'street_id', 'telephone', 'uid'],
          dtype='object')
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>address</th>
      <th>area</th>
      <th>city</th>
      <th>detail</th>
      <th>detail_info_checkin_num</th>
      <th>detail_info_children</th>
      <th>detail_info_comment_num</th>
      <th>detail_info_detail_url</th>
      <th>detail_info_facility_rating</th>
      <th>detail_info_favorite_num</th>
      <th>...</th>
      <th>detail_info_service_rating</th>
      <th>detail_info_tag</th>
      <th>detail_info_type</th>
      <th>location_lat</th>
      <th>location_lng</th>
      <th>name</th>
      <th>province</th>
      <th>street_id</th>
      <th>telephone</th>
      <th>uid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>西安市长安区昆明池七夕公园内</td>
      <td>长安区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>29.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=c733...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;景点</td>
      <td>scope</td>
      <td>34.206767</td>
      <td>108.768459</td>
      <td>昆明池遗址</td>
      <td>陕西省</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>c7332cd7fbcc0d82ebe582d9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>西安市高新区云萃路</td>
      <td>雁塔区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>34.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=6ccb...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;公园</td>
      <td>scope</td>
      <td>34.215761</td>
      <td>108.835407</td>
      <td>云水公园</td>
      <td>陕西省</td>
      <td>6ccb87a451f19a27626858b9</td>
      <td>NaN</td>
      <td>6ccb87a451f19a27626858b9</td>
    </tr>
    <tr>
      <th>6</th>
      <td>西安市长安区昆明池·七夕公园内</td>
      <td>长安区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>5.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=66f2...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;景点</td>
      <td>scope</td>
      <td>34.214174</td>
      <td>108.765677</td>
      <td>汉武大帝</td>
      <td>陕西省</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>66f2005e22dcafdbc7f50d07</td>
    </tr>
    <tr>
      <th>8</th>
      <td>西安市长安区沣东新城斗门新型工业园区汇新路北段昆明池·七夕公园内</td>
      <td>长安区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=576a...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;景点</td>
      <td>scope</td>
      <td>34.211592</td>
      <td>108.765670</td>
      <td>凤求凰</td>
      <td>陕西省</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>576a6060e0ea8ae21ce999c0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>陕西省西安市雁塔区西宝疏导线关中驾校南200米</td>
      <td>雁塔区</td>
      <td>西安市</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>http://api.map.baidu.com/place/detail?uid=8ed2...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>旅游景点;其他</td>
      <td>scope</td>
      <td>34.224172</td>
      <td>108.815592</td>
      <td>大明怀远将军园</td>
      <td>陕西省</td>
      <td>8ed2e7b406ac382b4bcc4a18</td>
      <td>NaN</td>
      <td>8ed2e7b406ac382b4bcc4a18</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 29 columns</p>
</div>



#### 3）将数据格式为DataFrame的POI数据转换为`GeoPandas`地理空间数据GeoDataFrame

可以使用`GeoPandas`库将`pandas`的DataFrame和Series数据转换为具有地理意义的GeoDataFrame和GeoSeries地理数据。`GeoPandas`基于`pandas`库，因此在数据结构上二者之间最大的区别是`GeoPandas`有一个列（column）名为'geometry'，用来存储几何数据，例如点数据`POINT (163.85316 -17.31631)`，单个多边形`POLYGON ((33.90371 -0.95000, 31.86617 -1.02736...))`，多边形集合` MULTIPOLYGON (((120.83390 12.70450, 120.32344 ...)))`等。其中对几何数据的表达是使用`shapely`库实现。大部分SHP地理信息矢量数据，在Python语言中，通常使用该库来建立几何对象。同时GeoDataFrame对象的建立，需要配置坐标系统，这是地理信息数据显著的一个标志，对于坐标系统可以查看[Spatial Reference](https://spatialreference.org/ )<sup>⑤</sup>。

GeoDataFrame可以直接通过`.plot()`方法显示地理空间信息数据。


```python
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point

poi_df=pd.read_pickle('./data/poi_df.pkl')  # 读取已经保存的.pkl(pickle)数据格式的POI
poi_2gdf=poi_df.copy(deep=True)
poi_2gdf['geometry']=poi_2gdf.apply(lambda row:Point(row.location_lng,row.location_lat),axis=1) 
epsg_wgs84=4326  # 配置坐标系统，参考：https://spatialreference.org/  
poi_gdf=gpd.GeoDataFrame(poi_2gdf,crs=epsg_wgs84)

xian_region_gdf=gpd.read_file('./data/xian_region/xian_region.shp')

import matplotlib.pyplot as plt
from pylab import mpl
mpl.rcParams['font.sans-serif']=['DengXian']  # 解决中文字符乱码问题

fig, ax=plt.subplots(figsize=(10,15))
xian_region_plot=xian_region_gdf.plot(ax=ax,color='white', edgecolor='black')
xian_region_gdf.apply(lambda row: xian_region_plot.annotate(text=row.NAME, xy=row.geometry.centroid.coords[0], ha='center',fontsize=13),axis=1) #标注
poi_gdf.plot(ax=ax,column='detail_info_comment_num',markersize=20)

plt.show()
```


<img src="./imgs/2_1_1/output_19_0.png" height='auto' width='auto' title="caDesign">


* 使用`Plotly`库建立地图

`GeoPandas`库提供的地图显示类型功能有限，但却便捷，通常用于数据的查看。当需要打印的地图具有一定质量，表达更多信息，甚至能够交互操作时，可以使用[Plotly](https://plotly.com/)<sup>⑥</sup>图表库实现（通常使用[Plotly Python](https://plotly.com/python/)<sup>⑦</sup>）。下述地图打印结果的底图使用了[mapbox](https://www.mapbox.com/)<sup>⑧</sup>提供的地图数据。要使用其功能，首先需要注册，并获取访问许可（access token）。

使用`Plotly`库建立地图，不需要将DataFrame转换为GeoDataFrame。


```python
import plotly.express as px

poi_gdf.detail_info_price=poi_gdf.detail_info_price.fillna(0)  # pandas库的方法同样适用于GeoPandas库，例如对`nan`位置填充指定数值
mapbox_token='pk.eyJ1IjoicmljaGllYmFvIiwiYSI6ImNrYjB3N2NyMzBlMG8yc254dTRzNnMyeHMifQ.QT7MdjQKs9Y6OtaJaJAn0A'

px.set_mapbox_access_token(mapbox_token)
fig=px.scatter_mapbox(poi_gdf,
                      lat=poi_gdf.location_lat, 
                      lon=poi_gdf.location_lng,
                      color="detail_info_comment_num",
                      color_continuous_scale=px.colors.cyclical.IceFire, 
                      size_max=15, 
                      zoom=10)  # 亦可以选择列，通过size=""配置增加显示信息
fig.show()
fig.write_html('./graph/POI_singlClassi.html') #保存地图
```

<img src="./imgs/2_1_1/output_21_1.png" height='auto' width='auto' title="caDesign">

### 2.1.1.2 多个分类POI数据检索

#### 1）多个分类POI检索

前文定义了两个函数，百度地图开放平台POI数据检索`baiduPOI_dataCrawler()`和转换CSV格式的POI数据为`pandas`的DataFrame格式数据`csv2df()`。为了方便应用建立的函数工具，使用Anaconda的Spyder创建一个新的文件（模块）为util_A.py，将上述两个函数置于其中，同时包括函数所使用的库。对于所包括的库，为方便日后函数迁移和打包自定义Python库发布，明确每个函数所调用的库，将对应调用库的语句分别置于各个函数内部。util_A.py与调用该工具的代码文件置于同一文件夹下。调入语句如下：


```python
import util_A
```

根据百度地图一级行业分类，建立映射字典，用于多个分类POI的数据检索。可以根据数据分析的需求选择行业分类，因为在进一步分析中不需要的分类包括出入口、自然地物、行政地标和门址等，因此在映射字典中未包含上述分类。


```python
poi_classificationName={
        "美食 ":"delicacy",
        "酒店 ":"hotel",
        "购物 ":"shopping",
        "生活服务":"lifeService",
        "丽人 ":"beauty",
        "旅游景点":"spot",
        "休闲娱乐":"entertainment",
        "运动健身":"sports",
        "教育培训":"education",
        "文化传媒":"media",
        "医疗 ":"medicalTreatment",
        "汽车服务":"carService",
        "交通设施":"trafficFacilities",
        "金融":"finance",
        "房地产":"realEstate",
        "公司企业":"corporation",
        "政府机构":"government"
        }
```

配置基本参数。上文配置用到`query_dic`。此次批量下载将所有参数在循环函数外以字典形式单独给出，方便调用。而`query_dic`字典参数在批量下载函数内配置。
```python
query_dic={
    'query':'旅游景点',
    'page_size':'20',
    'scope':2,
    'ak':'2Zh7jNunzIzKoWx59ucjHLlZ63oI9St0',
}
```


```python
poi_config_para={
    'data_path':'./data/poi_batchCrawler/',  # 配置数据存储位置
    'bound_coordinate':{'leftBottom':[108.776852,34.186027],'rightTop':[109.129275,34.382171]},  # 使用百度地图坐标拾取系统 http://api.map.baidu.com/lbsapi/getpoint/index.html
    'page_num_range':range(20),
    'partition':6, 
    'page_size':'20', 
    'scope':2,
    'ak':'2Zh7jNunzIzKoWx59ucjHLlZ63oI9St0', #需要改为读者自行申请的开发者访问密钥
}
```

建立批量下载的循环函数，依据给出的`poi_classificationName`字典键值逐次调用单个分类POI检索函数`baiduPOI_dataCrawler()`下载POI数据。在检索过程中，可以将每一次小批量下载数据存储在同一变量下，待全部下载完后一次性存储。但是这种一次性存储的方式并不推荐，其一，网络有时并不稳定，可能造成下载中断，那么已下载的数据未存储，造成数据丢失，并需要重新下载；其二，有时数据量很大，如果都存储在一个变量下，可能造成内存溢出。


```python
def baiduPOI_batchCrawler(poi_config_para):    
    '''
    function - 百度地图开放平台POI数据批量爬取，
               需要调用单个分类POI检索函数 baiduPOI_dataCrawler(query_dic,bound_coordinate,partition,page_num_range,poi_fn_list=False)
    
    Params:
        poi_config_para - 参数配置，包含：
            'data_path'（配置数据存储位置），
            'bound_coordinate'（矩形区域检索坐下、右上经纬度坐标），
            'page_num_range'（配置页数范围），
            'partition'（检索区域切分次数），
            'page_size'（单次召回POI数量），
            'scope'（检索结果详细程度），
            'ak'（开发者的访问密钥）
            
    Returns:
        None
    '''
    import os
    import util_A   
    
    for idx,(poi_ClassiName,poi_classMapping) in enumerate(poi_classificationName.items()):
        print(str(idx+16)+"_"+poi_ClassiName)
        poi_subFileName="poi_"+str(idx+16)+"_"+poi_classMapping
        data_path=poi_config_para['data_path']
        poi_fn_csv=os.path.join(data_path,poi_subFileName+'.csv')
        poi_fn_json=os.path.join(data_path,poi_subFileName+'.json')
        
        query_dic={
            'query':poi_ClassiName,
            'page_size':poi_config_para['page_size'],
            'scope':poi_config_para['scope'],
            'ak':poi_config_para['ak']                        
        }
        bound_coordinate=poi_config_para['bound_coordinate']
        partition=poi_config_para['partition']
        page_num_range=poi_config_para['page_num_range']
        util_A.baiduPOI_dataCrawler(query_dic,bound_coordinate,partition,page_num_range,poi_fn_list=[poi_fn_csv,poi_fn_json])  
        
baiduPOI_batchCrawler(poi_config_para)
```

    16_美食 
    Start downloading data...
    No.1 was written to the .csv file.
    No.2 was written to the .csv file.
    No.3 was written to the .csv file.
    No.4 was written to the .csv file.
    No.5 was written to the .csv file.
    No.6 was written to the .csv file.
    No.7 was written to the .csv file.
    No.8 was written to the .csv file.
    No.9 was written to the .csv file.
    No.10 was written to the .csv file.
    No.11 was written to the .csv file.
    No.12 was written to the .csv file.
    No.13 was written to the .csv file.
    No.14 was written to the .csv file.
    No.15 was written to the .csv file.
    No.16 was written to the .csv file.
    No.17 was written to the .csv file.
    No.18 was written to the .csv file.
    No.19 was written to the .csv file.
    No.20 was written to the .csv file.
    No.21 was written to the .csv file.
    No.22 was written to the .csv file.
    No.23 was written to the .csv file.
    No.24 was written to the .csv file.
    No.25 was written to the .csv file.
    No.26 was written to the .csv file.
    No.27 was written to the .csv file.
    No.28 was written to the .csv file.
    No.29 was written to the .csv file.
    No.30 was written to the .csv file.
    No.31 was written to the .csv file.
    No.32 was written to the .csv file.
    No.33 was written to the .csv file.
    No.34 was written to the .csv file.
    No.35 was written to the .csv file.
    No.36 was written to the .csv file.
    The download is complete.
    

#### 2）批量转换CSV格式数据为GeoDataFrame格式

在单个分类部分定义了CSV格式数据到GeoDataFrame数据的转换方法`csv2df()`。基于已有代码，该部分将达到两个目的，一是定义单独函数实现CSV格式数据批量转换为GeoDataFame格式数据并存储为.pkl文件；二是批量读取存储为.pkl文件的GeoDataFrame格式数据，并根据需要提取信息存储在单一变量下，再存储为.pkl文件。当读取所有数据于单一变量时，内存需要满足要求，如果有内存溢出，则需考虑是否根据内存情况调整每次读取的数据量。

##### 1. 定义提取文件夹下所有文件路径的函数

因为要批量下载POI数据为多个.csv文件及.json文件，因此在批量处理这些数据时，第一件事是要提取所有文件的路径。定义返回所有指定后缀名文件路径的函数为常用的函数之一，在之后的很多实验中，都需要调用该函数，因此可以将其保存在util_misc.py文件中，方便日后调用。文件夹下通常包括子文件夹，需要使用`os.walk()`方法遍历目录，给出条件语句判断是否存在子文件夹，如果存在则需要返回该文件夹下的文件路径。


```python
def filePath_extraction(dirpath,fileType): 
    '''
    funciton  - 以所在文件夹路径为键，值为包含该文件夹下所有文件名的列表。文件类型可以自行定义 
    
    Params:
        dirpath - 根目录，存储所有待读取的文件；string
        fileType - 待读取文件的类型；list(string)
        
    Returns:
        filePath_Info - 文件路径字典，文件夹路径为键，文件夹下的文件名列表为值；dict
    '''
    import os
    
    filePath_Info={}
    i=0
    for dirpath,dirNames,fileNames in os.walk(dirpath): # os.walk()遍历目录，使用help(os.walk)查看返回值解释
        i+=1
        if fileNames:  # 仅当文件夹中有文件时才提取
            tempList=[f for f in fileNames if f.split('.')[-1] in fileType]
            if tempList:  # 剔除文件名列表为空的情况，即文件夹下存在不为指定文件类型的文件时，上一步列表会返回空列表[]
                filePath_Info.setdefault(dirpath,tempList)
    return filePath_Info

dirpath='./data/poi_batchCrawler/'
fileType=["csv"]
poi_paths=filePath_extraction(dirpath,fileType)
print(poi_paths)
```

    {'./data/poi_batchCrawler/': ['poi_0_delicacy.csv', 'poi_10_medicalTreatment.csv', 'poi_11_carService.csv', 'poi_12_trafficFacilities.csv', 'poi_13_finance.csv', 'poi_14_realEstate.csv', 'poi_15_corporation.csv', 'poi_16_government.csv', 'poi_1_hotel.csv', 'poi_2_shopping.csv', 'poi_3_lifeService.csv', 'poi_4_beauty.csv', 'poi_5_spot.csv', 'poi_6_entertainment.csv', 'poi_7_sports.csv', 'poi_8_education.csv', 'poi_9_media.csv']}
    

##### 2. CSV格式POI数据批量转换为GeoDataFrame格式数据

```python
Index(['address', 'area', 'city', 'detail', 'detail_info_checkin_num','detail_info_children', 'detail_info_comment_num',
       'detail_info_detail_url', 'detail_info_facility_rating','detail_info_favorite_num', 'detail_info_hygiene_rating',
       'detail_info_image_num', 'detail_info_indoor_floor','detail_info_navi_location_lat', 'detail_info_navi_location_lng',
       'detail_info_overall_rating', 'detail_info_price','detail_info_service_rating', 'detail_info_tag', 'detail_info_type',
       'location_lat', 'location_lng', 'name', 'province', 'street_id','telephone', 'uid'],dtype='object')
```
上述为POI数据字段，可以用来确定提取的字段名。除了增加循环语句循环POI的.csv文件，逐一转换为`pandas`的DataFrame格式，再进一步转换为`GeoPandas`的GeoDataFrame格式之外，其它的条件同前文所述。用`GeoDataFrame.plot()`方法初步查看地理空间信息数据。

在文件保存部分，可以有多种保存格式，`GeoPandas`提供了Shapefile、GeoJSON和GeoPackage三种方式，也可以使用`pickle`库保存数据，或存入数据库。用`GeoPandas`提供的保存格式再读取后不再包含多重索引，而pickle格式则保持。

转换为SHP格式文件时，在QGIS等桌面GIS平台下打开时会出现两个问题，一是，如果列（字段）名称过长，转化为属性表的字段名会被字段压缩修改，往往不能反映字段的意义，因此需要置换列名称；二是，用POI一级行业分类名作为index时，列中并不包含该字段，转化为SHP文件时也不包含该字段，因此需要将index转换为列，再存储为.shp文件。


```python
def poi_csv2GeoDF_batch(poi_paths,fields_extraction,save_path):    
    '''
    funciton - CSV格式POI数据批量转换为GeoDataFrame格式数据，需要调用转换CSV格式的POI数据为pandas的DataFrame函数csv2df(poi_fn_csv)
    
    Params:
        poi_paths - 文件夹路径为键，值为包含该文件夹下所有文件名列表的字典；dict
        fields_extraction - 配置需要提取的字段；list(string)
        save_path - 存储数据格式及保存路径的字典；string
        
    Returns:
        poisInAll_gdf - 提取给定字段的POI数据；GeoDataFrame（GeoPandas）
    '''
    import os,pathlib
    import util_A
    import pandas as pd
    import geopandas as gpd
    from shapely.geometry import Point
    from pyproj import CRS
    
    poi_df_dic={}
    i=0
    for key in poi_paths:
        for val in poi_paths[key]:
            poi_csvPath=os.path.join(key,val)
            poi_df=util_A.csv2df(poi_csvPath)  
            # 注释掉csv2df() 函数内部的print("%s data type is not converted..."%(col))语句，以pass替代，减少提示内容，避免干扰
            print(val)
            poi_df_path=pathlib.Path(val)
            poi_df_dic[poi_df_path.stem]=poi_df
            i+=1
            
    poi_df_concat=pd.concat(poi_df_dic.values(),keys=poi_df_dic.keys(),sort=True)
    # print(poi_df_concat.loc[['poi_0_delicacy'],:])  # 提取index为 'poi_0_delicacy'的行，验证结果
    poi_fieldsExtraction=poi_df_concat.loc[:,fields_extraction]
    poi_geoDF=poi_fieldsExtraction.copy(deep=True)
    poi_geoDF['geometry']=poi_geoDF.apply(lambda row:Point(row.location_lng,row.location_lat),axis=1) 
    crs_4326=CRS('epsg:4326')  # 配置坐标系统，参考：https://spatialreference.org/        
    poisInAll_gdf=gpd.GeoDataFrame(poi_geoDF,crs=crs_4326)
    
    poisInAll_gdf.to_pickle(save_path['pkl'])
    poisInAll_gdf.to_file(save_path['geojson'],driver='GeoJSON',encoding='utf-8')
    
    poisInAll_gdf2shp=poisInAll_gdf.reset_index() # 不指定level参数，例如Level=0，会把多重索引中的所有索引转换为列
    poisInAll_gdf2shp.rename(columns={
        'location_lat':'lat', 'location_lng':'lng',
        'detail_info_tag':'tag','detail_info_overall_rating':'rating', 'detail_info_price':'price'},inplace=True)
    poisInAll_gdf2shp.to_file(save_path['shp'],encoding='utf-8')
        
    return poisInAll_gdf

fields_extraction=['name','location_lat', 'location_lng',
                   'detail_info_tag','detail_info_overall_rating', 
                   'detail_info_price']  # 配置需要提取的字段，即列（columns）
# 分别存储为GeoJSON、Shapefile和pickle三种数据格式  
save_path={'geojson':'./data/poisInAll/poisInAll_gdf.geojson',
           'shp':'./data/poisInAll/poisInAll_gdf.shp',
           'pkl':'./data/poisInAll/poisInAll_gdf.pkl'}            
poisInAll_gdf=poi_csv2GeoDF_batch(poi_paths,fields_extraction,save_path)
```

    __________________________________________________
    .csv to DataFrame completed!
    poi_0_delicacy.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_10_medicalTreatment.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_11_carService.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_12_trafficFacilities.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_13_finance.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_14_realEstate.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_15_corporation.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_16_government.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_1_hotel.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_2_shopping.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_3_lifeService.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_4_beauty.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_5_spot.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_6_entertainment.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_7_sports.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_8_education.csv
    __________________________________________________
    .csv to DataFrame completed!
    poi_9_media.csv
    


```python
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import geopandas as gpd
from pylab import mpl

mpl.rcParams['font.sans-serif']=['DengXian'] # 解决中文字符乱码问题

xian_region_gdf=gpd.read_file('./data/xian_region/xian_region.shp')

fig, ax=plt.subplots(figsize=(15,20))
xian_region_plot=xian_region_gdf.plot(ax=ax,color='white', edgecolor='black')
# 标注
xian_region_gdf.apply(lambda row: xian_region_plot.annotate(text=row.NAME, 
                                                            xy=row.geometry.centroid.coords[0], 
                                                            ha='center',
                                                            fontsize=13),
                                                            axis=1) 
divider=make_axes_locatable(ax)
cax=divider.append_axes("right", size="5%", pad=0.1)
poisInAll_gdf.loc[['poi_0_delicacy'],:].plot(ax=ax,
                                             column='detail_info_overall_rating',
                                             markersize=10,
                                             cmap='cividis',
                                             legend=True,
                                             cax=cax)  # 提取index为'poi_0_delicacy'的行查看结果
plt.show()
```

   
<img src="./imgs/2_1_1/output_34_0.png" height='auto' width='auto' title="caDesign">   


> 在[QGIS开源桌面GIS平台](https://www.qgis.org/en/site/index.html)下打开保存的.shp数据。虽然所有工作基本都是在Python中完成，但是有些工作与其它平台相互联系，需要以这些平台为辅助。在辅助平台选择上，尽可能使用具有广泛应用的开源软件。在地理信息系统（Geographic Information Systems，GIS）中主要使用的集成式平台有QGIS和ArcGIS。

<img src="./imgs/2_1_1/2_1_1_01.jpg" height="auto" width="auto" title="caDesign">

#### 3）使用`Plotly`库建立地图

用颜色表示POI一级分类，用大小表示rating字段。


```python
import geopandas as gpd
import plotly.express as px

poi_gdf=gpd.read_file('./data/poisInAll/poisInAll_gdf.shp')  # 读取存储的.shp格式文件
poi_gdf.rating=poi_gdf.rating.fillna(0) # pandas库的方法同样适用于GeoPandas库，例如对`nan`位置填充指定数值
mapbox_token='pk.eyJ1IjoicmljaGllYmFvIiwiYSI6ImNrYjB3N2NyMzBlMG8yc254dTRzNnMyeHMifQ.QT7MdjQKs9Y6OtaJaJAn0A'

px.set_mapbox_access_token(mapbox_token)
fig=px.scatter_mapbox(poi_gdf,lat=poi_gdf.lat, lon=poi_gdf.lng,
                      color="level_0",size='rating',
                      color_continuous_scale=px.colors.cyclical.IceFire, size_max=5, zoom=10) # 亦可以选择列，通过size=""配置增加显示信息
fig.show()
```

<img src="./imgs/2_1_1/output_37_1.png" height='auto' width='auto' title="caDesign">


### 2.1.1.3 描述性统计图表

#### 1）读取与查看数据

* 读取已经保存的.pkl数据。通过`.plot()`确认读取的数据是否正常，或者直接使用`poi_gdf.head()`查看数据。


```python
import pandas as pd

poi_gdf=pd.read_pickle('./data/poisInAll/poisInAll_gdf.pkl')
poi_gdf.plot(marker=".",markersize=5,column='detail_info_overall_rating',figsize=(10,15)) # 只有不设置columns参数时，可以使用color='green'参数
print(poi_gdf.columns) # 查看列名称
```

    Index(['name', 'location_lat', 'location_lng', 'detail_info_tag',
           'detail_info_overall_rating', 'detail_info_price', 'geometry'],
          dtype='object')
    


<img src="./imgs/2_1_1/output_39_1.png" height='auto' width='auto' title="caDesign"> 


#### 2）用`Plotly`表格显示DataFrame格式数据

`print()`是查看数据最为主要的方式，主要用于代码调试。当需要展示数据时，对于DataFrame格式的数据可以使用`Plotly`转换为表格形式。因为POI数据有万行之多，仅显示每一级行业分类的前两行内容。为方便调用，将该功能定义为一个函数`ployly_table()`。提取数据时，因为数据格式是多重索引的DataFrame，因此使用`pandas.IndexSlice()`函数辅助执行多重索引切分。因为用`Plotly`显示表格时，如果为多重索引会显示错误，因此需要`df.reset_index()`重置索引。`Plotly`也不能够显示'geometry'几何对象，在列提取时需要移除该列。

如果在Spyder解释器中打印`Plotly`图表，通常要增加如下代码：

```python
import plotly.io as pio
pio.renderers.default='browser'
```


```python
def plotly_table(df,column_extraction):
    '''
    funciton - 使用Plotly，以表格形式显示DataFrame格式数据
    
    Params:
        df - 输入的DataFrame或者GeoDataFrame；[Geo]DataFrame
        column_extraction - 提取的字段（列）；list(string)
    
    Returns:
        None
    '''
    import plotly.graph_objects as go
    import pandas as pd    
    
    fig=go.Figure(data=[go.Table(
        header=dict(values=column_extraction,
                    fill_color='paleturquoise',
                    align='left'),
        cells=dict(values=df[column_extraction].values.T.tolist(), # values参数值为按列的嵌套列表，因此需要使用参数.T反转数组
                   fill_color='lavender',
                   align='left'))
                   ])
    fig.show()    

df=poi_gdf.loc[pd.IndexSlice[:,:2],:]
df=df.reset_index()
column_extraction=['level_0','name', 'location_lat', 'location_lng', 'detail_info_tag','detail_info_overall_rating', 'detail_info_price']
plotly_table(df,column_extraction)
```

<img src="./imgs/2_1_1/output_41_0.png" height='auto' width='auto' title="caDesign">

#### 3） 描述性统计

> 本小节内容主要参考《漫画数据库》<sup>[1]</sup>、《漫画统计学》<sup>[2]</sup>及维基百科（Wikipedia）。将枯燥的知识以漫画的形式讲出来，并结合实际的案例由简入繁使枯燥的学习变得有趣起来。欧姆社学习漫画系列和众多以漫画和图示的方式讲解知识的优秀图书都值得推荐。但有利有弊，大部分的漫画图书往往以基础知识为主，深入的研究还是要搜索科学文献和相关论著。同时，漫画形式可以引起读者的兴趣，但是因为穿插故事情节，知识点不易定位，核心的知识内容相对分散，阅读上也要花费更多的时间，因此想学习一门知识，以哪种形式入手，需要根据个人的情况确定。


描述性统计分析是对调查总体所有变量的有关数据做统计性描述，了解各变量内的观察值集中与分散的情况。

- 表示集中趋势（集中量数）的有平均数、中位数、众数、几何平均数、调和平均数等；
- 表示离散程度（变异量数）的有极差（全距）、平均差、标准差、相对差、四分差等。数据的次数分配情况，往往会呈现正态分布；
- 数据的频数（次数）分配情况，往往会呈现正态分布。为了表示测量数据与正态分布偏离的情况，会使用偏度、峰度这两种统计数据；
- 为了解个别观察值在整体中所占的位置，会需要将观察值转换为相对量数，如百分等级、标准分数、四分位数等。

通常在描述性统计中将数据图表化，以直观的方式了解整体资料数据的分布情况，例如直方图、散点图、饼图、折现图、箱型图等。

##### 1. 数据种类

通常数据可以分为两类，不可测量的数据称为分类数据；可测量的数据称为数值数据。上述图表中'level_0'、 'detail_info_tag'字段均为分类数据，'location_lat'、'location_lng'、'detail_info_overall_rating'、'detail_info_price'均为数值数据，而'name'字段则是数据的索引名称。

##### 2. 数值数据的描述性统计

* 频数（次数）分布表和直方图

建立简单数据示例，数据来源于《漫画统计学》中“美味拉面畅销前50”上刊载的拉面馆的拉面价格。一般对于只有一组（一个特征、一个自变量/解释变量）的数据通常使用`pandas.Series()`建立Series格式数据，但是后续分析会加入新的数据，因此仍旧建立DataFrame格式数据。使用`df.describe()`可以初步查看主要统计值。


```python
ramen_price=pd.DataFrame([700,850,600,650,980,750,500,890,880,700,890,720,680,650,790,670,680,900,880,720,850,700,780,850,750,
                           780,590,650,580,750,800,550,750,700,600,800,800,880,790,790,780,600,690,680,650,890,930,650,777,700],columns=["price"])
print(ramen_price.describe())
```

                price
    count   50.000000
    mean   743.340000
    std    108.261891
    min    500.000000
    25%    672.500000
    50%    750.000000
    75%    800.000000
    max    980.000000
    

因为有些价格是相同的，一般可以直接使用上述ramen_price数据直接计算频数，但是很多时候相同的数据并不多，且希望分析内容为数值区段间的比较，分析才更具有意义，因此转换为相对量数，以100间隔为一级。范围则根据数据的最大和最小值来确定。


```python
bins=range(500,1000+100,100)  # 配置分割区间（组距）
ramen_price['price_bins']=pd.cut(x=ramen_price.price,bins=bins,right=False)  # 参数right=False指定为包含左边值，不包括右边值
ramenPrice_bins=ramen_price.sort_values(by=['price'])  # 按照分割区间排序
ramenPrice_bins.set_index(['price_bins',ramenPrice_bins.index],drop=False, inplace=True)  # 以price_bins和原索引值设置多重索引，同时配置drop=False参数保留原列
ramenPrice_bins.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>price</th>
      <th>price_bins</th>
    </tr>
    <tr>
      <th>price_bins</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="4" valign="top">[500, 600)</th>
      <th>6</th>
      <td>500</td>
      <td>[500, 600)</td>
    </tr>
    <tr>
      <th>31</th>
      <td>550</td>
      <td>[500, 600)</td>
    </tr>
    <tr>
      <th>28</th>
      <td>580</td>
      <td>[500, 600)</td>
    </tr>
    <tr>
      <th>26</th>
      <td>590</td>
      <td>[500, 600)</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">[600, 700)</th>
      <th>34</th>
      <td>600</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>41</th>
      <td>600</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>600</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>44</th>
      <td>650</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>650</td>
      <td>[600, 700)</td>
    </tr>
    <tr>
      <th>27</th>
      <td>650</td>
      <td>[600, 700)</td>
    </tr>
  </tbody>
</table>
</div>



数值区段间的频数计算


```python
ramenPriceBins_frequency=ramenPrice_bins.price_bins.value_counts()  
ramenPriceBins_relativeFrequency=ramenPrice_bins.price_bins.value_counts(normalize=True)  # 参数normalize=True将计算相对频数(次数)
ramenPriceBins_freqANDrelFreq=pd.DataFrame({'fre':ramenPriceBins_frequency,'relFre':ramenPriceBins_relativeFrequency})
ramenPriceBins_freqANDrelFreq
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fre</th>
      <th>relFre</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>[700, 800)</th>
      <td>18</td>
      <td>0.36</td>
    </tr>
    <tr>
      <th>[600, 700)</th>
      <td>13</td>
      <td>0.26</td>
    </tr>
    <tr>
      <th>[800, 900)</th>
      <td>12</td>
      <td>0.24</td>
    </tr>
    <tr>
      <th>[500, 600)</th>
      <td>4</td>
      <td>0.08</td>
    </tr>
    <tr>
      <th>[900, 1000)</th>
      <td>3</td>
      <td>0.06</td>
    </tr>
  </tbody>
</table>
</div>



组中值计算


```python
ramenPriceBins_median=ramenPrice_bins.groupby(level=0).median(numeric_only=True)
ramenPriceBins_median.rename(columns={'price':'median'},inplace=True)
ramenPriceBins_median
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median</th>
    </tr>
    <tr>
      <th>price_bins</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>[500, 600)</th>
      <td>565.0</td>
    </tr>
    <tr>
      <th>[600, 700)</th>
      <td>650.0</td>
    </tr>
    <tr>
      <th>[700, 800)</th>
      <td>750.0</td>
    </tr>
    <tr>
      <th>[800, 900)</th>
      <td>865.0</td>
    </tr>
    <tr>
      <th>[900, 1000)</th>
      <td>930.0</td>
    </tr>
  </tbody>
</table>
</div>



合并分割区间、频数计算和组中值的DataFrame格式数据。


```python
ramen_fre=ramenPriceBins_freqANDrelFreq.join(ramenPriceBins_median).sort_index().reset_index()  # 在合并时会自动匹配index
ramen_fre
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>fre</th>
      <th>relFre</th>
      <th>median</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[500, 600)</td>
      <td>4</td>
      <td>0.08</td>
      <td>565.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[600, 700)</td>
      <td>13</td>
      <td>0.26</td>
      <td>650.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[700, 800)</td>
      <td>18</td>
      <td>0.36</td>
      <td>750.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[800, 900)</td>
      <td>12</td>
      <td>0.24</td>
      <td>865.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[900, 1000)</td>
      <td>3</td>
      <td>0.06</td>
      <td>930.0</td>
    </tr>
  </tbody>
</table>
</div>



计算频数比例，即各个区间频数占总数的百分比，能够更清晰比较之间的差异大小。配合经常使用的`df.apply()`和`lambda`匿名函数，能够巧妙的以一种简洁的方式书写代码。


```python
ramen_fre['fre_percent%']=ramen_fre.apply(lambda row:row['fre']/ramen_fre.fre.sum()*100,axis=1)
ramen_fre
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>fre</th>
      <th>relFre</th>
      <th>median</th>
      <th>fre_percent%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[500, 600)</td>
      <td>4</td>
      <td>0.08</td>
      <td>565.0</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[600, 700)</td>
      <td>13</td>
      <td>0.26</td>
      <td>650.0</td>
      <td>26.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[700, 800)</td>
      <td>18</td>
      <td>0.36</td>
      <td>750.0</td>
      <td>36.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[800, 900)</td>
      <td>12</td>
      <td>0.24</td>
      <td>865.0</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[900, 1000)</td>
      <td>3</td>
      <td>0.06</td>
      <td>930.0</td>
      <td>6.0</td>
    </tr>
  </tbody>
</table>
</div>



直方图

`Pandas`自身就带有不少图表打印的功能（基于`Matplotlib`库），不必过多的调整数据自身结构，就可迅速的预览。但是不像`Plotly`具有图表交互功能。


```python
ramen_fre.loc[:,['fre','index']].plot.bar(x='index',rot=0,figsize=(5,5))
```




    <AxesSubplot:xlabel='index'>




<img src="./imgs/2_1_1/output_55_1.png" height='auto' width='auto' title="caDesign">
    


有了上述简单数据的示例，再返回到POI实验数据，可以直接迁移上述代码，略作调整后分析POI一级分类美食'poi_0_delicacy'的价格总体分布情况。将上述的所有分析代码置于一个函数中`frequency_bins()`，函数的主要功能是计算DataFrame数据格式下，指定组距，一列数据的频数分布。将上述零散的代码纳入到一个函数中，需要注意几点事宜。一是，尽可能让变量名具有普适性，例如原变量名ramenPrice_bins在函数中更改为df_bins，因为该函数同样可以计算'detail_info_overall_rating'字段的频数分布情况；二是，公用的常用变量尽量在开始配置，例如column_name和column_bins_name，这样避免不断的用原始的语句，例如重复使用`df.columns[0]+'_bins'`，从而导致代码可读性较差；再者，基本不可能直接迁移上述代码于函数中使用，需要逐句或者逐段的迁移测试，例如原代码`ramenPrice_bins.price_bins`，在函数中改为`df_bins[column_bins_name]`，因为列名是以变量名形式存储，无法直接用`.`的方式读取数据。最后，需要注意函数返回值的灵活性，例如并未在函数内部定义图表打印，而是返回DataFrame格式的数据，因为打印的方式比较多样化，可以仅打印一列数据的柱状图，或者多列，及图表的形式也会多样化，因此这部分工作放置于函数外处理，增加灵活性。


```python
def frequency_bins(df,bins,field):    
    '''
    function - 频数分布计算
    
    Params:
        df - 单列（数值类型）的DataFrame数据；DataFrame(Pandas)
        bins - 配置分割区间（组距）；range()，例如：range(0,600+50,50)
        field - 字段名；string
        
    Returns:
        df_fre - 统计结果字段包含：index（为bins）、fre、relFre、median和fre_percent%；DataFrame
    '''
    import pandas as pd
    
    # A-组织数据
    column_name=df.columns[0]
    column_bins_name=df.columns[0]+'_bins'
    df[column_bins_name]=pd.cut(x=df[column_name],bins=bins,right=False)  # 参数right=False指定为包含左边值，不包括右边值。
    df_bins=df.sort_values(by=[column_name])  # 按照分割区间排序
    df_bins.set_index([column_bins_name,df_bins.index],drop=False,inplace=True)  # 以price_bins和原索引值设置多重索引，同时配置drop=False参数保留原列。
    
    # B-频数计算
    dfBins_frequency=df_bins[column_bins_name].value_counts()  # dropna=False  
    dfBins_relativeFrequency=df_bins[column_bins_name].value_counts(normalize=True)  # 参数normalize=True将计算相对频数(次数) dividing all values by the sum of values
    dfBins_freqANDrelFreq=pd.DataFrame({'fre':dfBins_frequency,'relFre':dfBins_relativeFrequency})
    
    # C-组中值计算
    df_bins[field]=df_bins[field].astype(float)
    dfBins_median=df_bins.groupby(level=0).median(numeric_only=True)
    dfBins_median.rename(columns={column_name:'median'},inplace=True)
    
    # D-合并分割区间、频数计算和组中值的DataFrame格式数据。
    df_fre=dfBins_freqANDrelFreq.join(dfBins_median).sort_index().reset_index()  # 在合并时会自动匹配index
    
    # E-计算频数比例
    df_fre['fre_percent%']=df_fre.apply(lambda row:row['fre']/df_fre.fre.sum()*100,axis=1)
    
    return df_fre

pd.options.mode.chained_assignment=None
bins=range(0,600+50,50)  # 配置分割区间（组距）  
delicacy_price_df=poi_gdf[["detail_info_price"]]
delicacy_price_df["detail_info_price"]=delicacy_price_df["detail_info_price"].apply(pd.to_numeric,errors='coerce')  # 将str(object)数据类型转换为数值类型
poiPrice_fre_50=frequency_bins(delicacy_price_df.dropna(),bins,"detail_info_price")    
poiPrice_fre_50
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>fre</th>
      <th>relFre</th>
      <th>median</th>
      <th>fre_percent%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[0, 50)</td>
      <td>2824</td>
      <td>0.445918</td>
      <td>20.0</td>
      <td>44.591821</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[50, 100)</td>
      <td>2030</td>
      <td>0.320543</td>
      <td>73.0</td>
      <td>32.054319</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[100, 150)</td>
      <td>689</td>
      <td>0.108795</td>
      <td>115.0</td>
      <td>10.879520</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[150, 200)</td>
      <td>286</td>
      <td>0.045160</td>
      <td>170.0</td>
      <td>4.516027</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[200, 250)</td>
      <td>164</td>
      <td>0.025896</td>
      <td>223.0</td>
      <td>2.589610</td>
    </tr>
    <tr>
      <th>5</th>
      <td>[250, 300)</td>
      <td>145</td>
      <td>0.022896</td>
      <td>268.0</td>
      <td>2.289594</td>
    </tr>
    <tr>
      <th>6</th>
      <td>[300, 350)</td>
      <td>71</td>
      <td>0.011211</td>
      <td>321.0</td>
      <td>1.121112</td>
    </tr>
    <tr>
      <th>7</th>
      <td>[350, 400)</td>
      <td>45</td>
      <td>0.007106</td>
      <td>373.0</td>
      <td>0.710564</td>
    </tr>
    <tr>
      <th>8</th>
      <td>[400, 450)</td>
      <td>25</td>
      <td>0.003948</td>
      <td>423.0</td>
      <td>0.394758</td>
    </tr>
    <tr>
      <th>9</th>
      <td>[450, 500)</td>
      <td>21</td>
      <td>0.003316</td>
      <td>475.0</td>
      <td>0.331596</td>
    </tr>
    <tr>
      <th>10</th>
      <td>[500, 550)</td>
      <td>21</td>
      <td>0.003316</td>
      <td>522.0</td>
      <td>0.331596</td>
    </tr>
    <tr>
      <th>11</th>
      <td>[550, 600)</td>
      <td>12</td>
      <td>0.001895</td>
      <td>573.5</td>
      <td>0.189484</td>
    </tr>
  </tbody>
</table>
</div>




```python
poiPrice_fre_50.loc[:,['fre','index']].plot.bar(x='index',rot=0,figsize=(15,5))
```




    <AxesSubplot:xlabel='index'>




<img src="./imgs/2_1_1/output_58_1.png" height='auto' width='auto' title="caDesign">    
    


调整组距，查看总体频数分布情况。


```python
import matplotlib.pyplot as plt

bins=list(range(0,300+5,5))+[600]  # 通过df.describe()查看数据后，发现72%的价格位于72元之下，结合上图柱状图，重新配置组距，尽可能的显示数据变化的趋势。
poiPrice_fre_5=frequency_bins(delicacy_price_df,bins,"detail_info_price")    
poiPrice_fre_5.loc[:,['fre','index']].plot.bar(x='index',rot=0,figsize=(30,5))
_=plt.xticks(rotation=90) 
```


<img src="./imgs/2_1_1/output_60_0.png" height='auto' width='auto' title="caDesign">   
   


* 集中量数与变异量数

建立简单数据示例，数据来源于《漫画统计学》保龄球大赛的结果。首先建立嵌套字典，然后将其转换为多重索引的DataFrame数据。


```python
bowlingContest_scores_dic={'A_team':{'Barney':86,'Harold':73,'Chris':124,'Neil':111,'Tony':90,'Simon':38},
                           'B_team':{'Jo':84,'Dina':71,'Graham':103,'Joe':85,'Alan':90,'Billy':89},
                           'C_team':{'Gordon':229,'Wade':77,'Cliff':59,'Arthur':95,'David':70,'Charles':88}
                          }

# 可以逐步拆解来查看每一步的数据结构，结合搜索相关方法解释，可以理解每一步的作用。 例如df.stack()是由列返回多重索引的DataFrame,具体解释可以查看官方案例
bowlingContest_scores=pd.DataFrame.from_dict(bowlingContest_scores_dic, orient='index').stack().to_frame(name='score')  
# 使用print()或者直接在每一JupyterLab的Cell最后给出要查看的变量名，都可以查看数据，只是可能显示的模式略有差异。但建议使用print()查看，因为代码迁移时，单独变量的出现可能会造成代码运行错误
bowlingContest_scores 
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="6" valign="top">A_team</th>
      <th>Barney</th>
      <td>86.0</td>
    </tr>
    <tr>
      <th>Harold</th>
      <td>73.0</td>
    </tr>
    <tr>
      <th>Chris</th>
      <td>124.0</td>
    </tr>
    <tr>
      <th>Neil</th>
      <td>111.0</td>
    </tr>
    <tr>
      <th>Tony</th>
      <td>90.0</td>
    </tr>
    <tr>
      <th>Simon</th>
      <td>38.0</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">B_team</th>
      <th>Jo</th>
      <td>84.0</td>
    </tr>
    <tr>
      <th>Dina</th>
      <td>71.0</td>
    </tr>
    <tr>
      <th>Graham</th>
      <td>103.0</td>
    </tr>
    <tr>
      <th>Joe</th>
      <td>85.0</td>
    </tr>
    <tr>
      <th>Alan</th>
      <td>90.0</td>
    </tr>
    <tr>
      <th>Billy</th>
      <td>89.0</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">C_team</th>
      <th>Gordon</th>
      <td>229.0</td>
    </tr>
    <tr>
      <th>Wade</th>
      <td>77.0</td>
    </tr>
    <tr>
      <th>Cliff</th>
      <td>59.0</td>
    </tr>
    <tr>
      <th>Arthur</th>
      <td>95.0</td>
    </tr>
    <tr>
      <th>David</th>
      <td>70.0</td>
    </tr>
    <tr>
      <th>Charles</th>
      <td>88.0</td>
    </tr>
  </tbody>
</table>
</div>



求每一队的均值（算数平均数）


```python
bowlingContest_mean=bowlingContest_scores.groupby(level=0).mean()
bowlingContest_mean
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A_team</th>
      <td>87.0</td>
    </tr>
    <tr>
      <th>B_team</th>
      <td>87.0</td>
    </tr>
    <tr>
      <th>C_team</th>
      <td>103.0</td>
    </tr>
  </tbody>
</table>
</div>



求每一队的中位数。C队的均值最高，其原因不是每一个队员的成绩都高，而是Gordon获得了229远超其他队员的得分。因此求中位数更为适合。


```python
bowlingContest_median=bowlingContest_scores.groupby(level=0).median()
bowlingContest_median
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A_team</th>
      <td>88.0</td>
    </tr>
    <tr>
      <th>B_team</th>
      <td>87.0</td>
    </tr>
    <tr>
      <th>C_team</th>
      <td>82.5</td>
    </tr>
  </tbody>
</table>
</div>



箱型图（Box plot），又称盒须图，盒式（状）图或箱线图，一种用作显示一组数据分散情况的统计图。显示的一组数据包括最大值、最小值、中位数和上下四分位数，因此使用箱型图较之单一的数值而言可以更清晰的观察数据分布情况。如图（引自：Wikipedia）：
                            +-----+-+       
  *           o     |-------|   + | |---|
                            +-----+-+    
                                         
+---+---+---+---+---+---+---+---+---+---+   分数
0   1   2   3   4   5   6   7   8   9  10
这组数据显示：最小值（minimum）=5；下四分位数（Q1）=7；中位数（Med，即Q2）=8.5；上四分位数（Q3）=9；最大值（maximum）=10；平均值=8；四分位间距(interquartile range)=(Q3-Q2)=2（即ΔQ）。使用`Pandas`自带的plot功能打印箱型图，查看各队分数的分布情况。


```python
bowlingContest_scores_transpose=bowlingContest_scores.stack().unstack(level=0)
boxplot=bowlingContest_scores_transpose.boxplot(column=['A_team', 'B_team', 'C_team'])
```


<img src="./imgs/2_1_1/output_70_0.png" height='auto' width='auto' title="caDesign">   
    


`Plotly`库提供的箱型图可以互动显示具体的数值，具有相对更强的图示能力。


```python
import plotly.express as px

fig=px.box(bowlingContest_scores.xs('C_team',level=0), y="score")
fig.update_layout(
    autosize=False,
    width=500,
    height=500,
    title='C_team'
    )
fig.show()
```

<img src="./imgs/2_1_1/output_72_0.png" height='auto' width='auto' title="caDesign">

求标准差，又称准偏差或均方差（Standard Deviation，SD），数学符号通常用σ（sigma），用于测量一组数值的离散程度，公式：$SD= \sqrt{ \frac{1}{N}  \sum_{i=1}^N { ( x_{i}- \mu )^{2}  } } $ 其中$\mu$ 为平均值。虽然A_team和B_team具有相同的均值，但是数值的分布情况迥然不同，通过计算标准差比较离散程度，标准差越小，代表数据的离散程度越小；反之，标准差越大，离散程度越大。

> 上述公式为整体标准差，本书的实验数据通常为全部数据样本，而不是抽样，因此使用整体标准差。计算样本的标准差，公式为：$SD= \sqrt{ \frac{1}{N-1}  \sum_{i=1}^N { ( x_{i}- \mu )^{2}  } }$


```python
bowlingContest_std=bowlingContest_scores.groupby(level=0).std()
bowlingContest_std
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A_team</th>
      <td>30.172835</td>
    </tr>
    <tr>
      <th>B_team</th>
      <td>10.373042</td>
    </tr>
    <tr>
      <th>C_team</th>
      <td>63.033325</td>
    </tr>
  </tbody>
</table>
</div>



返回到POI实验数据，因为一级行业分类中美食部分的'detial_info_tag'包含餐厅的子分类，使用箱型图显示子分类评分'detail_info_overall_rating'的数值分布情况，并计算标准差。


```python
delicacy=poi_gdf.xs('poi_0_delicacy',level=0)
delicacy_rating=delicacy[['detail_info_tag','detail_info_overall_rating']] 
delicacy_rating
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>detail_info_tag</th>
      <th>detail_info_overall_rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>美食;中餐厅</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>美食;中餐厅</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>美食;中餐厅</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>美食;中餐厅</td>
      <td>4.5</td>
    </tr>
    <tr>
      <th>8</th>
      <td>美食;中餐厅</td>
      <td>4.6</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6590</th>
      <td>美食;小吃快餐店</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6592</th>
      <td>美食;小吃快餐店</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6594</th>
      <td>美食;小吃快餐店</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6596</th>
      <td>美食;小吃快餐店</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6598</th>
      <td>美食;茶座</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>3300 rows × 2 columns</p>
</div>



查看餐厅类型。并移除错误的分类数据，例如`'教育培训;其他'`。同时可以调整子分类的名称，例如由'美食;中餐厅' 修改为'中餐厅'，其中使用了`df.applay()`方法。最后将其映射为英文字符，在打印时也可以避免显示错误。如果显示中文字符错误，需要增加相应处理代码。

`pd.options.mode.chained_assignment=None`方法为`Pandas`的警告异常处理，选项包括：`None`，忽略警告；`warn`，打印警告消息；`raise`，引发异常。


```python
pd.options.mode.chained_assignment = None
print(delicacy_rating.detail_info_tag.unique())
print("_"*50)
delicacy_rating["clean_bool"]=delicacy_rating.detail_info_tag.apply(lambda row:row.split(";")[0]=="美食" if isinstance(row,str) else False)
delicacy_rating_clean=delicacy_rating[delicacy_rating.clean_bool].drop(columns=["clean_bool"]) 
print(delicacy_rating_clean.detail_info_tag.unique())
print("_"*50)

# 定义一个函数，传入df.apply()函数处理字符串
def str_row(row):
    if type(row)==str:
        row_=row.split(';')[-1]
    else:
        row_='nan' # 原数据类型为nan，通过type(row)查看后为float数据类型，此时将其转换为字符串
    return row_
delicacy_rating_clean.loc[:,["detail_info_tag"]]=delicacy_rating_clean["detail_info_tag"].apply(str_row)  
print(delicacy_rating_clean.detail_info_tag.unique())
print("_"*50)

tag_mapping={'中餐厅':'Chinese_restaurant',
             '小吃快餐店':'Snake_bar',
             'nan':'nan',
             '其他':'others',
             '外国餐厅':'Foreign_restaurant',
             '蛋糕甜品店':'CakeANDdessert_shop',
             '咖啡厅':'cafe',
             '茶座':'teahouse',
             '酒吧':'bar',
             '美食':'delicacy',
             '公司':'company',
             '商铺':'store',
             '洗浴按摩':'massage',
             '超市':'supermarket',
             '快捷酒店':'budgetHotel',
             '园区':'Park'}
delicacy_rating_clean.loc[:,["detail_info_tag"]]=delicacy_rating_clean["detail_info_tag"].replace(tag_mapping)
print(delicacy_rating_clean.detail_info_tag.unique())
```

    ['美食;中餐厅' '美食;其他' '美食;小吃快餐店' '美食;咖啡厅' '美食;蛋糕甜品店' '美食;茶座' '房地产;其他'
     '美食;外国餐厅' '美食' '公司企业;公司' '购物;商铺' nan '休闲娱乐;洗浴按摩' '美食;酒吧' '交通设施;其他'
     '公司企业;其他' '购物;超市' '酒店;快捷酒店' '教育培训;其他' '酒店;其他' '公司企业;园区']
    __________________________________________________
    ['美食;中餐厅' '美食;其他' '美食;小吃快餐店' '美食;咖啡厅' '美食;蛋糕甜品店' '美食;茶座' '美食;外国餐厅' '美食'
     '美食;酒吧']
    __________________________________________________
    ['中餐厅' '其他' '小吃快餐店' '咖啡厅' '蛋糕甜品店' '茶座' '外国餐厅' '美食' '酒吧']
    __________________________________________________
    ['Chinese_restaurant' 'others' 'Snake_bar' 'cafe' 'CakeANDdessert_shop'
     'teahouse' 'Foreign_restaurant' 'delicacy' 'bar']
    


```python
delicacy_rating_clean.boxplot(column=['detail_info_overall_rating'],by=['detail_info_tag'],figsize=(25,8))
delicacy_rating_clean_std=delicacy_rating_clean.set_index(['detail_info_tag']).groupby(level=0).std()
delicacy_rating_clean_std
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>detail_info_overall_rating</th>
    </tr>
    <tr>
      <th>detail_info_tag</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>CakeANDdessert_shop</th>
      <td>0.375599</td>
    </tr>
    <tr>
      <th>Chinese_restaurant</th>
      <td>0.703622</td>
    </tr>
    <tr>
      <th>Foreign_restaurant</th>
      <td>0.265208</td>
    </tr>
    <tr>
      <th>Snake_bar</th>
      <td>0.826895</td>
    </tr>
    <tr>
      <th>bar</th>
      <td>0.589164</td>
    </tr>
    <tr>
      <th>cafe</th>
      <td>0.273044</td>
    </tr>
    <tr>
      <th>delicacy</th>
      <td>0.529150</td>
    </tr>
    <tr>
      <th>others</th>
      <td>0.751135</td>
    </tr>
    <tr>
      <th>teahouse</th>
      <td>0.678449</td>
    </tr>
  </tbody>
</table>
</div>




<img src="./imgs/2_1_1/output_79_1.png" height='auto' width='auto' title="caDesign">    

* 标准计分（分数）

建立简单数据示例，数据来源于《漫画统计学》中的考试成绩数据。在这个案例中，虽然Mason和Reece分别在English和Chinese科目中，而history和biology科目中具有相同的分数，但是因为标准差，即离散程度不同，所表示的重要程度亦不一样。标准差越小，离散程度越小，则数值每一单位的变化都会影响最终排名，每一分都很重要。也可以理解为标准差小时，落后的同学相对容易追上成绩在前的同学；但是标准差大时，落后的同学相对不容易追上排名靠前的同学。


```python
test_score_dic={"English":{"Mason":90,"Reece":81,'A':73,'B':97,'C':85,'D':60,'E':74,'F':64,'G':72,'H':67,'I':87,'J':78,'K':85,'L':96,'M':77,'N':100,'O':92,'P':86},
                "Chinese":{"Mason":71,"Reece":90,'A':79,'B':70,'C':67,'D':66,'E':60,'F':83,'G':57,'H':85,'I':93,'J':89,'K':78,'L':74,'M':65,'N':78,'O':53,'P':80},
                "history":{"Mason":73,"Reece":61,'A':74,'B':47,'C':49,'D':87,'E':69,'F':65,'G':36,'H':7,'I':53,'J':100,'K':57,'L':45,'M':56,'N':34,'O':37,'P':70},
                "biology":{"Mason":59,"Reece":73,'A':47,'B':38,'C':63,'D':56,'E':75,'F':53,'G':80,'H':50,'I':41,'J':62,'K':44,'L':26,'M':91,'N':35,'O':53,'P':68},
               }

test_score=pd.DataFrame.from_dict(test_score_dic)
test_score.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>Chinese</th>
      <th>history</th>
      <th>biology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Mason</th>
      <td>90</td>
      <td>71</td>
      <td>73</td>
      <td>59</td>
    </tr>
    <tr>
      <th>Reece</th>
      <td>81</td>
      <td>90</td>
      <td>61</td>
      <td>73</td>
    </tr>
    <tr>
      <th>A</th>
      <td>73</td>
      <td>79</td>
      <td>74</td>
      <td>47</td>
    </tr>
    <tr>
      <th>B</th>
      <td>97</td>
      <td>70</td>
      <td>47</td>
      <td>38</td>
    </tr>
    <tr>
      <th>C</th>
      <td>85</td>
      <td>67</td>
      <td>49</td>
      <td>63</td>
    </tr>
  </tbody>
</table>
</div>



求标准计分（Standard Score），又称z-score，即Z-分数，或标准化值。z-score代表原始数值和平均值之间的距离，并以标准差为单位计算，即z-score是从感兴趣的点到均值之间有多少个标准差，这样就可以在不同组数据间比较某一数值的重要程度。公式为：$z=(x- \mu )/ \sigma $，其中，$\sigma  \neq 0$  ；并$x$是需要被标准化的原始分数；$\mu$是平均值；$\sigma$是标准差。

标准计分的特征：

1. 无论作为变量的取值范围是多少（例如，科目满分为100还是150分），其标准计分的平均数势必为0， 而其标准差势必为1；
2. 无论作为变量的单位是什么（例如分、公斤和小时等），其标准计分的平均数势必为0， 而其标准差势必为1。


```python
from scipy.stats import zscore

test_Zscore=test_score.apply(zscore)
test_Zscore.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>Chinese</th>
      <th>history</th>
      <th>biology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Mason</th>
      <td>0.770054</td>
      <td>-0.296174</td>
      <td>0.780635</td>
      <td>0.162355</td>
    </tr>
    <tr>
      <th>Reece</th>
      <td>-0.029617</td>
      <td>1.392020</td>
      <td>0.207107</td>
      <td>1.014719</td>
    </tr>
    <tr>
      <th>A</th>
      <td>-0.740436</td>
      <td>0.414644</td>
      <td>0.828429</td>
      <td>-0.568242</td>
    </tr>
    <tr>
      <th>B</th>
      <td>1.392020</td>
      <td>-0.385027</td>
      <td>-0.462008</td>
      <td>-1.116191</td>
    </tr>
    <tr>
      <th>C</th>
      <td>0.325792</td>
      <td>-0.651584</td>
      <td>-0.366420</td>
      <td>0.405887</td>
    </tr>
  </tbody>
</table>
</div>



其中Mason在English科目中的标准计分为0.77，在整体分布中位于平均分之上0.71个标准差的地位；而Reece在Chinese中的标准计分为1.39，在整体分布中位于平均分之上1.39个标准差的地位，即Reece获得的每一分值价值高于Mason所获取的每一分值。

返回到POI实验数据，分别计算美食部分总体评分'detail_info_overall_rating'和价格 'detail_info_price'的标准计分。


```python
pd.options.mode.chained_assignment=None
delicacy=poi_gdf.xs('poi_0_delicacy',level=0)
delicacy_dropna=delicacy.dropna(subset=['detail_info_overall_rating', 'detail_info_price'])
delicacy_dropna[['detail_info_overall_rating', 'detail_info_price']]=delicacy_dropna[['detail_info_overall_rating', 'detail_info_price']].astype(float)
delicacy_Zscore=delicacy_dropna[['detail_info_overall_rating', 'detail_info_price']].apply(zscore).join(delicacy["name"])
delicacy_Zscore
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>detail_info_overall_rating</th>
      <th>detail_info_price</th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>0.500851</td>
      <td>-0.903030</td>
      <td>关中印象咥长安(创汇店)</td>
    </tr>
    <tr>
      <th>10</th>
      <td>-0.597102</td>
      <td>-0.493965</td>
      <td>哪儿托海鲜焖面</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1.598804</td>
      <td>-0.493965</td>
      <td>赛百味(昆明池店)</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.720441</td>
      <td>-0.396569</td>
      <td>太平洋咖啡(昆明池店)</td>
    </tr>
    <tr>
      <th>28</th>
      <td>-1.255874</td>
      <td>-0.260214</td>
      <td>文丰厨8</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6432</th>
      <td>-0.597102</td>
      <td>-0.347870</td>
      <td>佳美蛋糕(庆华店)</td>
    </tr>
    <tr>
      <th>6448</th>
      <td>1.598804</td>
      <td>-0.786154</td>
      <td>袁师傅腊汁肉夹馍</td>
    </tr>
    <tr>
      <th>6462</th>
      <td>-7.184819</td>
      <td>-0.864071</td>
      <td>千里香馄饨王(灞桥堡路店)</td>
    </tr>
    <tr>
      <th>6490</th>
      <td>-0.377511</td>
      <td>0.051455</td>
      <td>窑村猪蹄坊总店(纺渭路店)</td>
    </tr>
    <tr>
      <th>6518</th>
      <td>0.500851</td>
      <td>-0.727716</td>
      <td>食膳坊中式快餐</td>
    </tr>
  </tbody>
</table>
<p>2286 rows × 3 columns</p>
</div>



通过计算z_score，比较某一饭店的价格和评分的重要性，其意义是，是否价格越接近均值，对应的评分越高于（低于）均值。但是单独看单一饭店的数据很难判断是否存在这样的一种关系，因此可以打印曲线，观察曲线的变化规律。使用`df.rolling()`方法平滑数据后绘制曲线观察数据。


```python
import matplotlib.pyplot as plt

delicacy_Zscore_selection=delicacy_Zscore.select_dtypes(include="number") # 通过.select_dtypes()方法，提取数值列
delicacy_Zscore_selection.rolling(50, win_type='triang').sum().plot.line(figsize=(25,8))
plt.axhline(y=0,color='gray',linestyle='--')
plt.show()
```


<img src="./imgs/2_1_1/output_87_0.png" height='auto' width='auto' title="caDesign">   


### 2.1.1.4 正态分布与概率密度函数，异常值处理

#### 1）正态分布

在开始概率密度函数之前，认识下正态分布，又称常态分布、高斯分布（normal distribution/Gaussian distribution），因为正态分布的形状，正态分布常见的名称为钟形曲线（bell curve）。用`NumPy`库中的`numpy.random.normal()`方法生成满足指定平均值、和标准差的一维数据（也可以用`SciPy`库stats类中提供的norm方法），并打印直方图和对应由概率密度函数计算的分布曲线。下图y轴表示随机生成数值的频数（因为配置了`density=Ture`，返回的频数为标准化后的结果）；x轴为随机生成的数据集数值分布，因为设置了`plt.hist()`中`bins=30`，将数值分为30等分，计算每一等分（频数宽度）的频数。曲线的顶点为均值、中位数，频数为最高，对应的值为0，该值也为众数。可以从新定义平均数`mu`参数的值，曲线顶点也会随之变动。从众数向两侧移动，曲线高度下降，表示这些值出现的情形逐渐减少，频数降低。在统计学上下图可以文字描述为：x服从平均值为0，标准差为0.1的正态分布。

依据正态分布的形状，其三个基本性质为：一，它是对称的，意味左右部分互为镜像；二，均值、中位数和众数处于同一位置，且在分布的中心，即钟形的顶点。表现为曲线中心最高，首尾两端向下倾斜，呈单峰状；三是，正态分布是渐近的，意味分布的左尾和右尾永远不会触及底线，即x轴。

正态分布具有重要意义，自然界与人类社会中经常出现正态分布的各类数据，例如经济中的收入水平，人的智商（IQ）分数和考试成绩，自然界中受大量微小随机扰动影响的事物等。因此能够依据这一现象推断出现某种情形的准确概率。同时，需要注意，正态分布是统计学中所谓的理论分布，很少有数值严格服从正态分布，而是近似于，也有可能相差较远。违背正态分布假设，则依据正态分布假设所计算的概率结果将不再有效<sup>[3]</sup>。

> 对于统计学的知识，通常是依据已出版的专著或教材为依据进行解释，并根据阐述的内容，以`Python`语言为工具，分析数据的变化，从而可以直接应用代码来解决对应的问题，并能够通过具体的数据分析更深入的理解统计学相关的知识。


```python
# 下述案例参考SciPy.org 中`numpy.random.normal`案例
import math
import numpy as np
import matplotlib.pyplot as plt

# 依据分布配置参数生成数据 Draw samples from the distribution
mu, sigma=0, 0.1  # 配置平均值和标准差 mean and standard deviation
s=np.random.normal(mu, sigma, 1000)
#  验证生成的数据是否满足配置参数 Verify the mean and the variance
print("mean<0.01:",abs(mu-np.mean(s)) < 0.01)
print("sigmag variance<0.01",abs(sigma - np.std(s, ddof=1)) < 0.01)

# 参数density=True；If True, the first element of the return tuple will be the counts normalized to form a probability density
count, bins, ignored=plt.hist(s,bins=30,density=True) 
PDF=1/(sigma * np.sqrt(2 * np.pi)) *np.exp( - (bins - mu)**2 / (2 * sigma**2) )  # y方向的值计算公式即为概率密度函数
plt.plot(bins,PDF ,linewidth=2, color='r',label='PDF') 

CDF=PDF.cumsum()  # 计算累积分布函数
CDF/=CDF[-1]  # 通过除以最大值，将CDF数值分布调整在[0,1]的区间
plt.plot(bins,CDF,linewidth=2, color='g',label='CDF') 

plt.legend()
plt.show()
```

    mean<0.01: True
    sigmag variance<0.01 True
    


<img src="./imgs/2_1_1/output_89_1.png" height='auto' width='auto' title="caDesign">   
    


#### 2）概率密度函数（Probability density function，PDF）

当直方图的组距无限缩小至极限后，能够拟合出一条曲线，计算这个分布曲线的公式为概率密度函数：$f(x)= \frac{1}{ \sigma  \times  \sqrt{2 \pi } }  \times  e^{- \frac{1}{2}  [ \frac{x- \mu }{  \sigma  } ]^{2} } ，$ 式中，$\sigma$为标准差；$\mu$为平均值；$e$为自然对数的底，其值大约为2.7182...。在上述程序中，计算概率密度函数时，并未计算每个数值，而是使用`plt.hist()`返回值`bins`替代，为每一组距的左边沿和右边沿，这里划分了30份，因此首位数为第1个频数宽度的左边沿，末位数为最后一个频数宽度的右边沿，而中间的所有为左右边沿重叠。概率密度函数的积分，为累积分布函数（cumulative distribution function，CDF），可以用`numpy.cumsum()`计算，为给定轴上数组元素的累积和。

图表打印的库主要包括[Matplotlib](https://matplotlib.org/)，[Plotly（含dash）](https://plotly.com/)，[Bokeh](https://docs.bokeh.org/en/latest/index.html)，[seaborn](https://seaborn.pydata.org/)等<sup>⑨</sup>，具体选择哪个打印图表，没有固定的标准，通常根据数据图表打印的目的、由哪个库能够满足要求，及个人习惯用哪个库来确定。在上述使用`Matplotlib`库打印概率密度函数曲线时，是自行计算，而`Seaborn`提供了`seaborn.histplot()`方法，指定`bins`参数后可以直接获取上述结果。`bins`参数的配置，如果为一个整数值，则是划分同宽度频数宽度（bin）的数量，如果是列表，则为频数宽度的边缘，例如[1,2,3,4]，表示[[1,2),[2,3),[3,4]]的频数宽度列表，`[`代表包含左边沿数据，`]`代表包含右边沿数据，而`(`，和`)`则是分别代表不包含左或右边沿数据。同`Pandas`的`pandas.core.indexes.range.RangeIndex`的RangeIndex数据格式。

返回到POI实验数据，直接计算打印POI美食数据的价格概率密度函数的值（纵轴），并连为曲线（分布曲线）。为了进一步观察，组距不断缩小时，与曲线的拟合程度，定义一个循环打印多个连续组距变化的直方图，来观察直方图的变化情况。


```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import math

poi_gdf=pd.read_pickle('./data/poisInAll/poisInAll_gdf.pkl') # 读取已经存储为.pkl格式的POI数据，其中包括geometry字段，为GeoDataFrame地理信息数据，可以通过poi_gpd.plot()迅速查看数据。
delicacy_price=poi_gdf.xs('poi_0_delicacy',level=0).detail_info_price  # 提取美食价格数据
delicacy_price_df=delicacy_price.to_frame(name='price').astype(float)

sns.set(style="white", palette="muted", color_codes=True)
bins=list(range(0,50,5))[1:]
bin_num=len(bins)

ncol=5
nrow=math.ceil(bin_num/ncol)
fig, axs = plt.subplots(ncols=ncol,nrows=nrow,figsize=(30,10),sharex=True)
ax=[(row,col) for row in range(nrow) for col in range(ncol)]
i=0
for bin in bins:
    sns.histplot(delicacy_price_df.price, bins=bin, color="r",ax=axs[ax[i]],kde=True, stat="density", linewidth=0).set(title="bins=%d"%bin)    
    i+=1
```


<img src="./imgs/2_1_1/output_91_0.png" height='auto' width='auto' title="caDesign">   


#### 3）偏度与峰度

描述正态分布的特征（或概率密度函数的分布曲线）有两个概念，一个是偏度（skew），另一个是峰度（kurtosis）。

由`SciPy`库`skewnorm`方法建立具有正偏态和负偏态属性的数据，并由`skew`方法计算偏度值，其公式为：$skewness= \frac{3( \ \mu  -median)}{ \sigma } $ ，式中$\mu$为均值；$\sigma$为标准差。偏度值为负，即负偏态，则概率密度函数左侧尾部比右侧长；偏度值为正，即正偏态，概率密度函数右侧尾部比左侧长。

由`NumPy`建立具有尖峰分布（瘦尾）和扁峰分布（厚尾）属性的数据，配置绝对值参数均为0，变化标准差，使用`SciPy`库`kurtosis`方法计算峰度值（峰度值计算有多个公式，不同软件平台公式也会有所差异）。当平均值为0，标准差为1时的正态分布为标准正态分布，概率密度函数公式可以简化为：$f(x)= \frac{1}{   \sqrt{2 \pi } }  \times  e^{- \frac{ x^{2}}{2}}$

> 在用`NumPy`生成数据集时，间接使用了方差$ \sigma ^{2} $的参数，其概率密度函数公式可以调整为：$f(x |  \mu ,  \sigma ^{2} )= \frac{1}{ \sqrt{2 \pi } \sigma ^{2} }  \times  e^{- \frac{ (x- \mu )^{2} }{2\sigma ^{2} }  } $, $\sigma ^{2}$为方差：  $\sigma ^{2} =   \frac{1}{N}   \sum_{i=1}^N { ( x_{i}- \mu ) ^{2} } $


```python
import math
import numpy as np
from scipy.stats import kurtosis
from scipy.stats import skewnorm
from scipy.stats import skew
import matplotlib.pyplot as plt

# 建立具有正偏态和负偏态属性的数据
skew_list=[7,-7]
skewNorm_list=[skewnorm.rvs(a, scale=1,size=1000,random_state=None) for a in skew_list]
skewness=[skew(d) for d in skewNorm_list]
print('skewness for data:',skewness) # 验证偏度值

# 建立具有尖峰分布、和扁峰分布属性的数据
mu_list=[0,0,0,]
variance=[0.2,1.0,5.0,]
normalDistr_paras=zip(mu_list,[math.sqrt(sig2) for sig2 in variance])  # 配置多个平均值和标准差对
s_list=[np.random.normal(para[0], para[1], 1000) for para in normalDistr_paras]
kur=[kurtosis(s,fisher=True) for s in s_list]
print("kurtosis for data:",kur)

i=0
for skew_data in skewNorm_list:
    sns.kdeplot(skew_data,label="skewness=%.2f"%skewness[i])
    i+=1

n=0
for s in s_list:
    sns.kdeplot(s,label="μ=%s, $σ^2$=%s,kur=%.2f"%(mu_list[n],variance[n],kur[n]))
    n+=1  
plt.legend()
_=plt.plot()
```

    skewness for data: [0.9722605767531536, -0.8485166469879731]
    kurtosis for data: [0.1385811357392006, 0.012989429359109739, -0.11650103191067807]
    


<img src="./imgs/2_1_1/output_93_1.png" height='auto' width='auto' title="caDesign">    



返回POI实验数据，计算POI美食数据价格的偏度和峰度。偏度计算结果值为正，为正偏态，概率密度函数右侧尾部比左侧长，说明多数价格较低，少数较高价格将分布的尾巴托向另一端。峰度为尖峰分布。


```python
delicacy_price_df_clean=delicacy_price_df.dropna()
print("skewness:%.2f, kurtosis:%.2f"%(skew(delicacy_price_df_clean.price),kurtosis(delicacy_price_df_clean.price,fisher=True)))
```

    skewness:4.18, kurtosis:29.84
    

#### 4）检验数据集是否服从正态分布

首先计算标准计分，标准化价格数据集后其平均值为0，标准差为1，绘制概率密度函数分布曲线（理论值）。同时叠加满足上述条件，由`NumPy`随机生成，满足正态分布数据集的分布曲线（观察值），可以比较二者的差异。因为实验数据（美食价格）是正偏态，可以观察到位于均值左侧的数据有所差异，而右侧趋势基本吻合。同时，具有较高的峰度值，高于标准正态分布曲线。


```python
def comparisonOFdistribution(df,field,bins=100):
    '''
    funciton-数据集z-score概率密度函数分布曲线（即观察值/实验值 observed/empirical data）与标准正态分布(即理论值 theoretical set)比较
    
    Params:
        df - 包含待分析数据集的DataFrame格式类型数据；DataFrame(Pandas)
        field - 指定分析数据数据（DataFrame格式）的列名；string
        bins - 指定频数宽度，为单一整数代表频数宽度（bin）的数量；或者列表，代表多个频数宽度的列表。The default is 100；int;list(int)
        
    Returns:
        None
    '''
    import pandas as pd
    import numpy as np
    import seaborn as sns    
    
    df_field_mean=df[field].mean()
    df_field_std=df[field].std()
    print("mean:%.2f, SD:%.2f"%(df_field_mean,df_field_std))

    df['field_norm']=df[field].apply(lambda row: (row-df_field_mean)/df_field_std)  # 标准化价格(标准计分，z-score)，或者使用`from scipy.stats import zscore`方法

    # 验证z-score，标准化后的均值必为0， 标准差必为1.0
    df_fieldNorm_mean=df['field_norm'].mean()
    df_fieldNorm_std=df['field_norm'].std()
    print("norm_mean:%.2f, norm_SD:%.2f"%(df_fieldNorm_mean,df_fieldNorm_std))
  
    sns.histplot(df['field_norm'], bins=bins,kde=True,stat="density",linewidth=0,color='r')

    s=np.random.normal(0, 1, len(df[field]))
    sns.histplot(s, bins=bins,kde=True,stat="density",linewidth=0,color='b')
    
import pandas as pd
pd.options.mode.chained_assignment=None    

comparisonOFdistribution(delicacy_price_df_clean,'price',bins=100)    
```

    mean:54.19, SD:51.22
    norm_mean:0.00, norm_SD:1.00
    


<img src="./imgs/2_1_1/output_97_1.png" height='auto' width='auto' title="caDesign">    
   


* 异常值处理

从直方图或者箱型图中可以观察到偏度的差异，有少数部分较高的价格出现，估计为异常值。

处理异常值，先建立简单数据来理解异常值，并找到相应的方法。使用*How to Detect and Handle Outliers*<sup>[4]</sup>中提供的方法，其公式为：

$MAD= median_{i}{ \{ |  x_{i}-   \widetilde{x} |\} }$ ，式中MAD（the median of the absolute deviation about the median /Median Absolute Deviation）为绝对中位差，其中$\widetilde{x}$为中位数；

$M_{i}= \frac{0.6745( x_{i} - \widetilde{x} )}{MAD}$  ，式中$M_{i}$为修正的z-score（modified  z-score），$\widetilde{x}$为中位数。

由计算结果，确定该公式能够有效的判断异常值。


```python
import numpy as np
import matplotlib.pyplot as plt

outlier_data=np.array([2.1,2.6,2.4,2.5,2.3,2.1,2.3,2.6,8.2,8.3])  # 建立简单的数据，便于观察
ax1=plt.subplot(221)
ax1.margins(0.05) 
ax1.boxplot(outlier_data)
ax1.set_title('simple data before')

def is_outlier(data,threshold=3.5):
    '''
    function-判断异常值
        
    Params:
        data - 待分析的数据，列表或者一维数组；list/array
        threshold - 判断是否为异常值的边界条件, The default is 3.5；float
        
    Returns
        is_outlier_bool - 判断异常值后的布尔值列表；list(bool)
        data[~is_outlier_bool] - 移除异常值后的数值列表；list
    '''
    import numpy as np    
    
    MAD=np.median(abs(data-np.median(data)))
    modified_ZScore=0.6745*(data-np.median(data))/MAD
    is_outlier_bool=abs(modified_ZScore)>threshold    
    return is_outlier_bool, data[~is_outlier_bool]
    
is_outlier_bool,data_clean=is_outlier(outlier_data,threshold=3.5)    
print(is_outlier_bool,data_clean)

plt.rcParams["figure.figsize"]=(10,10)
ax2=plt.subplot(222)
ax2.margins(0.05) 
ax2.boxplot(data_clean)
ax2.set_title('simple data after')

_,delicacyPrice_outliersDrop=is_outlier(delicacy_price_df_clean.price,threshold=3.5)
print("原始数据描述:",delicacy_price_df_clean.price.describe(),"\n")
print("-"*50)
print("异常值处理后数据描述:", delicacyPrice_outliersDrop.describe(), "\n")

ax3=plt.subplot(223)
ax3.margins(0.05) 
ax3.boxplot(delicacy_price_df_clean.price)
ax3.set_title('experimental data before')

ax3=plt.subplot(224)
ax3.margins(0.05) 
ax3.boxplot(delicacyPrice_outliersDrop)
ax3.set_title('experimental data after')

plt.show()
```

    [False False False False False False False False  True  True] [2.1 2.6 2.4 2.5 2.3 2.1 2.3 2.6]
    原始数据描述: count    2303.000000
    mean       54.186062
    std        51.215379
    min         0.000000
    25%        22.000000
    50%        41.000000
    75%        72.000000
    max       617.000000
    Name: price, dtype: float64 
    
    --------------------------------------------------
    异常值处理后数据描述: count    2232.000000
    mean       47.695116
    std        30.958886
    min         0.000000
    25%        22.000000
    50%        40.000000
    75%        68.000000
    max       155.000000
    Name: price, dtype: float64 
    
    


<img src="./imgs/2_1_1/output_99_1.png" height='auto' width='auto' title="caDesign">    
    



```python
plt.rcParams["figure.figsize"]=(5,5)
comparisonOFdistribution(pd.DataFrame(delicacyPrice_outliersDrop,columns=['price']),'price',bins=100)
```

    mean:47.70, SD:30.96
    norm_mean:0.00, norm_SD:1.00
    


<img src="./imgs/2_1_1/output_100_1.png" height='auto' width='auto' title="caDesign"> 
    


如果将较高的价格视为异常值移除，调用定义的`comparisonOFdistribution(df,field,bins=100)`函数打印概率密度函数，比较标准正态分布可以发现右侧较高值的部分发生了变化，其它部分未发生明显变化。那么如何检验数据集是否服从正态分布？`SciPy`库提供有多种正态性检验的方法，分别为`kstest`，`shapiro`，`normaltest`和`anderson`。计算结果显示的p-value基本为0，即p-value<0.05，拒绝原假设，表明清理异常值后的美食价格数据集仍不服从正态分布，为非正态数据集。只有服从正态分布的数据可以计算从一个总体中选取特定值或区间的概率，本次美食价格数据集为正偏和尖峰分布，为非正态数据集，正态分布的概率不能很好适用于该类数据集。

`SciPy`检验数据集是否服从正态分布的返回值，第1个为统计量，该值越接近0表明数据和标准正态分布拟合的越好；第2个为p-value值，如果该值大于0.05的显著性水平，接受原假设，判断样本的总体服从正态分布。


```python
from scipy import stats

kstest_test=stats.kstest(delicacyPrice_outliersDrop,cdf='norm')
print("original data:",kstest_test)
z_score_kstest_test=stats.kstest(stats.zscore(delicacyPrice_outliersDrop),cdf='norm')
print('z_score:',z_score_kstest_test)

# 官方文档注释，当N>5000时，只有统计量正确，但是p-value值不一定正确。本次实验`len(delicacyPrice_outliersDrop)`数据量为770，可以使用
shapiro_test=stats.shapiro(delicacyPrice_outliersDrop)  
print("original data - shapiroResults(statistic=%f,pvalue=%f)"%(shapiro_test))

normaltest_test=stats.normaltest(delicacyPrice_outliersDrop,axis=None)
print("original data:",normaltest_test)

anderson_test=stats.anderson(delicacyPrice_outliersDrop,dist='norm')
print("original data:",anderson_test)
```

    original data: KstestResult(statistic=0.999551684674593, pvalue=0.0)
    z_score: KstestResult(statistic=0.11464960111854455, pvalue=5.153632014592587e-26)
    original data - shapiroResults(statistic=0.918487,pvalue=0.000000)
    original data: NormaltestResult(statistic=232.93794469002853, pvalue=2.6191959927510468e-51)
    original data: AndersonResult(statistic=53.19820994109705, critical_values=array([0.575, 0.655, 0.786, 0.916, 1.09 ]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]))
    

#### 5）给定特定值计算概率，及找到给定概率的值

如果数据集服从正态分布，则可以直接使用`SciPy`库`stats`类中提供的`norm`方法进行计算。下述案例使用`norm.cdf()`、`norm.sf()`和`norm.ppf()`分别计算小于或等于特定值、大于或等于特定值、或找到给定概率的值进行计算。


```python
from scipy.stats import norm
import matplotlib.pyplot as plt
import numpy as np

fig, ax=plt.subplots(1, 1)
mean, var, skew, kurt = norm.stats(moments='mvsk')  
print('mean, var, skew, kurt=',(mean, var, skew, kurt))  # 验证符合标准正态分布的相关统计量
x=np.linspace(norm.ppf(0.01),norm.ppf(0.99), 100)  # norm.ppf 百分比点函数 - Percent point function (inverse of cdf — percentiles)
ax.plot(x, norm.pdf(x), 'r-', lw=5, alpha=0.6, label='norm pdf')  # norm.pdf为概率密度函数
rv=norm()  # 固定形状（偏度和峰度）、位置loc（平均值）和比例scale（标准差）参数，即指定固定值
ax.plot(x, rv.pdf(x), 'k-', lw=3, label='frozen pdf')  # 固定/“冻结”分布（frozen distribution）

vals = norm.ppf([0.001, 0.5, 0.999]) # 返回概率为0.1%、50%和99.9%的值，默认loc=0,scale=1
print("验证累积分布函数CDF返回值与其PPF返回值是否相等或近似：",np.allclose([0.001, 0.5, 0.999], norm.cdf(vals)))

r=norm.rvs(size=1000) # 指定数据集大小
ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
ax.legend(loc='best', frameon=False)
plt.show()
```

    mean, var, skew, kurt= (array(0.), array(1.), array(0.), array(0.))
    验证累积分布函数CDF返回值与其PPF返回值是否相等或近似： True
    


<img src="./imgs/2_1_1/output_104_1.png" height='auto' width='auto' title="caDesign">    
    



```python
# 如果需要计算概率则定义固定分布的数据集。
print("用.cdf计算值小于或等于113的概率为：",norm.cdf(113,100,12))  # pdf(x, loc=0, scale=1) 配置Loc(均值)和scale(标准差)
print("用.sf计算值大于或等于113待概率为：",norm.sf(113,100,12)) 
print("可以观察到.cdf（<=113）概率结果+.sf(>=113)概率结果为：",norm.cdf(113,100,12)+norm.sf(113,100,12))
print("用.ppf找到给定概率值为98%的数值为：",norm.ppf(0.86066975,100,12))
```

    用.cdf计算值小于或等于113的概率为： 0.8606697525503779
    用.sf计算值大于或等于113待概率为： 0.13933024744962208
    可以观察到.cdf（<=113）概率结果+.sf(>=113)概率结果为： 1.0
    用.ppf找到给定概率值为98%的数值为： 112.99999986204986
    

在传统的概率计算中，已知z-score（标准计分），可以通过查表的方式来获取对应的概率值，这种方式已经很少使用。为了直观的观察概率值，由曲线、横轴和通过对应概率密度函数值的垂直线围合的面积即为概率值，通过绘制图表可以更清晰的观察。


```python
def probability_graph(x_i,x_min,x_max,x_s=-9999,left=True,step=0.001,subplot_num=221,loc=0,scale=1):    
    '''
    function - 正态分布概率计算及图形表述
    
    Paras:
        x_i - 待预测概率的值；float
        x_min - 数据集区间最小值；float
        x_max - 数据集区间最大值；float
        x_s - 第2个带预测概率的值，其值大于x_i值。The default is -9999；float
        left - 是否计算小于或等于，或者大于或等于指定值的概率。The default is True；bool
        step - 数据集区间的步幅。The default is 0.001；float
        subplot_num - 打印子图的序号，例如221中，第一个2代表列，第二个2代表行，第三个是子图的序号，即总共2行2列总共4个子图，1为第一个子图。The default is 221；int
        loc - 即均值。The default is 0；float
        scale - 标准差。The default is 1；float
        
    Returns:
        None
    '''
    import math
    import numpy as np
    from scipy.stats import norm
    
    x=np.arange(x_min,x_max,step)
    ax=plt.subplot(subplot_num)
    ax.margins(0.2) 
    ax.plot(x,norm.pdf(x,loc=loc,scale=scale))
    ax.set_title('N(%s,$%s^2$),x=%s'%(loc,scale,x_i))
    ax.set_xlabel('x')
    ax.set_ylabel('pdf(x)')
    ax.grid(True)
    
    if x_s==-9999:
        if left:
            px=np.arange(x_min,x_i,step)
            ax.text(loc-loc/10,0.01,round(norm.cdf(x_i,loc=loc,scale=scale),3), fontsize=20)
        else:
            px=np.arange(x_i,x_max,step)
            ax.text(loc+loc/10,0.01,round(1-norm.cdf(x_i,loc=loc,scale=scale),3), fontsize=20)
        
    else:
        px=np.arange(x_s,x_i,step)
        ax.text(loc-loc/10,0.01,round(norm.cdf(x_i,loc=loc,scale=scale)-norm.cdf(x_s,loc=loc,scale=scale),2), fontsize=20)
    ax.set_ylim(0,norm.pdf(loc,loc=loc,scale=scale)+0.005)
    ax.fill_between(px,norm.pdf(px,loc=loc,scale=scale),alpha=0.5, color='g')
    
plt.figure(figsize=(10,10))
probability_graph(x_i=113,x_min=50,x_max=150,step=1,subplot_num=221,loc=100,scale=12)
probability_graph(x_i=113,x_min=50,x_max=150,step=1,left=False,subplot_num=223,loc=100,scale=12)
probability_graph(x_i=113,x_min=50,x_max=150,x_s=90,step=1,subplot_num=222,loc=100,scale=12)
probability_graph(x_i=90,x_min=50,x_max=150,step=1,subplot_num=224,loc=100,scale=20)
plt.show()
```


<img src="./imgs/2_1_1/output_107_0.png" height='auto' width='auto' title="caDesign">    

---

注释（Notes）：

① 百度地图开放平台（API），是为开发者免费提供的一套基于百度地图服务的应用接口，包括JavaScript API、Web服务API、Android SDK、iOS SDK、定位SDK、车联网API、LBS云等多种开发工具与服务，提供基本地图展现、搜索、定位、逆/地理编码、路线规划、LBS云存储与检索等功能，适用于PC端、移动端、服务器等多种设备，多种操作系统下的地图应用开发（<https://lbsyun.baidu.com/index.php?title=%E9%A6%96%E9%A1%B5>）。

② 兴趣点（POI，points of interest），是一个具有一定意义的特定点位置，例如地球上代表菲尔铁塔（Eiffel Tower）的位置，或是火星上代表最高峰奥林匹斯山（Olympus Mons）的位置。在提及酒店、露营地、加油站或汽车导航系统中任何其它类别都会使用这个术语（<>）。

③ 服务文档，接口功能介绍，含参数说明（<https://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi>）。

④ 百度地图坐标拾取系统，支持POI点坐标显示、复制；地址精确/模糊查询；坐标鼠标跟随显示；支持坐标查询（<http://api.map.baidu.com/lbsapi/getpoint/index.html>）。

⑤ Spatial Reference， 空间参考系统（spatial reference system，SRS)或坐标参考系统（ coordinate reference system，CRS）是用于精确测量地球表面位置，用坐标表示的一个体系。是坐标系和解析几何的抽象数学在地理空间的应以。例如一个CRS坐标表示为：`Universal Transverse Mercator WGS 84 Zone 16`，包括选择地球椭圆体、水平基准、地图投影，原点和计量单位等（<https://spatialreference.org/>）。

⑥ Plotly，是一个通过数据应用实现数据驱动决策的实践者，构建有支持多种编程语言的图表库（<https://plotly.com/>）。

⑦ Plotly Python，Plotly的Python图形库可以制作互动的、具有出版质量的图表。例如制作线图（line plots）、散点图（scatter plots）、面积图（area charts）、条形图（bar charts）、误差条（error bars）、箱形图（box plots）、柱状图（histograms）、热图（heatmaps）、子图（subplots）、多轴图（multiple-axes）、极地图（polar charts）和气泡图（bubble charts）等（<https://plotly.com/python/>）。

⑧ mapbox，用于移动和网络应用的位置数据平台，提供定制在线地图的供应商（<https://www.mapbox.com/>）。

⑨ Matplotlib，<https://matplotlib.org/>；Plotly（含dash），<https://plotly.com/>；Bokeh，<https://docs.bokeh.org/en/latest/index.html>；seaborn，<https://seaborn.pydata.org/>。

参考文献（References）:

[1] (日)高桥麻奈著,崔建锋译.株式会社TREND-PRO漫画制作.漫画数据库[M].科学出版社.北京,2010.5.

[2] (日)高桥 信著,陈刚译.株式会社TREND-PRO漫画制作.漫画统计学[M].科学出版社.北京,2019.8.

[3] Timothy C.Urdan(蒂莫西•C•厄丹),彭志文译.白话统计学[M].中国人民大学出版社.2013,12.第3版. <Timothy C. Urdan.Statistics in plain English[M]. New York: Routledge, 2010>.

[4] Boris Iglewicz and David Hoaglin (1993), "Volume 16: How to Detect and Handle Outliers", The ASQC Basic References in Quality Control: Statistical Techniques, Edward F. Mykytka, Ph.D., Editor.
